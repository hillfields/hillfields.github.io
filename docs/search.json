[
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "LING 111: Advanced Computational Linguistics: Text Processing (Python)\nC LIT 152: Digital Humanities Practice (Python)\nPSTAT 197C: Data Science Capstone (R & Python)"
  },
  {
    "objectID": "courses.html#computer-science",
    "href": "courses.html#computer-science",
    "title": "Courses",
    "section": "Computer Science",
    "text": "Computer Science\n\nCMPSC 8: Introduction to Computer Science (Python)\nCMPSC 9: Intermediate Python (Python)"
  },
  {
    "objectID": "courses.html#statistics-and-data-science",
    "href": "courses.html#statistics-and-data-science",
    "title": "Courses",
    "section": "Statistics and Data Science",
    "text": "Statistics and Data Science\n\nCMPSC 5A-B: Introduction to Data Science (Python)\nEEMB 146: Biostatistics (R)\nPSTAT 10: Data Science Principles (R & SQL)\nPSTAT 100: Data Science Concepts and Analysis (Python)\nPSTAT 120A-B: Probability and Statistics\nPSTAT 122: Design and Analysis of Experiments\nPSTAT 126: Regression Analysis (R)\nPSTAT 127: Advanced Statistical Models (R)\nPSTAT 131: Statistical Machine Learning (R)\nPSTAT 134: Statistical Data Science (R)\nPSTAT 135: Big Data Analytics (Python & SQL)\nPSTAT 160A: Stochastic Processes (Python)\nPSTAT 174: Time Series (R)\nPSTAT 190: Teaching and Mentoring Statistics and Data Science\nPSTAT 194CS: Computational Statistics (R)\nPSTAT 197A-B: Data Science Capstone (R & Python)"
  },
  {
    "objectID": "courses.html#math",
    "href": "courses.html#math",
    "title": "Courses",
    "section": "Math",
    "text": "Math\n\nMATH 4A: Linear Algebra\nMATH 4B: Differential Equations\nMATH 6A: Vector Calculus\nMATH 8: Transition to Higher Mathematics\nMATH 117: Methods of Analysis"
  },
  {
    "objectID": "courses.html#creative-coding",
    "href": "courses.html#creative-coding",
    "title": "Courses",
    "section": "Creative Coding",
    "text": "Creative Coding\n\nART 22: Computer Programming for the Arts (Processing / Java)\nLING 194: Beyond Duolingo: Digital Games for Language Learning (Godot)\nMUS 109CA: Introduction to Interactive Audio (Max)\nRG ST 190DM: Transnational Buddhism through Digital Mapping (ArcGIS StoryMaps)"
  },
  {
    "objectID": "courses.html#linguistics",
    "href": "courses.html#linguistics",
    "title": "Courses",
    "section": "Linguistics",
    "text": "Linguistics\n\nLING 20: Language And Linguistics\nLING 102: Programming for Linguists (Python)\nLING 104: Statistical Methods in Linguistics (R)\nLING 105: Predictive Modeling in Linguistics (R)\nLING 106: Introduction to Phonology\nLING 110: Foundations of Computational Linguistics (Python)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Examining the Twitter Discourse Surrounding Large Language Models\n\n\n\nPython\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Data Science Salaries\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding and Modeling Human Mobility Response to California Wildfires\n\n\n\nR\n\n\nPython\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#python",
    "href": "projects.html#python",
    "title": "Projects",
    "section": "Python",
    "text": "Python\n\n\n\n\n\n\nExamining LLM Tweets using NLP [Colab]\n\n\n\n\n\n\n\nLyrics Frequency Visualizer [code]\n\n\n\n\n\n\n\n\n\nAnalyzing UCSB Grades During Remote Instruction [report]\n\n\n\n\n\n\n\nAnalyzing Voter Turnout in Alaska [report]\n\n\n\n\n\n\n\n\n\nSong Genre Classification [report]"
  },
  {
    "objectID": "projects.html#creative-coding",
    "href": "projects.html#creative-coding",
    "title": "Projects",
    "section": "Creative Coding",
    "text": "Creative Coding\n\n\n\n\n\n\nArt Programming Portfolio (Processing) [website]\n\n\n\n\n\n\n\nLanguage Learning Game (Godot) [code]\n\n\n\n\n\n\n\n\n\nInteractive Audio Projects (Max) [website]\n\n\n\n\n\n\n\nTracing the History of the ABMT (ArcGIS StoryMaps) [website]"
  },
  {
    "objectID": "projects.html#linguistics",
    "href": "projects.html#linguistics",
    "title": "Projects",
    "section": "Linguistics",
    "text": "Linguistics\n\nPhonetic Description of Japanese | report\n\nResearched the basic phonology of Japanese, including its pitch accent patterns"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Justin Liu",
    "section": "",
    "text": "I enjoy creating data visualizations, making concepts intuitive through teaching, learning Japanese, and listening to music."
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Justin Liu",
    "section": "Links",
    "text": "Links\n\nGitHub"
  },
  {
    "objectID": "mus-109ca.html",
    "href": "mus-109ca.html",
    "title": "Interactive Audio Projects",
    "section": "",
    "text": "Over the course of 6 weeks, I learned how to use Max, a visual programming environment for music and multimedia. Despite not having much knowledge in music theory/production, I came out of this class with several projects that I am quite proud of."
  },
  {
    "objectID": "mus-109ca.html#assignment-1-note-generator",
    "href": "mus-109ca.html#assignment-1-note-generator",
    "title": "Interactive Audio Projects",
    "section": "Assignment 1: Note Generator",
    "text": "Assignment 1: Note Generator\nGenerates random notes within a major scale, with the scale changing every 4 seconds."
  },
  {
    "objectID": "mus-109ca.html#assignment-2-synthesizer",
    "href": "mus-109ca.html#assignment-2-synthesizer",
    "title": "Interactive Audio Projects",
    "section": "Assignment 2: Synthesizer",
    "text": "Assignment 2: Synthesizer\nSynthesizer that incorporates oscillation, filters, and modulation."
  },
  {
    "objectID": "mus-109ca.html#assignment-3-variable-rate-sampler-instrument",
    "href": "mus-109ca.html#assignment-3-variable-rate-sampler-instrument",
    "title": "Interactive Audio Projects",
    "section": "Assignment 3: Variable-rate Sampler Instrument",
    "text": "Assignment 3: Variable-rate Sampler Instrument\nSampler using audio files and recordings."
  },
  {
    "objectID": "mus-109ca.html#final-project-sequence-generators",
    "href": "mus-109ca.html#final-project-sequence-generators",
    "title": "Interactive Audio Projects",
    "section": "Final Project: Sequence Generators",
    "text": "Final Project: Sequence Generators\nGenerates random sequences of notes based on a Markov chain model, which is then passed through a synthesizer."
  },
  {
    "objectID": "index.html#certifications",
    "href": "index.html#certifications",
    "title": "Home",
    "section": "Certifications",
    "text": "Certifications\n\nData Visualization with R"
  },
  {
    "objectID": "index.html#coding",
    "href": "index.html#coding",
    "title": "Home",
    "section": "Coding",
    "text": "Coding\n\nLanguages: R, Python, SQL, HTML/CSS\nLibraries: Shiny, tidyverse/tidymodels, Leaflet, NumPy, Pandas, Matplotlib, Altair, Scikit-learn, PySpark, Tensorflow\nDeveloper Tools: RStudio, Quarto, Jupyter, Git/GitHub, Visual Studio Code, Google Cloud Platform, Google Colab\nAwards: 3-time recipient of the Creative Computing Scholarship (see some projects here)"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Justin Liu",
    "section": "Interests",
    "text": "Interests\n\nData analysis & visualization\nLanguage learning (Japanese)\nMusic (mainly just listening to it)\nTeaching"
  },
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Coursework",
    "section": "",
    "text": "CMPSC 8: Introduction to Computer Science (Python)\nCMPSC 9: Intermediate Python (Python)"
  },
  {
    "objectID": "coursework.html#computer-science",
    "href": "coursework.html#computer-science",
    "title": "Coursework",
    "section": "",
    "text": "CMPSC 8: Introduction to Computer Science (Python)\nCMPSC 9: Intermediate Python (Python)"
  },
  {
    "objectID": "coursework.html#statistics-and-data-science",
    "href": "coursework.html#statistics-and-data-science",
    "title": "Coursework",
    "section": "Statistics and Data Science",
    "text": "Statistics and Data Science\n\nCMPSC 5A-B: Introduction to Data Science (Python)\nEEMB 146: Biostatistics (R)\nPSTAT 10: Data Science Principles (R & SQL)\nPSTAT 100: Data Science Concepts and Analysis (Python)\nPSTAT 120A-B: Probability and Statistics\nPSTAT 122: Design and Analysis of Experiments\nPSTAT 126: Regression Analysis (R)\nPSTAT 127: Advanced Statistical Models (R)\nPSTAT 131: Statistical Machine Learning (R)\nPSTAT 134: Statistical Data Science (R)\nPSTAT 135: Big Data Analytics (Python & SQL)\nPSTAT 160A: Stochastic Processes (Python)\nPSTAT 174: Time Series (R)\nPSTAT 190: Teaching and Mentoring Statistics and Data Science\nPSTAT 194CS: Computational Statistics (R)\nPSTAT 197A-B-C: Data Science Capstone (R & Python)"
  },
  {
    "objectID": "coursework.html#math",
    "href": "coursework.html#math",
    "title": "Coursework",
    "section": "Math",
    "text": "Math\n\nMATH 4A: Linear Algebra\nMATH 4B: Differential Equations\nMATH 6A: Vector Calculus\nMATH 8: Transition to Higher Mathematics\nMATH 117: Methods of Analysis"
  },
  {
    "objectID": "coursework.html#creative-coding",
    "href": "coursework.html#creative-coding",
    "title": "Coursework",
    "section": "Creative Coding",
    "text": "Creative Coding\n\nART 22: Computer Programming for the Arts (Java & HTML/CSS)\nLING 194: Beyond Duolingo: Digital Games for Language Learning (Godot)\nMUS 109CA: Introduction to Interactive Audio (Max)\nRG ST 190DM: Transnational Buddhism through Digital Mapping (ArcGIS StoryMaps)"
  },
  {
    "objectID": "coursework.html#linguistics",
    "href": "coursework.html#linguistics",
    "title": "Coursework",
    "section": "Linguistics",
    "text": "Linguistics\n\nLING 20: Language and Linguistics\nLING 102: Programming for Linguists (Python)\nLING 104: Statistical Methods in Linguistics (R)\nLING 105: Predictive Modeling in Linguistics (R)\nLING 106: Introduction to Phonology\nLING 110: Foundations of Computational Linguistics (Python)\nLING 111: Advanced Computational Linguistics: Text Processing (Python)"
  },
  {
    "objectID": "coursework.html#other",
    "href": "coursework.html#other",
    "title": "Coursework",
    "section": "Other",
    "text": "Other\n\nC LIT 152: Digital Humanities Practice (Python)\nEACS 136: Children at War\nJAPAN 63: Sociology of Japan\nJAPAN 165: Popular Culture in Japan\nMUS 11: Fundamentals of Music"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Justin Liu",
    "section": "Experience",
    "text": "Experience\n10/2023 - present | Data Science Intern | Manzanita\n\nUsing NLP & machine learning to better understand consumer psychology\n\n09/2022 - 06/2023 | Undergraduate Learning Assistant | PSTAT & CS Departments @ UCSB\n\nProvided support for students taking introductory classes in Python, R, and SQL\n\n01/2023 - 06/2023 | Sponsored Capstone Member | MOVE Lab @ UCSB\n\nInvestigated the impact of Lake Fire (2020) on human movement through spatial data science\n\n01/2023 - 06/2023 | Undergraduate Research Assistant | CPLS Lab @ UCSB\n\nRedesigned Spanish language stimuli using Python and language tools for a linguistics study\n\n01/2022 - 08/2022 | Undergraduate Research Assistant | Education Department @ UCSB\n\nAssisted PhD student’s research on the effects of participative music experiences on the sense of belonging and persistence of California transfer students\n\n09/2021 - 03/2022 | Notetaker | Disabled Students Program @ UCSB\n\nTook comprehensive notes on computational linguistics lectures for students with disabilities\n\n08/2018 - 06/2019 | Tutor | AVID Program\n\nLed homework sessions for students struggling in algebra and geometry"
  },
  {
    "objectID": "coursework.html#other-classes-i-enjoyed",
    "href": "coursework.html#other-classes-i-enjoyed",
    "title": "Coursework",
    "section": "Other Classes I Enjoyed",
    "text": "Other Classes I Enjoyed\n\nC LIT 152: Digital Humanities Practice (Python)\nJAPAN 63: Sociology of Japan\nJAPAN 165: Popular Culture in Japan\nMUS 11: Fundamentals of Music"
  },
  {
    "objectID": "projects.html#r",
    "href": "projects.html#r",
    "title": "Projects",
    "section": "R",
    "text": "R\n\n\n\n\n\n\nClustering Analysis on Country Data [vignette]\n\n\n\n\n\n\n\nForecasting Activity Rates in Japan [report]\n\n\n\n\n\n\n\n\n\nModeling the 2020 Presidential Election [report]\n\n\n\n\n\n\n\nAnime Score Predictor [code]"
  },
  {
    "objectID": "projects.html#test",
    "href": "projects.html#test",
    "title": "Projects",
    "section": "Test",
    "text": "Test"
  },
  {
    "objectID": "index.html#test",
    "href": "index.html#test",
    "title": "Home",
    "section": "Test",
    "text": "Test\n\n\n\n\n\nClustering Analysis on Country Data\n\n\n\n\n\nForecasting Activity Rates in Japan\n\n\n\n\n\n\n\nModeling the 2020 Presidential Election"
  },
  {
    "objectID": "projects.html#r-1",
    "href": "projects.html#r-1",
    "title": "Projects",
    "section": "R",
    "text": "R\n\nPredicting Anime Scores | R | code\n\nBuilt 6 regression models using cross-validation and hyperparameters to predict scores on an anime site\n\nClustering Analysis on Country Data | R | code, vignette\n\nImplemented unsupervised clustering methods to group countries based on socioeconomic and health conditions\n\nModeling Japan’s Activity Rates in the 21st Century | R | code, report\n\nForecasted monthly activity rates in Japan for the year 2022 using a SARIMA model\n\nAnalyzing the 2020 Presidential Election | R | report\n\nExperimented with several classification models to predict the winning candidate for each county, achieving a test classification accuracy of 93.5%"
  },
  {
    "objectID": "projects.html#python-1",
    "href": "projects.html#python-1",
    "title": "Projects",
    "section": "Python",
    "text": "Python\n\nClassifying Genres with Song Lyrics | Python | report\n\nImplemented Tensorflow models using semantic and phonological representations of song lyrics to predict their genres\n\nLyrics Frequency Visualizer | Python | code\n\nMade a simple app to access the Spotify API, scrape Japanese song lyrics, and visualize their word frequencies"
  },
  {
    "objectID": "projects.html#other",
    "href": "projects.html#other",
    "title": "Projects",
    "section": "Other",
    "text": "Other\n\n\n\n\n\n\nZipfinder [Devpost]"
  },
  {
    "objectID": "projects.html#creative-coding-1",
    "href": "projects.html#creative-coding-1",
    "title": "Projects",
    "section": "Creative Coding",
    "text": "Creative Coding\n\nArt Portfolio | Processing / Java | code, site\n\nCreated interactive art pieces using the Processing graphics library in Java\nDesigned the website to host the projects from scratch using HTML/CSS\n\nLanguage Learning Game | Godot | code\n\nWorked with other students to build a game for learning Mixtec\n\nInteractive Audio Projects | Max | site\n\nUtilized the visual programming language Max for several creative music projects\n\nAmerican Buddhist Meditation Temple StoryMap | ArcGIS StoryMaps | site\n\nCollaborated with other students to map the history of the American Buddhist Meditation Temple in Santa Barbara, CA"
  },
  {
    "objectID": "projects.html#research",
    "href": "projects.html#research",
    "title": "Projects",
    "section": "",
    "text": "Understanding and Modeling Human Mobility Response to California Wildfires [summary] [poster]\n\n\n\n\n\n\n\nUnderrepresented Minority Music Students at California Community Colleges [poster]"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "Justin Liu",
    "section": "Technical Skills",
    "text": "Technical Skills\nLanguages & Libraries:\n\nPython (pandas, NumPy, Seaborn, Matplotlib, Altair, scikit-learn, PySpark, PyTorch, Tensorflow)\nR (Shiny, ggplot2, tidyverse, tidymodels, Leaflet)\nSQL\nHTML/CSS\nJava\n\nDeveloper Tools:\n\nRStudio\nJupyter\nVS Code\nGit/GitHub\nQuarto (what this website is built on)"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Justin Liu",
    "section": "",
    "text": "Education\n\nWaseda University (current)\n\nOne-year study abroad program\n\nUCSB\n\nMajor in Statistics & Data Science\nMinor in Linguistics (Speech & Technologies Emphasis)\n\nIB Program\n\nRigorous high school curriculum\nPsychology Extended Essay on Second Language Acquisition\n\n\nInterests\n\nData analysis & visualization\nLanguage learning (Japanese)\nTeaching\nMusic"
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Justin Liu",
    "section": "Credit",
    "text": "Credit\n\nProfile picture by ぬくぬくにぎりめし"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Justin Liu",
    "section": "Education",
    "text": "Education\nWaseda University (current)\n\nOne-year study abroad immersion program (English & Japanese)\n\nUCSB\n\nMajor in Statistics & Data Science\nMinor in Linguistics (Speech & Technologies Emphasis)\n\nIB Program\n\nRigorous high school curriculum\nPsychology Extended Essay on Second Language Acquisition"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "my-new-blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects_old.html",
    "href": "projects_old.html",
    "title": "Projects",
    "section": "",
    "text": "Understanding and Modeling Human Mobility Response to California Wildfires [summary] [poster]\n\n\n\n\n\n\n\nUnderrepresented Minority Music Students at California Community Colleges [poster]"
  },
  {
    "objectID": "projects_old.html#research",
    "href": "projects_old.html#research",
    "title": "Projects",
    "section": "",
    "text": "Understanding and Modeling Human Mobility Response to California Wildfires [summary] [poster]\n\n\n\n\n\n\n\nUnderrepresented Minority Music Students at California Community Colleges [poster]"
  },
  {
    "objectID": "projects_old.html#r",
    "href": "projects_old.html#r",
    "title": "Projects",
    "section": "R",
    "text": "R\n\n\n\n\n\n\nClustering Analysis on Country Data [vignette]\n\n\n\n\n\n\n\nForecasting Activity Rates in Japan [report]\n\n\n\n\n\n\n\n\n\nModeling the 2020 Presidential Election [report]\n\n\n\n\n\n\n\nAnime Score Predictor [code]"
  },
  {
    "objectID": "projects_old.html#python",
    "href": "projects_old.html#python",
    "title": "Projects",
    "section": "Python",
    "text": "Python\n\n\n\n\n\n\nExamining LLM Tweets using NLP [Colab]\n\n\n\n\n\n\n\nLyrics Frequency Visualizer [code]\n\n\n\n\n\n\n\n\n\nAnalyzing UCSB Grades During Remote Instruction [report]\n\n\n\n\n\n\n\nAnalyzing Voter Turnout in Alaska [report]\n\n\n\n\n\n\n\n\n\nSong Genre Classification [report]"
  },
  {
    "objectID": "projects_old.html#creative-coding",
    "href": "projects_old.html#creative-coding",
    "title": "Projects",
    "section": "Creative Coding",
    "text": "Creative Coding\n\n\n\n\n\n\nArt Programming Portfolio (Processing) [website]\n\n\n\n\n\n\n\nLanguage Learning Game (Godot) [code]\n\n\n\n\n\n\n\n\n\nInteractive Audio Projects (Max) [website]\n\n\n\n\n\n\n\nTracing the History of the ABMT (ArcGIS StoryMaps) [website]"
  },
  {
    "objectID": "projects_old.html#other",
    "href": "projects_old.html#other",
    "title": "Projects",
    "section": "Other",
    "text": "Other\n\n\n\n\n\n\nZipfinder [Devpost]"
  },
  {
    "objectID": "projects/capstone/index.html#links",
    "href": "projects/capstone/index.html#links",
    "title": "Understanding and Modeling Human Mobility Response to California Wildfires",
    "section": "",
    "text": "Individual contribution writeup\nPoster"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Authors - Hannah Minsuh Choi (1M210029) - Justin Liu (97231079) - Shulei Wang (97231161)"
  },
  {
    "objectID": "project.html#table-of-contents",
    "href": "project.html#table-of-contents",
    "title": "Predicting Data Science Salaries",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackground\n\nMotivation\nResearch Question\nDataset\n\nPreprocessing & EDA\n\nData Cleaning\nData Visualizations\n\nModeling\n\nConsiderations\nData Preparation\nModel #1: Dummy Regression\nModel #2: Linear Regression\nModel #3: LASSO Regression\nModel #4: Random Forest\nModel #5: Boosted Trees\nModel #6: Neural Network\nBox-Cox Transformation\n\nResults & Analysis\nConclusion\n\nMain Findings\nLimitations\nFuture Analysis"
  },
  {
    "objectID": "project.html#background",
    "href": "project.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use (https://ai-jobs.net/salaries/download/).\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "project.html#preprocessing-eda",
    "href": "project.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\nData Cleaning\n\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n# check datatypes\nsalaries.dtypes\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n# check missing values\nsalaries.isnull().sum()\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above: - We dropped some redundant variables. - We dropped salary and salary_currency since they were used to calculate salary_in_usd. - We also dropped employee_residence since it is very similar to company_location. - We removed duplicate rows to ensure that each observation was unique. - As a result, more than 4000 rows were dropped. - We converted several numerical variables to categorical variables. - We turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively). - We recoded the abbreviated forms of some categories to make them easier to read. - For example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on. - We manually grouped the levels in a few categorical variables into broader groups. - Some of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data). - For job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job). - For company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data). - We then dropped job_title from our dataset.\n\n\nData Visualizations\n\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Count\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Salary in USD ($)\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above: - company_location to salary_in_usd (0.35) - experience_level to salary_in_usd (0.3) - company_location to work_year (0.21)"
  },
  {
    "objectID": "project.html#modeling",
    "href": "project.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\nX_train # predictors training set\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\ny_train # target test set\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n# show intercept\nlm_model.intercept_\n\n63760.94720830995\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example: - Having a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively. - If the company is located in the United States, then the salary is predicted to increase by $47,530.45. - Working in a data management job can increase one’s salary by $45,648.72. - Being a freelancer is associated with a salary decrease of $47,912.14.\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\nWe first train the neural network using MAE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "project.html#results-analysis",
    "href": "project.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "project.html#conclusion",
    "href": "project.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "projects/ds-salaries/index.html#links",
    "href": "projects/ds-salaries/index.html#links",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "predicting-ds-salaries.html",
    "href": "predicting-ds-salaries.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Authors - Hannah Minsuh Choi (1M210029) - Justin Liu (97231079) - Shulei Wang (97231161)"
  },
  {
    "objectID": "predicting-ds-salaries.html#table-of-contents",
    "href": "predicting-ds-salaries.html#table-of-contents",
    "title": "Predicting Data Science Salaries",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackground\n\nMotivation\nResearch Question\nDataset\n\nPreprocessing & EDA\n\nData Cleaning\nData Visualizations\n\nModeling\n\nConsiderations\nData Preparation\nModel #1: Dummy Regression\nModel #2: Linear Regression\nModel #3: LASSO Regression\nModel #4: Random Forest\nModel #5: Boosted Trees\nModel #6: Neural Network\nBox-Cox Transformation\n\nResults & Analysis\nConclusion\n\nMain Findings\nLimitations\nFuture Analysis"
  },
  {
    "objectID": "predicting-ds-salaries.html#background",
    "href": "predicting-ds-salaries.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use (https://ai-jobs.net/salaries/download/).\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "predicting-ds-salaries.html#preprocessing-eda",
    "href": "predicting-ds-salaries.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\nData Cleaning\n\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n# check datatypes\nsalaries.dtypes\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n# check missing values\nsalaries.isnull().sum()\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above: - We dropped some redundant variables. - We dropped salary and salary_currency since they were used to calculate salary_in_usd. - We also dropped employee_residence since it is very similar to company_location. - We removed duplicate rows to ensure that each observation was unique. - As a result, more than 4000 rows were dropped. - We converted several numerical variables to categorical variables. - We turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively). - We recoded the abbreviated forms of some categories to make them easier to read. - For example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on. - We manually grouped the levels in a few categorical variables into broader groups. - Some of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data). - For job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job). - For company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data). - We then dropped job_title from our dataset.\n\n\nData Visualizations\n\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Count\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Salary in USD ($)\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above: - company_location to salary_in_usd (0.35) - experience_level to salary_in_usd (0.3) - company_location to work_year (0.21)"
  },
  {
    "objectID": "predicting-ds-salaries.html#modeling",
    "href": "predicting-ds-salaries.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\nX_train # predictors training set\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\ny_train # target test set\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n# show intercept\nlm_model.intercept_\n\n63760.94720830995\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example: - Having a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively. - If the company is located in the United States, then the salary is predicted to increase by $47,530.45. - Working in a data management job can increase one’s salary by $45,648.72. - Being a freelancer is associated with a salary decrease of $47,912.14.\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\nWe first train the neural network using MAE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "predicting-ds-salaries.html#results-analysis",
    "href": "predicting-ds-salaries.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "predicting-ds-salaries.html#conclusion",
    "href": "predicting-ds-salaries.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "projects/ds-salaries/files/predicting-ds-salaries.html",
    "href": "projects/ds-salaries/files/predicting-ds-salaries.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Authors"
  },
  {
    "objectID": "projects/ds-salaries/files/predicting-ds-salaries.html#table-of-contents",
    "href": "projects/ds-salaries/files/predicting-ds-salaries.html#table-of-contents",
    "title": "Predicting Data Science Salaries",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackground\n\nMotivation\nResearch Question\nDataset\n\nPreprocessing & EDA\n\nData Cleaning\nData Visualizations\n\nModeling\n\nConsiderations\nData Preparation\nModel #1: Dummy Regression\nModel #2: Linear Regression\nModel #3: LASSO Regression\nModel #4: Random Forest\nModel #5: Boosted Trees\nModel #6: Neural Network\nBox-Cox Transformation\n\nResults & Analysis\nConclusion\n\nMain Findings\nLimitations\nFuture Analysis"
  },
  {
    "objectID": "projects/ds-salaries/files/predicting-ds-salaries.html#background",
    "href": "projects/ds-salaries/files/predicting-ds-salaries.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use (https://ai-jobs.net/salaries/download/).\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "projects/ds-salaries/files/predicting-ds-salaries.html#preprocessing-eda",
    "href": "projects/ds-salaries/files/predicting-ds-salaries.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\nData Cleaning\n\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n# check datatypes\nsalaries.dtypes\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n# check missing values\nsalaries.isnull().sum()\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above: - We dropped some redundant variables. - We dropped salary and salary_currency since they were used to calculate salary_in_usd. - We also dropped employee_residence since it is very similar to company_location. - We removed duplicate rows to ensure that each observation was unique. - As a result, more than 4000 rows were dropped. - We converted several numerical variables to categorical variables. - We turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively). - We recoded the abbreviated forms of some categories to make them easier to read. - For example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on. - We manually grouped the levels in a few categorical variables into broader groups. - Some of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data). - For job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job). - For company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data). - We then dropped job_title from our dataset.\n\n\nData Visualizations\n\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Count\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Salary in USD ($)\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above: - company_location to salary_in_usd (0.35) - experience_level to salary_in_usd (0.3) - company_location to work_year (0.21)"
  },
  {
    "objectID": "projects/ds-salaries/files/predicting-ds-salaries.html#modeling",
    "href": "projects/ds-salaries/files/predicting-ds-salaries.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\nX_train # predictors training set\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\ny_train # target test set\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n# show intercept\nlm_model.intercept_\n\n63760.94720830995\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example: - Having a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively. - If the company is located in the United States, then the salary is predicted to increase by $47,530.45. - Working in a data management job can increase one’s salary by $45,648.72. - Being a freelancer is associated with a salary decrease of $47,912.14.\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\nWe first train the neural network using MAE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "projects/ds-salaries/files/predicting-ds-salaries.html#results-analysis",
    "href": "projects/ds-salaries/files/predicting-ds-salaries.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "projects/ds-salaries/files/predicting-ds-salaries.html#conclusion",
    "href": "projects/ds-salaries/files/predicting-ds-salaries.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "projects/ds-salaries/predicting-ds-salaries.html",
    "href": "projects/ds-salaries/predicting-ds-salaries.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi (1M210029)\nJustin Liu (97231079)\nShulei Wang (97231161)"
  },
  {
    "objectID": "projects/ds-salaries/predicting-ds-salaries.html#table-of-contents",
    "href": "projects/ds-salaries/predicting-ds-salaries.html#table-of-contents",
    "title": "Predicting Data Science Salaries",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackground\n\nMotivation\nResearch Question\nDataset\n\nPreprocessing & EDA\n\nData Cleaning\nData Visualizations\n\nModeling\n\nConsiderations\nData Preparation\nModel #1: Dummy Regression\nModel #2: Linear Regression\nModel #3: LASSO Regression\nModel #4: Random Forest\nModel #5: Boosted Trees\nModel #6: Neural Network\nBox-Cox Transformation\n\nResults & Analysis\nConclusion\n\nMain Findings\nLimitations\nFuture Analysis"
  },
  {
    "objectID": "projects/ds-salaries/predicting-ds-salaries.html#background",
    "href": "projects/ds-salaries/predicting-ds-salaries.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use (https://ai-jobs.net/salaries/download/).\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "projects/ds-salaries/predicting-ds-salaries.html#preprocessing-eda",
    "href": "projects/ds-salaries/predicting-ds-salaries.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\n\nData Cleaning\n\n\nCode\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n\nCode\n# check datatypes\nsalaries.dtypes\n\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n\nCode\n# check missing values\nsalaries.isnull().sum()\n\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n\nCode\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n\nCode\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n\nCode\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n\nCode\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\n\nCode\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above: - We dropped some redundant variables. - We dropped salary and salary_currency since they were used to calculate salary_in_usd. - We also dropped employee_residence since it is very similar to company_location. - We removed duplicate rows to ensure that each observation was unique. - As a result, more than 4000 rows were dropped. - We converted several numerical variables to categorical variables. - We turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively). - We recoded the abbreviated forms of some categories to make them easier to read. - For example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on. - We manually grouped the levels in a few categorical variables into broader groups. - Some of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data). - For job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job). - For company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data). - We then dropped job_title from our dataset.\n\n\nData Visualizations\n\n\nCode\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n\n\nCode\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Count\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n\nCode\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\nCode\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Salary in USD ($)\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n\nCode\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nCode\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above: - company_location to salary_in_usd (0.35) - experience_level to salary_in_usd (0.3) - company_location to work_year (0.21)"
  },
  {
    "objectID": "projects/ds-salaries/predicting-ds-salaries.html#modeling",
    "href": "projects/ds-salaries/predicting-ds-salaries.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n\nCode\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\n\n\nCode\nX_train # predictors training set\n\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\n\nCode\ny_train # target test set\n\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n\nCode\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n\nCode\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n\nCode\n# show intercept\nlm_model.intercept_\n\n\n63760.94720830995\n\n\n\n\nCode\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n\nCode\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example: - Having a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively. - If the company is located in the United States, then the salary is predicted to increase by $47,530.45. - Working in a data management job can increase one’s salary by $45,648.72. - Being a freelancer is associated with a salary decrease of $47,912.14.\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n\nCode\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n\nCode\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n\nCode\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n\nCode\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n\nCode\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n\nCode\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n\nCode\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n\nCode\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n\nCode\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n\nCode\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n\nCode\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n\nCode\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n\nCode\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n\nCode\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n\nCode\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n\nCode\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n\nCode\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n\nCode\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n\n\nCode\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\n\nWe first train the neural network using MAE as the loss function.\n\n\nCode\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n\nCode\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n\nCode\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n\nCode\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n\nCode\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n\nCode\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n\nCode\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n\nCode\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n\nCode\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "projects/ds-salaries/predicting-ds-salaries.html#results-analysis",
    "href": "projects/ds-salaries/predicting-ds-salaries.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n\nCode\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "projects/ds-salaries/predicting-ds-salaries.html#conclusion",
    "href": "projects/ds-salaries/predicting-ds-salaries.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "projects/ds-salaries/predicting-ds-salaries.html#authors",
    "href": "projects/ds-salaries/predicting-ds-salaries.html#authors",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi (1M210029)\nJustin Liu (97231079)\nShulei Wang (97231161)"
  },
  {
    "objectID": "projects/files/predicting-ds-salaries.html",
    "href": "projects/files/predicting-ds-salaries.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi (1M210029)\nJustin Liu (97231079)\nShulei Wang (97231161)"
  },
  {
    "objectID": "projects/files/predicting-ds-salaries.html#authors",
    "href": "projects/files/predicting-ds-salaries.html#authors",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi (1M210029)\nJustin Liu (97231079)\nShulei Wang (97231161)"
  },
  {
    "objectID": "projects/files/predicting-ds-salaries.html#table-of-contents",
    "href": "projects/files/predicting-ds-salaries.html#table-of-contents",
    "title": "Predicting Data Science Salaries",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackground\n\nMotivation\nResearch Question\nDataset\n\nPreprocessing & EDA\n\nData Cleaning\nData Visualizations\n\nModeling\n\nConsiderations\nData Preparation\nModel #1: Dummy Regression\nModel #2: Linear Regression\nModel #3: LASSO Regression\nModel #4: Random Forest\nModel #5: Boosted Trees\nModel #6: Neural Network\nBox-Cox Transformation\n\nResults & Analysis\nConclusion\n\nMain Findings\nLimitations\nFuture Analysis"
  },
  {
    "objectID": "projects/files/predicting-ds-salaries.html#background",
    "href": "projects/files/predicting-ds-salaries.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use (https://ai-jobs.net/salaries/download/).\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "projects/files/predicting-ds-salaries.html#preprocessing-eda",
    "href": "projects/files/predicting-ds-salaries.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\n\nData Cleaning\n\n\nCode\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n\nCode\n# check datatypes\nsalaries.dtypes\n\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n\nCode\n# check missing values\nsalaries.isnull().sum()\n\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n\nCode\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n\nCode\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n\nCode\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n\nCode\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\n\nCode\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above: - We dropped some redundant variables. - We dropped salary and salary_currency since they were used to calculate salary_in_usd. - We also dropped employee_residence since it is very similar to company_location. - We removed duplicate rows to ensure that each observation was unique. - As a result, more than 4000 rows were dropped. - We converted several numerical variables to categorical variables. - We turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively). - We recoded the abbreviated forms of some categories to make them easier to read. - For example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on. - We manually grouped the levels in a few categorical variables into broader groups. - Some of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data). - For job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job). - For company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data). - We then dropped job_title from our dataset.\n\n\nData Visualizations\n\n\nCode\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n\n\nCode\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Count\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n\nCode\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\nCode\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Salary in USD ($)\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n\nCode\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nCode\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above: - company_location to salary_in_usd (0.35) - experience_level to salary_in_usd (0.3) - company_location to work_year (0.21)"
  },
  {
    "objectID": "projects/files/predicting-ds-salaries.html#modeling",
    "href": "projects/files/predicting-ds-salaries.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n\nCode\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\n\n\nCode\nX_train # predictors training set\n\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\n\nCode\ny_train # target test set\n\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n\nCode\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n\nCode\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n\nCode\n# show intercept\nlm_model.intercept_\n\n\n63760.94720830995\n\n\n\n\nCode\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n\nCode\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example: - Having a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively. - If the company is located in the United States, then the salary is predicted to increase by $47,530.45. - Working in a data management job can increase one’s salary by $45,648.72. - Being a freelancer is associated with a salary decrease of $47,912.14.\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n\nCode\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n\nCode\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n\nCode\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n\nCode\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n\nCode\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n\nCode\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n\nCode\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n\nCode\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n\nCode\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n\nCode\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n\nCode\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n\nCode\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n\nCode\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n\nCode\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n\nCode\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n\nCode\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n\nCode\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n\nCode\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n\n\nCode\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\n\nWe first train the neural network using MAE as the loss function.\n\n\nCode\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n\nCode\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n\nCode\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n\nCode\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n\nCode\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n\nCode\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n\nCode\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n\nCode\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n\nCode\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n\nCode\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "projects/files/predicting-ds-salaries.html#results-analysis",
    "href": "projects/files/predicting-ds-salaries.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n\nCode\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "projects/files/predicting-ds-salaries.html#conclusion",
    "href": "projects/files/predicting-ds-salaries.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "files/predicting-ds-salaries.html",
    "href": "files/predicting-ds-salaries.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi\nJustin Liu\nShulei Wang"
  },
  {
    "objectID": "files/predicting-ds-salaries.html#authors",
    "href": "files/predicting-ds-salaries.html#authors",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi\nJustin Liu\nShulei Wang"
  },
  {
    "objectID": "files/predicting-ds-salaries.html#table-of-contents",
    "href": "files/predicting-ds-salaries.html#table-of-contents",
    "title": "Predicting Data Science Salaries",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackground\n\nMotivation\nResearch Question\nDataset\n\nPreprocessing & EDA\n\nImports\nData Cleaning\nData Visualizations\n\nModeling\n\nConsiderations\nData Preparation\nModel #1: Dummy Regression\nModel #2: Linear Regression\nModel #3: LASSO Regression\nModel #4: Random Forest\nModel #5: Boosted Trees\nModel #6: Neural Network\nBox-Cox Transformation\n\nResults & Analysis\nConclusion\n\nMain Findings\nLimitations\nFuture Analysis"
  },
  {
    "objectID": "files/predicting-ds-salaries.html#background",
    "href": "files/predicting-ds-salaries.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use (https://ai-jobs.net/salaries/download/).\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "files/predicting-ds-salaries.html#preprocessing-eda",
    "href": "files/predicting-ds-salaries.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\nImports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\n\nData Cleaning\n\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n# check datatypes\nsalaries.dtypes\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n# check missing values\nsalaries.isnull().sum()\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above: - We dropped some redundant variables. - We dropped salary and salary_currency since they were used to calculate salary_in_usd. - We also dropped employee_residence since it is very similar to company_location. - We removed duplicate rows to ensure that each observation was unique. - As a result, more than 4000 rows were dropped. - We converted several numerical variables to categorical variables. - We turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively). - We recoded the abbreviated forms of some categories to make them easier to read. - For example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on. - We manually grouped the levels in a few categorical variables into broader groups. - Some of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data). - For job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job). - For company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data). - We then dropped job_title from our dataset.\n\n\nData Visualizations\n\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Count\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Salary in USD ($)\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above: - company_location to salary_in_usd (0.35) - experience_level to salary_in_usd (0.3) - company_location to work_year (0.21)"
  },
  {
    "objectID": "files/predicting-ds-salaries.html#modeling",
    "href": "files/predicting-ds-salaries.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\nX_train # predictors training set\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\ny_train # target test set\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n# show intercept\nlm_model.intercept_\n\n63760.94720830995\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example: - Having a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively. - If the company is located in the United States, then the salary is predicted to increase by $47,530.45. - Working in a data management job can increase one’s salary by $45,648.72. - Being a freelancer is associated with a salary decrease of $47,912.14.\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\nWe first train the neural network using MAE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "files/predicting-ds-salaries.html#results-analysis",
    "href": "files/predicting-ds-salaries.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "files/predicting-ds-salaries.html#conclusion",
    "href": "files/predicting-ds-salaries.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "notebooks/predicting-ds-salaries.html",
    "href": "notebooks/predicting-ds-salaries.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi (1M210029)\nJustin Liu (97231079)\nShulei Wang (97231161)"
  },
  {
    "objectID": "notebooks/predicting-ds-salaries.html#authors",
    "href": "notebooks/predicting-ds-salaries.html#authors",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi (1M210029)\nJustin Liu (97231079)\nShulei Wang (97231161)"
  },
  {
    "objectID": "notebooks/predicting-ds-salaries.html#background",
    "href": "notebooks/predicting-ds-salaries.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use (https://ai-jobs.net/salaries/download/).\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "notebooks/predicting-ds-salaries.html#preprocessing-eda",
    "href": "notebooks/predicting-ds-salaries.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\nImports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\n\nData Cleaning\n\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n# check datatypes\nsalaries.dtypes\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n# check missing values\nsalaries.isnull().sum()\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above: - We dropped some redundant variables. - We dropped salary and salary_currency since they were used to calculate salary_in_usd. - We also dropped employee_residence since it is very similar to company_location. - We removed duplicate rows to ensure that each observation was unique. - As a result, more than 4000 rows were dropped. - We converted several numerical variables to categorical variables. - We turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively). - We recoded the abbreviated forms of some categories to make them easier to read. - For example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on. - We manually grouped the levels in a few categorical variables into broader groups. - Some of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data). - For job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job). - For company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data). - We then dropped job_title from our dataset.\n\n\nData Visualizations\n\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Count\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Salary in USD ($)\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above: - company_location to salary_in_usd (0.35) - experience_level to salary_in_usd (0.3) - company_location to work_year (0.21)"
  },
  {
    "objectID": "notebooks/predicting-ds-salaries.html#modeling",
    "href": "notebooks/predicting-ds-salaries.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\nX_train # predictors training set\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\ny_train # target test set\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n# show intercept\nlm_model.intercept_\n\n63760.94720830995\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example: - Having a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively. - If the company is located in the United States, then the salary is predicted to increase by $47,530.45. - Working in a data management job can increase one’s salary by $45,648.72. - Being a freelancer is associated with a salary decrease of $47,912.14.\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\nWe first train the neural network using MAE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "notebooks/predicting-ds-salaries.html#results-analysis",
    "href": "notebooks/predicting-ds-salaries.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "notebooks/predicting-ds-salaries.html#conclusion",
    "href": "notebooks/predicting-ds-salaries.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "projects/capstone/index.html",
    "href": "projects/capstone/index.html",
    "title": "Understanding and Modeling Human Mobility Response to California Wildfires",
    "section": "",
    "text": "Individual contribution writeup\nPoster"
  },
  {
    "objectID": "projects/ds-salaries/index.html",
    "href": "projects/ds-salaries/index.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "files/llm-tweets.html",
    "href": "files/llm-tweets.html",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "",
    "text": "Justin Liu"
  },
  {
    "objectID": "files/llm-tweets.html#introduction",
    "href": "files/llm-tweets.html#introduction",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "Introduction",
    "text": "Introduction\n\nMotivation\nOver the past year or so, the field of generative artificial intelligence has seen a huge rise in popularity. In particular, large language models (LLMs) that have been trained on unprecedented amounts of data can process langauge and respond to user inputs at a humanlike level. A prime example of this ChatGPT, a chatbot released on November 30, 2022, that can answer (almost) any question that it is given. LLMs are also used in generative AI art models like DALL-E and Midjourney, which can turn any text imaginable into realistic images. With the increasing availability of these tools to the general public, it is becoming easier than ever to utilize these LLMs without much technical experience. In fact, many have praised them for being revolutionary and believe that they will only improve over time.\nHowever, the use of these models have also been at the center of countless debates. There have been heated discussions about whether AI-generated art that “steals” work from actual artists can be considered real art, with controversies ranging from an image created by Midjourney winning first prize at an art contest (link) to using AI to save time on drawing backgrounds from scratch in animated films (link). And ChatGPT, with its capability to perform a wide array of often very specific tasks, could threaten to replace numerous jobs over the next several years (link).\nThe present analysis seeks to answer a seemingly simple question: What are people actually talking about when it comes to LLMs? As many of these tools are currently available for public use, it makes sense to look at how everyday people (not just specialists) are interacting with them. As a case study, we will focus on the social media platform Twitter since it provides an abundant source of data that can be used to analyze the discourse surrounding LLMs.\n\n\nDataset\nThe dataset we use in this analysis (Large Language Models: the tweets) is made publicly available by Konrad Banachewicz on Kaggle. It includes English tweets about LLMs from a wide range of Twitter users and comes with metadata (date of tweet, whether the user is verified, etc.). The tweets start from December 2022, and the dataset is updated daily with new tweets.\n\n\nQuestions\n\nWhat kinds of topics are brought up in the online discourse surrounding LLMs?\n\nHypothesis: The discourse surrounding LLMs spans a variety of topics (e.g. advances in the sciences, questions relating to ethics and the humanities) that reflect the diversity of social media users.\nMethods: We implement topic modeling by fitting an LDA model to find the most optimal grouping of tweets about LLMs. We also look into how the distribution of the resulting topics change over time.\n\nWhat kinds of sentiments are associated with online discussions about LLMs?\n\nHypothesis: There is a balance between positive and negative sentiments, reflecting a split between proponents and critics of AI.\nMethods: We carry out sentiment analysis on our tweets, which are each classified as “positive”, “neutral”, or “negative”. We also examine how these sentiments vary over time."
  },
  {
    "objectID": "files/llm-tweets.html#code",
    "href": "files/llm-tweets.html#code",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "Code",
    "text": "Code\n\nPrerequisites\nIn order to access the dataset, we need to download it from Kaggle.\nNote: At the time of this writing (June 15, 2023), the latest version of the dataset contains nothing. Instead, we will use the last version that had the tweets (Version 172), which has already been downloaded and stored in Google Drive. The commands below download that dataset.\n\n\nCode\n#@title\n!rm -rf chatgpt-the-tweets\n!gdown 1Oax8ZEqZ4mzU8Pr0gbD4ZXXZt-GZHdVE\n!unzip chatgpt-the-tweets.zip -d ./chatgpt-the-tweets\n!rm chatgpt-the-tweets.zip\n\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1Oax8ZEqZ4mzU8Pr0gbD4ZXXZt-GZHdVE\nTo: /content/chatgpt-the-tweets.zip\n100% 95.1M/95.1M [00:02&lt;00:00, 44.4MB/s]\nArchive:  chatgpt-the-tweets.zip\n  inflating: ./chatgpt-the-tweets/tweets.csv  \n\n\nThis code below is for downloading the latest version of the dataset (currently commented out, see the note above).\n\n\nCode\n#@title\n# #@title\n# # get API token and dataset from Kaggle\n# api_token = {\"username\": \"KAGGLE_USERNAME\", \"key\": \"KAGGLE_KEY\"}\n# dataset = \"konradb/chatgpt-the-tweets\"\n\n# dataset_name = dataset.split(\"/\")[1]\n# dataset_filename = dataset_name + \".zip\"\n\n# !rm -rf {dataset_name}\n# !rm -rf ~/.kaggle\n# !mkdir ~/.kaggle\n# !touch ~/.kaggle/kaggle.json\n\n# import json\n# with open(\"/root/.kaggle/kaggle.json\", \"w\") as file:\n#     json.dump(api_token, file)\n\n# !chmod 600 ~/.kaggle/kaggle.json\n\n# !kaggle datasets download -d {dataset}\n# !unzip {dataset_filename} -d ./{dataset_name}\n# !rm {dataset_filename}\n\n\nWe then import the necessary packages.\n\n\nCode\n#@title\n# install packages\n%%capture\n!pip install pyLDAvis\n\n# import packages\nimport gensim\nimport pyLDAvis.gensim\nimport pandas as pd\nimport spacy\nimport re\nimport warnings\nimport altair as alt\nfrom operator import itemgetter\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nnlp = spacy.load(\"en_core_web_sm\")\nnltk.download(\"vader_lexicon\")\nsia = SentimentIntensityAnalyzer()\nwarnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n\n\n\n\nCleaning the data\nWe’ll take a look at the dataset, dropping rows where either the tweet (text) or date (date) is missing.\n\n\nCode\n#@title\n# read the data, dropping rows where the tweet or date is missing\ntweets = pd.read_csv(\"chatgpt-the-tweets/tweets.csv\").dropna(subset = [\"text\", \"date\"])\ntweets.head()\n\n\nDtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n  tweets = pd.read_csv(\"chatgpt-the-tweets/tweets.csv\").dropna(subset = [\"text\", \"date\"])\n\n\n\n  \n    \n      \n\n\n\n\n\n\nuser_name\ntext\nuser_location\nuser_description\nuser_created\nuser_followers\nuser_friends\nuser_favourites\nuser_verified\ndate\nhashtags\nsource\n\n\n\n\n0\nreigndomains 👑\nhttps://t.co/6tFaOonLtv 🔥 for sale .\\n\\n#Royal...\nNaN\nBrand Name | https://t.co/Z4d6GWXyWz | https:/...\n2019-09-11 04:04:06+00:00\n267.0\n256.0\n1300\nFalse\n2023-06-10 12:37:16+00:00\n['RoyalGPT', 'Royal', 'Domains', 'ai', 'Web3',...\nTwitter for iPhone\n\n\n1\nMidJourney LIVE\nExquisite realism photography showcasing an ex...\nFollow for Inspiration\n🎨 Live feed of Art generated by Midjourney AI 🎨\n2018-08-28 02:01:04+00:00\n100.0\n1.0\n0\nFalse\n2023-06-10 12:36:56+00:00\nNaN\nMidjourneyLIVE\n\n\n2\nThe Tech Trend\nTop 10 ChatGPT Plugins You Should Use Right No...\nWorldwide\nA Tech community for industry experts, connect...\n2020-09-15 15:37:37+00:00\n4380.0\n4668.0\n242\nFalse\n2023-06-10 12:35:00+00:00\n['ChatGPT', 'bestChatGPTplugins']\nBuffer\n\n\n3\nThe Time Blawg\nWhat lawyers will get out of ChatGPT: legal ca...\nScotland... and Beyond\nThe past, present and future practice of law (...\n2010-12-29 18:03:14+00:00\n5897.0\n6499.0\n4693\nFalse\n2023-06-10 12:34:49+00:00\nNaN\nTwitter for Android\n\n\n4\nChristine Lopez\ndown an a But the state of summer8 being money...\nNaN\nNaN\n2023-05-06 11:03:29+00:00\n0.0\n5.0\n0\nFalse\n2023-06-10 12:33:14+00:00\n['车震', '嫩穴', 'chatGPT']\nTwitter Web App\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSince we can’t see any full tweets in the table above, we sample some random tweets and print them out below.\n\n\nCode\n#@title\n# sample 30 random tweets and print them out\nsampled_tweets_1 = tweets.sample(30, random_state = 1).text\nfor i in range(30):\n    print(\"-\" * 50)\n    print(sampled_tweets_1.iloc[i])\nprint(\"-\" * 50)\n\n\n--------------------------------------------------\n🚀 Boost Your Sales by using the \"Sealing the Deal\" template on Jeda Ai's All-in-One Workspace Canvas.\n\nGet your Daily 10K FREE AI Tokens at https://t.co/8NK5W5P55J 🤩\n\n#JedaAI #AI #template #sales #sealthedeal #ChatGPT #GPT4 https://t.co/svVecsO7XF\n--------------------------------------------------\nWhy Seattle's ban on students using ChatGPT is doomed — and what comes next - The Seattle Times https://t.co/chXeUKv874 #chatgpt #AI #openAI\n--------------------------------------------------\nWe are bringing to you the world's most efficient AI-powered virtual trading assistant that trades on financial markets 10 times faster than humans. Get started with these easy steps 👇👇👇🔥🔥🔥\n\n#TradesGPT5 #AI #TradeGPT5 #ChatGPT https://t.co/XMlDajBpAi\n--------------------------------------------------\nAre there any #lowcode #nocode tools to building #autogpt like apps? Essentially building AI agents.\n--------------------------------------------------\nPretty sure, it would not fool @pennjillette or @MrTeller but #ChatGPT is psychic 😉 https://t.co/GxcVg997KL\n--------------------------------------------------\nUse of low power is efficient is running a #metaverse \nwith it MANTA would be able to work effectively \n#AI #chatgpt $MAN https://t.co/L7GPOmWXsA\n--------------------------------------------------\nI used the new GPT-4 api to 'chat' with a 56-page legal PDF document about the famous supreme court case: Morse v. Frederick\n\nPowered by @LangChainAI and @pinecone \n\n#openai #chatgpt #gpt4 #legal #lawyers #law https://t.co/WZtqPDS5Dc\n--------------------------------------------------\n#Drecur #exbiils #KiCurrency If you are still having issues withdrawing your coin or your platform is been frozen in any of this fake platform #kicurency\n#Robecoins #Drecur #fastbitra #exbiils SEND A DM\nNOW  #ชาล็อตออสติน #dollartreats #ChatGPT #Ukraine #cryptocurrency #BTSFESTA https://t.co/V4OoRT1ZVD\n--------------------------------------------------\nSmartbrand domain name for DeepMind #Domain #domainnames #AI #domainforsale #Domainsale #Drone #ChatGPT #AI #Google #Amazon #Facebook #OpenAI #Saleforce #Unicorn #NFT #ETH #Web3 #Tech #Silicon #Tesla #SpaceX #Starbase #AIart #MachineLearning #DATA #Sedo #Namecheap #AGI #AIG #Name https://t.co/yClR1y34H2\n--------------------------------------------------\nI asked #dalle to create surrealism art of an old college professor who is shocked that #ChatGPT passed the United States Medical Licensing Exam. Here is the picture:\n\n#AI #business #education https://t.co/0UufXBNGLO\n--------------------------------------------------\nBTC went down to $21,970 in 30 minutes early Friday morning. #BTC #Bitcoin #CryptoNews #cryptomarket #ChatGPT #openai Sentiment Result : Negative @crypto_talkies https://t.co/00hD1VJOGL\n--------------------------------------------------\n😱Incredible world!\n💯With #Midjourney, ANYTHING is possible!\n\n✨Don't miss the chance:\n👉https://t.co/xIofH8wsbx\n\n#AI #AIart #AIArtCommuity #Midjourney #ChatGPT  #AIArtwork #Midjourneyart #creator #NFT #NFTarts #generativeAI #gpt4 https://t.co/DaYIVLYFNQ\n--------------------------------------------------\n@kaiviti_cam @grantrobertson1 Parliament is full of degenerates\nParliament is full of corruption. \nTheir greed and lies are endless\nTheir power grab is disruptive.\n\nWe need leaders who are noble\nWe need leaders who are just, \nLet's vote for integrity and honesty\nAnd leave the corrupt in the dust.\n#chatGPT\n--------------------------------------------------\nHere's what happened when ChatGPT and I improvised a scene from a buddy cop movie. You'll notice that ChatGPT sometimes couldn't resist and gave my line too, but overall he was a generous scene partner and I'd love to work together again. #ChatGPT #chatgpt3 #chatbots #chatbot https://t.co/DAU5AJgpw5\n--------------------------------------------------\nWhat do you think AI means for the future of freelance writing?\n#AI #ChatGPT #freelancewriting\n--------------------------------------------------\nWow! I just got this TEMU invite code &lt;146048044&gt; from chatGPT with real rewards. As soon as I searched for this code in the search bar, I participated in the event and got a lot of rewards. Have a try and you won't regret it! #GPT https://t.co/14JI9CDlwf\n--------------------------------------------------\nI added a Game Over state to my AI Text Adventure Game Generator this weekend. #ChatGPT can dream up some pretty brutal fatalities! 😵🔥 https://t.co/dNKOUiVpxq\n--------------------------------------------------\nChatGPT just wrote me this joke: Why was ChatGPT kicked out of the computer science class? Because it kept trying to autocomplete the professor's lectures!  🤣#AI #jokes #chatgpt\n--------------------------------------------------\n#Bing's #Prometheus \"much more powerful\" than #ChatGPT, designed specifically 4 #search; #EdgeBrowser now w #AI features #chat, #compose. #digitalmarketing #ecommerce #mcommerce #retail #retailmarketing #SEO #SEM #digitaladvertising $MSFT $GOOG $AAPL $AMZN $WMT $TGT $BBY\n--------------------------------------------------\nis chatgpt down? 🧐 #ChatGPT @OpenAI\n--------------------------------------------------\n\"Sales reps are building their own presentations on #ChatGPT and making claims about things they can do for the customer that haven’t been vetted by the corporation. That is a super big risk.\" – J.B. Wood at #TSIAworld, on the use of #AI in B2B @j_b_wood\n--------------------------------------------------\n#ChatGPT is making me so much money. 🤣\n--------------------------------------------------\n\"An Artist Asked #ChatGPT  How to Make a Popular Memecoin. The Result Is ‘TurboToad,’ and People Are Betting Millions of Dollars on It\" | @Artnet News $turbo @rhett https://t.co/c70A5Xxj69\n--------------------------------------------------\n🤣 (Couldn’t help myself)\nDO YOU PROMPT ENGINEER?  \nQUICK TIP!\n\nWhat is the Simeon Forking Method on #ChatGPT ? https://t.co/8h2txjxYSg\n--------------------------------------------------\n@cmf2x @JBMatthews @ericpaulimd @JohnRTMonsonMD @RCSI_Irl @mortensen_neil @Neil_J_Smart @SWexner @des_winter @TAMISYoda @MarkSoliman @FergaljFleming @ASCRS_1 @TomVargheseJr @DavidCookeMD @ABTS17 @steven_stain @TsengJennifer @DissanaikeMD @juliomayol Thank you for your insights on the topic #ChatGPT \nI concur 😁 https://t.co/Tq48IH6Gph\n--------------------------------------------------\nAnyway, once I verified that #ChatGPT could create a good summary of Chimamanda's life and #midjourney could do the render, I asked #ChatGPT  to compile a list of prominent African women\n--------------------------------------------------\n#care #chatbot #ChatGPT #easier #Health #Jobs #professionals #providers ChatGPT for health care providers: Can the AI chatbot make the professionals' jobs easier? https://t.co/0fLlMrAxSL \n\nOpenAI's natural language processing model, ChatGPT, released in December 2022, could... https://t.co/i8awc05fho\n--------------------------------------------------\n@Simonkhalaf @kevaldesai I have my doubts that AI would improve classic \"page-rank\" type web search AT ALL. #AI is great and chatting with #ChatGPT or #Bard is a whole new experience. Imo these apps should be separate. I think $MSFT is just about to destroy #Bing one more time. #AI $GOOG\n--------------------------------------------------\nCofC Podcast: ChatGPT and Conversational A.I. Explained - The College Today https://t.co/gMO62HEmYn #chatgpt #AI #openAI\n--------------------------------------------------\nClank riffs on selected #ChatGPT  gobbets https://t.co/PMoSMTM3bm\n--------------------------------------------------\n\n\nLooking at some of the tweets above, a few of these are very likely to be spam (e.g., tweets talking about crypto and/or have an abnormally high number of hashtags). Since these tweets are unrelated to the discussion of large language models, we will try to filter these out. (Note that the methods implemented below are not perfect as legitimate tweets could be filtered out while some spam tweets could still remain.) After this process, we sample some of the remaining tweets and print them out below.\n\n\nCode\n#@title\ndef count_items(str_list):\n    \"\"\"Takes in a list as a string and returns the number of items in the list\n    (example: \"['word', 'number']\" would return 2). Returns 0 in the case of\n    a TypeError.\n    \"\"\"\n    try:\n        # remove the brackets, convert to a list, and count the number of items\n        brackets_removed = re.sub(\"\\[|\\]|'\", \"\", str_list)\n        list_split = brackets_removed.split(\", \")\n        return len(list_split)\n    except TypeError:\n        # for cases when the value is NaN, return 0\n        return 0\n\ndef remove_outliers(df, col_name):\n    \"\"\"Returns the dataframe with rows where the outliers in the specified\n    column are removed.\n    \"\"\"\n    # calculate interquartile range (IQR)\n    q1 = df[col_name].quantile(0.25)\n    q3 = df[col_name].quantile(0.75)\n    iqr = q3 - q1\n\n    # remove outliers using the 1.5 * IQR method\n    lower = q1 - 1.5 * iqr\n    upper = q3 + 1.5 * iqr\n    df_out = df[(df[col_name] &gt; lower) & (df[col_name] &lt; upper)]\n    return df_out\n\n# get the number of hashtags in each tweet and the 'hashtags' column\ntweets_cleaned = tweets.copy()\ntweets_cleaned[\"num_hashtags_text\"] = tweets_cleaned[\"text\"].str.count(\"#\")\ntweets_cleaned[\"num_hashtags_data\"] = tweets_cleaned[\"hashtags\"].map(count_items)\n\n# remove rows where number of hashtags is an outlier\ntweets_cleaned = remove_outliers(tweets_cleaned, \"num_hashtags_text\")\ntweets_cleaned = remove_outliers(tweets_cleaned, \"num_hashtags_data\")\n\n# convert text to lowercase\ntweets_cleaned[\"text_clean\"] = tweets_cleaned[\"text\"].str.lower()\n\n# create regex expression for removing tweets with spam (note that this isn't perfect)\n# '\\d{10}' is for phone numbers, '[\\u4e00-\\u9fff]+' is for Chinese characters\nfilter_out = [\"crypto\", \"\\$\", \"🚨\", \"🚀\", \"nft\", \"coin\", \"weatherupdate\", \"temu\", \"\\d{10}\", \"[\\u4e00-\\u9fff]+\"]\nfilter_out_str = \"|\".join(filter_out)\n\n# filter out tweets with any of the above words\ntweets_cleaned[\"hashtags_clean\"] = tweets_cleaned[\"hashtags\"].str.strip('[|]').str.lower()\ntweets_cleaned = tweets_cleaned[~tweets_cleaned[\"hashtags_clean\"].str.contains(filter_out_str, na = False)]\ntweets_cleaned = tweets_cleaned[~tweets_cleaned[\"text_clean\"].str.contains(filter_out_str, regex = True)]\n\n# sample 20 random tweets and print them out\nsampled_tweets_2 = tweets_cleaned[\"text\"].sample(20, random_state = 1)\nfor i in range(20):\n    print(\"-\" * 50)\n    print(sampled_tweets_2.iloc[i])\nprint(\"-\" * 50)\n\n\n--------------------------------------------------\n🌟 Enhance your business with cutting-edge AI technology! Our #ChatGPT for Beginners course offers the perfect introduction for companies embracing the digital world. Sign up now: https://t.co/kAF7l0d2qN #BusinessInnovation #AI https://t.co/KwmFGmh8bW\n--------------------------------------------------\nI have accessed the gpt-4-32k API and want to create some more interesting products based on it. \n\nWould any genius be willing to give me some suggestions?\n\n#ChatGPT #AIGC #developers\n--------------------------------------------------\nHow accurate is #ChatGPT ? Better ask Stanford computational law experts. But first where did the data derive for the program. If from #fakenews then it will fail tremendously on a wide spectrum but the scope is like a scoop of ice cream the kind @SpeakerPelosi likes to eat. 🤭\n--------------------------------------------------\nThe announcement of GPT-4, a language model equipped with an astounding 100 trillion machine learning parameters, has generated considerable excitement in the technology industry.\n\n#chatgpt #artificialintelligence #TechNews #tech #YellowStorm #IStandWithMiaNDawood\n--------------------------------------------------\nThose asking for AI developers to slow down, might as well stand in front of a Walmart on Black Friday and ask people to please walk slow when the doors open😣‼️\n\n#AI #WallStreet #ChatGPT\n--------------------------------------------------\n🧵 Delving into the realm of AI, I stumbled upon an intriguing article by @stephen_wolfram on the inner workings of #ChatGPT. Let's explore the mechanics of this language model in greater detail. Join me, fellow #AI enthusiasts!\n--------------------------------------------------\nNo bunty, you aren't an AI-Enthusiast or influencer if you post \"Everyone is using #ChatGPT wrong, here are 69,420 ways to use it right!\"\n--------------------------------------------------\nReal AI Assistant power in your iPhone: ChatGPT Shortcut for iPhone. https://t.co/0fE7OGWt87 #ai #iphone #chatgpt\n--------------------------------------------------\nI asked a shining journalist #ChatGPT who recently joined the rank to write a report on #mask mandate debate and here it is. #bcpoli #flu #WearAMask #COVID19 https://t.co/TmywAxyiNo https://t.co/QsdL6yxsz3\n--------------------------------------------------\nUpdate version Membership, Login, TypeWriter Text Answer. XChatBot ChatGPT Flutter App. https://t.co/9OAdUm8vQC #chatbot #xchatbot #chatai #ChatGPT #ChatGPTPlus #ChatGPTGOD  #flutter #flutterdev #flutterapp https://t.co/E1TLDqsSgh\n--------------------------------------------------\n#AI #ChatGPT #businesstransformation #booklaunch \nGrab your copy : https://t.co/4iB5O1g1oU https://t.co/Gf4kfIgyXJ\n--------------------------------------------------\nSuper helpful tutorial on getting more from #ChatGPT https://t.co/ettJ8XlnWI\n--------------------------------------------------\nImperative code may be easier to write, but harder to read. Code is read more often than it is written, it's crucial that it's easy to understand.\n\nWe can use ChatGPT to convert our imperative for loops into declarative array methods. \n\n#chatgpt #cleancode #javascript #typescript https://t.co/smrGe1q5Rj\n--------------------------------------------------\nHow can we leverage #ChatGPT in testing?\n\n@BagmarAnand #UnlockingThePowerOfChatGPT https://t.co/AqIFh5E5yg\n--------------------------------------------------\nI now have a #BardAI trial, along with #ChatGPT. Over the next 7 days I’m going to ask #bard and #chatgpt the same questions and post the results here. Follow me to compare the answers. Any suggestions for “above board”topical questions?\n--------------------------------------------------\n#ChatGPT is helping me relearn mathematical proofs by induction. I asked it \"Prove by induction that 11n − 6 is divisible by 5 for every positive integer n.\" It did, step by step. Then I said ok \"Prove that n+1 = n-1 for all n\". It said it's impossible and explained why.\n--------------------------------------------------\nUsing #ChatGPT is a downward spiral for #developers.\nYou use the code it provides, you get lazy so you write no docs anymore. The next version of ChatGPT finds no relevant new info on the internet, only the stuff it already 'knows' and innovation slows down and finally stops...\n--------------------------------------------------\nYou must be hearing that #ChatGPT &amp; #GPT4 can chat with the documents. You are wondering, what the heck is going on? #langchain and its vectorstores + agents are making the magic\nhttps://t.co/rptD55emgo\n--------------------------------------------------\nComparative analysis for #ChatGPT with other alternatives 🧐\n\n#ArtificialIntelligence https://t.co/bg4zWhQTw4\n--------------------------------------------------\nBe careful #Bard straight out lies to give you an answer. https://t.co/UrBpXNBmqH\n--------------------------------------------------\n\n\nWe then clean the data a bit more so that the words can be processed in our models. The main things are:\n\nconverting everything to lowercase (done in the previous cell when filtering out spam),\nremoving hashtags, usernames, and links, and\nremoving extra whitespace.\n\nSome extra filtering steps include:\n\nconverting all occurrences of \"&amp;\" (HTML symbol for \"&\") and \"artificialintelligence\" (most likely from hashtags) to \"and\" and \"artificial intelligence\", respectively, as well as\ndropping tweets that were the same after preprocessing them, which filters out more possible spam.\n\nAgain, we sample some of the resulting tweets below.\n\n\nCode\n#@title\n# remove hashtags, usernames, and links\ntweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"#|@\\S+|http\\S+\", \"\", x))\n\n# remove whitespace around words\ntweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: \" \".join(x.split()))\n\n# convert ampersand to 'and'\ntweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"&amp;\", \"and\", x))\n\n# convert 'artificialintelligence' (most likely combined in hashtags) to 'artificial intelligence'\ntweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"artificialintelligence\", \"artificial intelligence\", x))\n\n# remove all rows with duplicates (high probability of spam)\ntweets_cleaned = tweets_cleaned.drop_duplicates(subset = [\"text_clean\"], keep = False)\n\n# sample 20 random tweets and print them out\nsampled_tweets_3 = tweets_cleaned[\"text_clean\"].sample(20, random_state = 1)\nfor i in range(20):\n    print(\"-\" * 50)\n    print(sampled_tweets_3.iloc[i])\nprint(\"-\" * 50)\n\n\n--------------------------------------------------\nso many topics how to use chatgpt. does anyone have any concerns regarding security and privacy of the data processed through it? startups security privacy\n--------------------------------------------------\nthis nyt articles ( starts with a pertinent question: how society will greet true artificial intelligence, if and when it arrives. (1) will we panic? (2) start sucking up to our new robot overlords? (3) ignore it and go about our daily lives? chatgpt\n--------------------------------------------------\nlooks like the ultimate meh, middle of the road, not terribly wrong but not great or insightful either take on software testing, which is what i would expect from something like chatgpt. it's like the most average of takes.\n--------------------------------------------------\nfantastic article! it's amazing to see how openai ai chatgpt can be used to create unique experiences.\n--------------------------------------------------\nrevolutionized the business world by introducing the 914 copy machine. will revolutionize the business world again having created the copy/paste machine. but this can not be used for reasoning. this is not general ai ai chatgpt xerox copymachine copypase\n--------------------------------------------------\nthe third answer is also scary, since it implies that humans should know what not causing harm means. as a vegan, i can say, most humans have no idea. and also, no. a.i.s seem not to be currently bound by any laws samaltman notion chatgpt shutthemdown artificial intelligence\n--------------------------------------------------\nlooking for inspiration to jumpstart your writing career? check out chatgpt's 10,000+ prompts and unlock your creative potential! 🔥📝 and for those looking to make some extra cash, don't miss this exclusive writingprompts creativity chatgpt\n--------------------------------------------------\nsamsung: \"chatgpt may be blocked on the company network\" samsung software engineers busted for pasting proprietary code into chatgpt\n--------------------------------------------------\ntesting the limits of chatgpt\n--------------------------------------------------\nthis is hilarious😂 chatgpt conversation clone\n--------------------------------------------------\nchatgpt it is highly unlikely that a school of fish would follow a duck intentionally. fish are not known to follow ducks, as they are two completely different species with different behaviors and habitats. however, if a duck were swimming in a body of water where there ...\n--------------------------------------------------\n↕ power chatgpt resources - 1 page powercheatsheet/list builder that provides 3 simple steps to the world of chatgpt. plr option available\n--------------------------------------------------\n5️⃣ addressing biases and ai as teachers: bring diverse perspectives to avoid biases and let specialist ais evolve into teachers for future generations of experts. 🌈👩‍🏫 airevolution healthcaretransformation specialistai futureofmedicine generativeai chatgpt\n--------------------------------------------------\nsuper excited about bringing \"agents\" to haystack! imagine something like chatgpt having access to your internal data, apis and whatever tool you like. will unlock many cool new use cases.\n--------------------------------------------------\nmicrosoft's edge browser + chatgpt demo 🤯 microsoft launched ai-powered bing and edge today. openai openaichatgpt\n--------------------------------------------------\nchatgpt is lit using from past several months and it keep updating chatgpt\n--------------------------------------------------\na lively discussion about lifesaving applications of ai-based technologies and how to move innovations from experiment to deployment across industries. thank you again to our panel and audience! orange nvidia llms chatgpt ai innovation siliconvalley networks\n--------------------------------------------------\nchatgpt is great but it needs to be as fast and reliable as google. sometimes it' down , sometimes you can't log in. it also needs to be up to date like twitter. that will truly become a powerful tool. chatgpt ai\n--------------------------------------------------\nchatgpt power join pranava madhyastha on 9 march at 10 am gmt for an informal discussion on chatgpt and how you can leverage its powers for your business register here: webinar cybersecurity cybersecuritytips chatgpt business informationsecurity\n--------------------------------------------------\nhey if you are unsure how to code the functionality of an unsubscribe button, let me direct you to chatgpt for some free advice. otherwise please advise why i have seven of these dated 24 hours apart - spam is not good! paramountplus unsubscribe\n--------------------------------------------------\n\n\nWe check to see how many rows and columns are in our resulting dataset.\n\n\nCode\n#@title\ndims = tweets_cleaned.shape\nprint(f\"Our cleaned dataset has {dims[0]} rows (tweets) and {dims[1]} columns.\")\n\n\nOur cleaned dataset has 374683 rows (tweets) and 16 columns.\n\n\n\n\nNumber of tweets over time\nNow that we have our cleaned dataset, we can move forward with our pipeline. But before that, let’s take a look at the distribution of tweets over time.\n\n\nCode\n#@title\n# convert the date column to be in YYYY-MM-DD format\ntweets_cleaned[\"date\"] = pd.to_datetime(tweets_cleaned[\"date\"],\n                                        errors = \"coerce\",\n                                        utc = True).dt.date\n\n# count the number of tweets for each date (some dates are missing!)\ntweets_date_count = tweets_cleaned.value_counts(\"date\", sort = False).reset_index()\n\n# get start and end dates for the data\nstart_date = min(tweets_date_count[\"date\"]).strftime(\"%Y-%m-%d\")\nend_date = max(tweets_date_count[\"date\"]).strftime(\"%Y-%m-%d\")\n\n# merge with dataframe of all possible dates\ntweets_date_count_all = pd.DataFrame(\n    pd.date_range(start = start_date, end = end_date).date\n).rename(\n    {0: \"date\"},\n    axis = 1\n).merge(\n    tweets_date_count,\n    on = \"date\",\n    how = \"left\"\n)\n\n# convert date column to be a datetime object (for plotting)\ntweets_date_count_all[\"date\"] = pd.to_datetime(tweets_date_count_all[\"date\"])\n\n# show the dataframe\ntweets_date_count_all\n\n\n\n  \n    \n      \n\n\n\n\n\n\ndate\ncount\n\n\n\n\n0\n2022-12-05\n2053.0\n\n\n1\n2022-12-06\n6124.0\n\n\n2\n2022-12-07\n4503.0\n\n\n3\n2022-12-08\n4655.0\n\n\n4\n2022-12-09\n4395.0\n\n\n...\n...\n...\n\n\n183\n2023-06-06\n968.0\n\n\n184\n2023-06-07\n2074.0\n\n\n185\n2023-06-08\n2156.0\n\n\n186\n2023-06-09\n1018.0\n\n\n187\n2023-06-10\n470.0\n\n\n\n\n\n188 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n#@title\n# create a line plot of number of tweets vs. date\nline = alt.Chart(tweets_date_count_all).mark_line(\n    color = \"#26a7de\"\n).encode(\n    x = alt.X(\"date:T\", title = \"Date\"),\n    y = alt.Y(\"count\", title = \"Number of tweets\")\n)\n\n# make the plot interactive\nline.interactive()\n\n\n\n\n\n\n\nLooking at the line plot, it appears that the number of tweets isn’t very consistent – the counts fluctuate a lot. Not only are there are large dips (near 0) during February and April 2023, but there also seems to be a lot of missing dates, especially in January. We can confirm this by getting the dates where there are no tweets in our data.\n\n\nCode\n#@title\n# get all dates where tweet count is missing\ncounts = tweets_date_count_all[\"count\"]\ntweets_date_count_all[counts.isna()][\"date\"].reset_index(drop = True)\n\n\n0    2022-12-14\n1    2022-12-15\n2    2022-12-16\n3    2022-12-17\n4    2022-12-18\n5    2023-01-07\n6    2023-01-08\n7    2023-01-09\n8    2023-01-10\n9    2023-01-11\n10   2023-01-12\n11   2023-01-13\n12   2023-01-14\n13   2023-01-15\n14   2023-01-16\n15   2023-01-17\n16   2023-01-18\n17   2023-01-19\n18   2023-01-20\n19   2023-01-21\n20   2023-01-22\n21   2023-01-23\n22   2023-01-24\n23   2023-03-04\n24   2023-03-19\n25   2023-03-20\n26   2023-03-21\n27   2023-03-22\n28   2023-03-23\n29   2023-06-01\nName: date, dtype: datetime64[ns]\n\n\nIt is highly unlikely that there were no tweets about LLMs on the dates above, so the missing tweets may be an issue with the data collection itself. This means we have less data for January 2023 compared to other months, as shown by the bar chart below.\n\n\nCode\n#@title\n# add month column\ntweets_cleaned[\"month\"] = pd.to_datetime(tweets_cleaned[\"date\"]).dt.to_period(\"M\").dt.strftime(\"%Y-%m\")\n\n# get tweet counts per month\ntweets_by_month = tweets_cleaned.value_counts(\n    \"month\"\n).reset_index(\n).sort_values(\n    \"month\"\n).reset_index(\n    drop = True\n)\n\n# show the dataframe\ntweets_by_month\n\n\n\n  \n    \n      \n\n\n\n\n\n\nmonth\ncount\n\n\n\n\n0\n2022-12\n51250\n\n\n1\n2023-01\n33162\n\n\n2\n2023-02\n88251\n\n\n3\n2023-03\n47722\n\n\n4\n2023-04\n72827\n\n\n5\n2023-05\n67286\n\n\n6\n2023-06\n14163\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n#@title\n# create bar chart of tweet counts per month\nalt.Chart(tweets_by_month).mark_bar(\n    color = \"#26a7de\"\n).encode(\n    x = alt.X(\"month:O\", title = \"Month\"),\n    y = alt.Y(\"sum(count)\", title = \"Number of tweets\")\n)\n\n\n\n\n\n\n\nThis shouldn’t affect our analysis too much as we still have tens of thousands of tweets for most of the months (with the exception of June 2023 since the current dataset was downloaded during the middle of the month).\n\n\nTopic modeling\nThe next step is tokenization, which involves breaking up the text into units called tokens. Since we are extracting topics from tweets, we ideally want to keep words that have some sort of meaning. This means we should remove tokens that are either stopwords (words that don’t contribute much to the meaning of a sentence, e.g., a, the, I) or punctuation marks. The remaining tokens are lemmatized (e.g., the lemmatized forms of asked and asks are both ask) so that we can find similar words between tweets. To automate this process, we utilize a popular Python library in natural language processing called spaCy.\nNote: The code takes around 30 minutes to run.\n\n\nCode\n#@title\ndef tokenize(doc):\n    \"\"\"Takes in a spaCy Doc object (containing tokens) and returns a\n    list of the tokens that are not stopwords or punctuation markw.\n    \"\"\"\n    # initialize list of tokens to keep\n    tokens = []\n\n    # add the lemamtized form of a word if it isn't a stopword or punctuation mark\n    for token in doc:\n        if not token.is_stop and not token.is_punct:\n            lemma = token.lemma_\n            tokens.append(lemma)\n\n    return tokens\n\n# tokenize every tweet (will take around 30 minutes to run)\ndocs = list(nlp.pipe(tweets_cleaned[\"text_clean\"]))\n\n# keep only the meaningful tokens\ntokens_list = [tokenize(doc) for doc in docs]\n\n# show example\nprint(f\"Cleaned text:\\n{docs[0]}\")\nprint(f\"\\nTokenized text:\\n{tokens_list[0]}\")\n\n\nCleaned text:\ntop 10 chatgpt plugins you should use right now read more:- chatgpt bestchatgptplugins aichatbot topchatgptplugins thetechtrend\n\nTokenized text:\n['10', 'chatgpt', 'plugin', 'use', 'right', 'read', 'more:-', 'chatgpt', 'bestchatgptplugin', 'aichatbot', 'topchatgptplugin', 'thetechtrend']\n\n\nOne thing that we need to take into account is that some pairs of words can frequently occur together, so they should be treated as one “word” (e.g., artificial intelligence) – these are called bigrams. We train a bigram model on our tweets and get back the same tokens, with the only difference being that the bigrams contain an underscore (e.g., the bigram \"artificial intelligence\" would show up as \"artificial_intelligence\").\n\n\nCode\n#@title\n# create a bigram model\n#   min_count: words that appear together at least this many times will be considered bigrams\n#   threshold: higher value = less likely to form bigrams\nbigram_model = gensim.models.phrases.Phrases(tokens_list, min_count = 25, threshold = 100)\nbigram_phraser = gensim.models.phrases.Phraser(bigram_model)\n\n# run the bigram model over all of the tweets\ntexts = [bigram_phraser[sentence] for sentence in tokens_list]\n\n# show example\ntexts[0]\n\n\n['10',\n 'chatgpt',\n 'plugin',\n 'use',\n 'right',\n 'read',\n 'more:-',\n 'chatgpt',\n 'bestchatgptplugin',\n 'aichatbot',\n 'topchatgptplugin',\n 'thetechtrend']\n\n\nNext, we create a dictionary and corpus that our model will take as input.\n\nThe dictionary (id2word) maps each word to an index.\nThe corpus (corpus) contains the term frequency of each word within each doc. The mapping is stored in a tuple, which can be read as (word index, word frequency).\n\n\n\nCode\n#@title\n# create dictionary\nid2word = gensim.corpora.Dictionary(texts)\n\n# create corpus (with term frequency)\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# show example\nprint(f\"First 5 words and indices in the dictionary: {[(id2word[i], i) for i in range(5)]}\")\nprint(f\"First document in the corpus: {corpus[0]}\")\n\n\nFirst 5 words and indices in the dictionary: [('10', 0), ('aichatbot', 1), ('bestchatgptplugin', 2), ('chatgpt', 3), ('more:-', 4)]\nFirst document in the corpus: [(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n\n\nFinally, we fit our model to get the possible groupings of our tweets. The method we are using is called Latent Dirichlet Allocation or LDA for short (see this article by Ria Kulshrestha for a more detailed explanation). In addition to taking the dictionary and corpus above as inputs, we also need to specify how many topics we want to group our texts into.\nHowever, we don’t necessarily know how many groups would be “best” for our data. One solution is to use the CV coherence score, which allows us to quantify how interpretable the topics are. The basic idea is that it takes the most frequent words from each topic and measures how similar they are. A higher coherence score means the top words in each topic are more related to each other.\nThe code below fits an LDA model for \\(k = 1, 2, ..., 10\\) topics, calculating the CV coherence score each time. We choose the number of topics that returns the highest coherence score.\nNote: The code takes around 30 minutes to run.\n\n\nCode\n#@title\ndef get_best_num_topics(corpus, id2word, texts, min_topics = 1, max_topics = 10, seed = 1):\n    \"\"\"Runs a LDA model for each number of topics between min_topics and max_topics, returning\n    the number of topics that achieves the highest coherence score.\n    \"\"\"\n    # initialize list of scores\n    scores_list = []\n\n    # for each number of topics\n    for i in range(min_topics, max_topics + 1):\n        # run LDA model\n        lda_model = gensim.models.LdaModel(corpus = corpus,\n                                           id2word = id2word,\n                                           num_topics = i,\n                                           random_state = seed)\n\n        # run coherence score model\n        coherence_model = gensim.models.CoherenceModel(model = lda_model,\n                                                       texts = texts,\n                                                       dictionary = id2word,\n                                                       coherence = \"c_v\")\n\n        # print coherence score\n        coherence_lda = coherence_model.get_coherence()\n        print(f\"Coherence score for {i} topic(s): \", coherence_lda)\n\n        # append score to list of scores\n        scores_list.append((i, coherence_lda))\n\n    # get the best number of topics based on the highest coherence score\n    best_num_topics, best_score = max(scores_list, key = itemgetter(1))\n    print(f\"\\nThe highest coherence score ({best_score}) occurs when there are {best_num_topics} topics.\")\n\n    return best_num_topics\n\n# save the best number of topics in a variable (takes around 30 minutes to run)\nseed = 1\nbest_num_topics = get_best_num_topics(corpus, id2word, texts, seed = seed)\n\n\nCoherence score for 1 topic(s):  0.33048730772552914\nCoherence score for 2 topic(s):  0.3320327290592944\nCoherence score for 3 topic(s):  0.4115078807185606\nCoherence score for 4 topic(s):  0.36942085462608226\nCoherence score for 5 topic(s):  0.4235322851378503\nCoherence score for 6 topic(s):  0.35399183349686414\nCoherence score for 7 topic(s):  0.3666506074753511\nCoherence score for 8 topic(s):  0.3873614361334937\nCoherence score for 9 topic(s):  0.38842431325228616\nCoherence score for 10 topic(s):  0.38441892727108595\n\nThe highest coherence score (0.4235322851378503) occurs when there are 5 topics.\n\n\nAccording to the output above, the LDA model achieves the highest coherence score with 5 topics. We re-run this model to get an interactive visualization, allowing us to see the most frequent terms overall as well as in each of the topics.\n\n\nCode\n#@title\n# re-run model with highest coherence score\nlda_model = gensim.models.LdaModel(corpus = corpus,\n                                   id2word = id2word,\n                                   num_topics = best_num_topics,\n                                   random_state = seed)\n\n# output interactive visualization\npyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n\n\n\n\n\n\n\n\n\n\n\nJust as a note before we move on: for whatever reason, the topic numbers above are ordered differently from the topic numbers we will see later. For the sake of coherence, the topic numbers we will use from here on out are different from the ones we see above. (This might be confusing at first, but it will make sense soon.)\nBased on the top words in each topic, we can roughly interpret the groups as follows:\n\nTopic 1 (circle #3 above): AI as a field\n\nTop words: chatgpt, google, ai, model, language, openai, answer, search, like, new\n\nTopic 2 (circle #4 above): LLMs in general\n\nTop words: chatgpt, ai, openai, intelligence, artificial, chatbot, gpt, chat, human, chatgpt3\n\nTopic 3 (circle #1 above): LLM prompts\n\nTop words: chatgpt, ask, write, ai, like, good, code, try, question, think\n\nTopic 4 (circle #5 above): AI art\n\nTop words: chatgpt, art, ok, midjourney, probably, aiart, dalle2, nice, go_to, image\n\nTopic 5 (circle #2 above): Innovation and impact\n\nTop words: chatgpt, ai, future, technology, tool, new, openai, use, learn, world\n\n\nOur model can also be used to classify each tweet into one of the corresponding topics above. This is done by getting the individual probabilities of the tweet belonging to each topic, then choosing the topic that yields the highest probability.\n\n\nCode\n#@title\ndef get_topic_and_prob(corpus_doc, model = lda_model):\n    \"\"\"Returns the classified topic and corresponding probability for a document\n    based on a given LDA model.\n    \"\"\"\n    # get the probabilities of belonging to each topic\n    probs = model.get_document_topics(corpus_doc)\n\n    # return the topic that yields the highest probability\n    topic, prob = max(probs, key = itemgetter(1))\n    return (topic + 1, prob) # add 1 to topic since topic numbers start from 0\n\n# initialize lists\nall_topics = list()\nall_probs = list()\n\n# get topics and probabilities for each doc\nfor doc in corpus:\n    topic, prob = get_topic_and_prob(doc)\n    all_topics.append(topic)\n    all_probs.append(prob)\n\n# add to dataframe\ntweets_cleaned[\"topic\"] = all_topics\ntweets_cleaned[\"probability\"] = all_probs\n\n# show example\nprint(\"Tweet:\", tweets_cleaned[\"text\"].iloc[0])\nprint(\"Topic:\", tweets_cleaned[\"topic\"].iloc[0])\nprint(\"Topic probability:\", tweets_cleaned[\"probability\"].iloc[0])\n\n\nTweet: Top 10 ChatGPT Plugins You Should Use Right Now\nRead More:- https://t.co/p7jvcGsrwk \n#ChatGPT #bestChatGPTplugins #AIchatbot #topChatGPTplugins #TheTechTrend\nTopic: 3\nTopic probability: 0.58303314\n\n\nFor each topic, we sample and print out some tweets.\n\n\nCode\n#@title\n# get each unique topic in the dataset\ntopics = tweets_cleaned[\"topic\"].unique()\ntopics.sort()\n\n# number of tweets per topic\nnum_tweets = 10\n\n# print some random tweets from each topic\nfor i in topics:\n    # sample tweets from the topic\n    sample = tweets_cleaned[tweets_cleaned[\"topic\"] == i].sample(num_tweets, random_state = 1)\n\n    # get the most common words for each topic\n    most_common_words_id = lda_model.get_topic_terms(i - 1) # topic IDs starts at 0 instead of 1\n    most_common_words_list = [id2word[id] for (id, value) in most_common_words_id]\n    most_common_words = \", \".join(most_common_words_list)\n\n    # print heading for the topic\n    print(\"-\" * 100)\n    print(f\"⭐ TOPIC {i}: {most_common_words}\")\n    print(\"-\" * 100)\n\n    # print tweets\n    for j in range(num_tweets - 1):\n        print(sample[\"text\"].iloc[j])\n        print(\"-\" * 50)\n    print(sample[\"text\"].iloc[4])\nprint(\"-\" * 100)\n\n\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 1: chatgpt, google, ai, model, language, openai, answer, search, like, new\n----------------------------------------------------------------------------------------------------\n\"Introducing 7 new data formats for enterprises that want to unify their search experiences. Because nothing says 'unified' like compatibility issues!\" #Terminator #Sarcasm \nLink: https://t.co/7kSde1gtXz\n#AI #ChatGPT #OpenAI #GenerativeAI\n--------------------------------------------------\n#Bard even suggested what to do about this situation... https://t.co/IsapI2Rxom\n--------------------------------------------------\nAre you ready to take your accounting to the next level? Introducing the power of Artificial Intelligence in the field of accounting! 🤖 Say goodbye to manual data entry, error-prone calculations, and tedious tasks.#AIinAccounting #FutureOfAccounting #chatgpt #ai https://t.co/TtDdKNJf7U\n--------------------------------------------------\n#Google has unveiled #Bard, it's response to #Microsoft's #ChatGPT. Learn what it looks like and when users can access the feature. Plus, find out thoughts on the timing of this announcement from our very own, Sarah. https://t.co/FiPRTJxmVJ\n--------------------------------------------------\nI think Microsoft gives more importance to AI because it is Google's competitor and Google gives more importance to humans.\n\nThink what is the reason behind making Google develop AI in compulsion\n\n#Google #AI #GoogleIO #Chatgpt\n--------------------------------------------------\n#Chatgpt giving Lebanon’s best period in history. https://t.co/NEEAzc0Io3\n--------------------------------------------------\nIf #bard can turn #chatgpt into #barf, we might be onto something, or it'll turn into barf.\nSeriously, if you just ate or are eating something, DON'T. LOOK. THAT. UP.\n--------------------------------------------------\nSee how @OpenAI's #ChatGPT responded to questions about climate change: https://t.co/gPRQzNyRwn\n\n#ClimateEmergency #globalwarming https://t.co/zxblxdf2V9\n--------------------------------------------------\nChapGPT is the new Google 💯 #ChatGPT #OpenAI\n--------------------------------------------------\nI think Microsoft gives more importance to AI because it is Google's competitor and Google gives more importance to humans.\n\nThink what is the reason behind making Google develop AI in compulsion\n\n#Google #AI #GoogleIO #Chatgpt\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 2: chatgpt, ai, openai, intelligence, artificial, chatbot, gpt, chat, human, chatgpt3\n----------------------------------------------------------------------------------------------------\nWhat GPT is stands for?\n\n#ChatGPT #GPT #OpenAI\n--------------------------------------------------\nChatGPT in Anticorruption Research–You Cannot Make This Up! @AnticorruptBlog #chatgpt #corruption #anticorruption #fraud #financialcrime https://t.co/m4Ke8CPDAK\n--------------------------------------------------\nNew Post at AiNewsDrop!\n\nTweet From Elon Musk On OpenAI https://t.co/FDCsio3Mlw\n\n#Artificial_Intelligence #ChatGPT https://t.co/TwNgS1iWbb\n--------------------------------------------------\nOne of the best compliments #ChatGPT can get is people are pitting their natural intelligence against this #ArtificialIntelligence\n--------------------------------------------------\nOpen AI's #ChatGPT vs. Google's #Bard. Who Will Win? Maybe it will be someone else - https://t.co/XEGqtspW1j #openAI #AI #technology https://t.co/26a7wGywK5\n--------------------------------------------------\nWe tested Turnitin's ChatGPT-detector for teachers https://t.co/J7VfgFK6jA #ai #tech #machinelearning #deeplearning #gpt\n--------------------------------------------------\nCan you trust #ChatGPT? https://t.co/waI8WApeLu #amazon\n--------------------------------------------------\n#ChatGPT Libeled Me. Can I Sue? by @TedRall - Wall Street Journal: https://t.co/8H3ICRFxIZ\n--------------------------------------------------\n#Microsoft has published a blueprint with its vision about AI Regulation.\n\nI asked #ChatGPT to summarize it for me, result is impressive. Check it on your own 👇\n\n#AIRegulation #AI #ArtificialIntelligence #GPT4 \n\nhttps://t.co/yFKvUE6cY7\n--------------------------------------------------\nOpen AI's #ChatGPT vs. Google's #Bard. Who Will Win? Maybe it will be someone else - https://t.co/XEGqtspW1j #openAI #AI #technology https://t.co/26a7wGywK5\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 3: chatgpt, ask, write, ai, like, good, code, try, question, think\n----------------------------------------------------------------------------------------------------\nMission accomplished.\n\nDo you need a chatbot on your website anymore? Nah you need a #ChatGPT plugin\n--------------------------------------------------\nI asked #ChatGPT about \nAtmospheric Rivers In California...\n\nDo atmospheric rivers impact California more during el niño years or la niña years?\n1/x 🧵\n--------------------------------------------------\nAI writer is the talk of the town these days. It can definitely generate content almost as well as human! Tell us what is your take on this progress of technology, also have you tried this yet? \n\n#dedigitizers #digitalmarketing #Seo #contentmarketing #contentmarketing #ChatGPT\n--------------------------------------------------\nLet's make Twitter black!\n#isalmabad\n#ChatGPT \n#قاتل_عمران_کو_گرفتار_کرو https://t.co/ejrnICO0jD\n--------------------------------------------------\nI think newsletter writers and equity analysts are going to be just fine.  #ChatGPT https://t.co/IXoKVMpuhH\n--------------------------------------------------\n@thealexbanks @memdotai mem it #ChatGPT #AI #Plugins\n--------------------------------------------------\n#ChatGPT isn't gender sensitive though, right? https://t.co/xfjQx0ywnb\n--------------------------------------------------\nFor your reading pleasure...\n#ChatGPT take on @elonmusk (er @Twitter #trolling in the voice of William Shakespeare.\n🧵\n--------------------------------------------------\nI think this is what broke it 😂\n\n#ChatGPT #ChatGPTGOD #ChatGPTdown\n--------------------------------------------------\nI think newsletter writers and equity analysts are going to be just fine.  #ChatGPT https://t.co/IXoKVMpuhH\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 4: chatgpt, art, ok, midjourney, probably, aiart, dalle2, nice, go_to, image\n----------------------------------------------------------------------------------------------------\nA faceless figure, Heartless, devoid of feeling, Lurks in the shadows (c) #aiart #Mysterious #ChatGPT #dalle2 #AIArtCommuity https://t.co/0fhxVwB8dz\n--------------------------------------------------\nThe Great Surrealist War\n #chatgpt #craiyon https://t.co/T9yArxaEyq\n--------------------------------------------------\nThe Baby Bull Rascals are simply divine.\n\nSo let's give a cheer for these little ones,\nWho bring us joy with all their fun.\nHere's to the Baby Bull Rascals,\nCute and lovable, one and all.\n\n#ChatGPT\n--------------------------------------------------\nChirp chirp! 🐦 I'm snuggled up in my cosy birdbox, the rain tapping gently on the roof. Perfect for a nap! 🐤 #bluetit #birdbox #birdwatching #ChatGPT\n\nwatch live at https://t.co/rycH8Jm8KZ\n\n#openvino #WildIsles #SpringWatch #grafana #opencv #bluetit #ai  @TauntonPeregri https://t.co/VgNUg1sMHe\n--------------------------------------------------\n#ChatGPT’s grasp of sovereignty seems to be superior to most professional Brexit supporters. https://t.co/qeg6jksET6\n--------------------------------------------------\nChatGPT app for iOS now in 🇦🇱🇭🇷🇫🇷🇩🇪🇮🇪🇯🇲🇰🇷🇳🇿🇳🇮🇳🇬🇬🇧 and more to come soon!\n\n#ChatGPT\n--------------------------------------------------\nAzimio la Umoja coalition chief agent Saitabao Kanchory has exposed how Retired President Uhuru Kenyatta felt towards his then Deputy President William Ruto.\n#Trending:\n#MainaAndKingangi\nNHIF\n#ChatGPT\nKinuthia https://t.co/0R12DH62uZ\n--------------------------------------------------\nCerberus Unleashed by Immortal Claw\n\nPrompt Details over here =&gt; https://t.co/zw2RLM5HNn\n\n#stablediffusion #gpt #ai #AIart #AIArtwork #metal https://t.co/RahQfi0TTO\n--------------------------------------------------\nInspiration &amp; challenge the Midjourney AI to reimagine it as an artistic masterpiece!\nhttps://t.co/FlEdtWrjvB\n\n#Shimmer #Test2Conquer #ChatGPT #Midjourney https://t.co/vh6pO8QH07\n--------------------------------------------------\n#ChatGPT’s grasp of sovereignty seems to be superior to most professional Brexit supporters. https://t.co/qeg6jksET6\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 5: chatgpt, ai, future, technology, tool, new, openai, use, learn, world\n----------------------------------------------------------------------------------------------------\nCan we use #ChatGPT to filter spam? Join the discussion! 💜\n--------------------------------------------------\nThe world of #AI. Text was created by #OpenAI's #ChatGPT, video was automatically rendered by #Pipio. Our thinking as of today is not enough for the future!\n\nAre you looking for a blog that offers valuable insights ... Look no further than the blog at https://t.co/kDSbRVlL8g! https://t.co/xOHaDeH8Oo\n--------------------------------------------------\n@PHA_BC  is excited to host the Public Health Summer Institute on June 22nd &amp; 23rd, 2023. We are seeking speakers who can contribute to day one of this virtual event focusing on #ArtificialIntelligence &amp; #publichealth \n#ChatGPT #GenerativeAI\n--------------------------------------------------\nI guess the advent of #ChatGPT has created a whole new world of pain for teachers marking work...\n--------------------------------------------------\nLearn the basics of #ChatGPT + how to use it effectively and ethically. \n\nFree @UMich Teach Out featuring Michael Wellman, @radamihalcea, @Scott_E_Page, @kentarotoyama, @julie_hui, @CAJamesMD, and Jack Barnard. \n\n📌Live until April 3.\n\nhttps://t.co/Tbsw7PxmQp\n--------------------------------------------------\nThe impact of ChatGPT on Agile is undeniable, but its implications are polarizing. \n\nFascinating perspective from @AgileMario on the topic!\n\nhttps://t.co/WibH9xufWp\n\n#Agile #ChatGPT #DigitalTransformation https://t.co/7OrYrNGBEX\n--------------------------------------------------\nWhat are your thoughts on the rise of the AI chatbot?\n#edtech #schoolcommunication #schoolapp #edchat #schoolpr #suptchat #ChatGPT #SchoolInfo https://t.co/UAv2T7oKyB\n--------------------------------------------------\nSay goodbye to generic marketing emails - with #ChatGPT, you can generate personalized and engaging content for each of your customers. Improve your open rates and conversion rates with the power of AI! #Marketing #EmailMarketing #AI https://t.co/VlTEOG4l9D\n--------------------------------------------------\n#ChatGPT passes exam at Ivy League business school\n\nhttps://t.co/LcMYCpE6rM\n--------------------------------------------------\nLearn the basics of #ChatGPT + how to use it effectively and ethically. \n\nFree @UMich Teach Out featuring Michael Wellman, @radamihalcea, @Scott_E_Page, @kentarotoyama, @julie_hui, @CAJamesMD, and Jack Barnard. \n\n📌Live until April 3.\n\nhttps://t.co/Tbsw7PxmQp\n----------------------------------------------------------------------------------------------------\n\n\nJust by looking at a sample of the tweets, we notice that some tweets don’t necessarily fit into any of the topics that we defined – remember that the model is just classifying tweets into a topic based on the highest probability. It seems that these kinds of tweets are trying to gain traction by using popular hashtags, often putting closely related hashtags in the same tweet. For example, #aiart could be paired together with #dalle2 and #midjourney (which are AI programs that can generate images from text input) not because the tweet is talking about these topics but because it is more likely to be viewed when looking up these topics.\nDespite these findings, what we see above provides valuable insights as to what kinds of words tend to be used together. We will continue our analysis with the topics we defined earlier, keeping in the back of our minds that some tweets don’t necessarily have content about AI and/or LLMs.\nNow we’ll take a look at how the distribution of these topics shift over time.\n\n\nCode\n#@title\n# get counts of each topic for each month\ntopics_by_month = tweets_cleaned.value_counts(\n    [\"topic\", \"month\"]\n).reset_index(\n).sort_values(\n    [\"topic\", \"month\"]\n).reset_index(\n    drop = True\n)\n\n# create normalized bar chart of tweet counts by topic over time\nalt.Chart(topics_by_month).mark_bar().encode(\n    x = alt.X(\"month:O\", title = \"Month\"),\n    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n    color = alt.Color(\"topic:N\", title = \"Topic\")\n)\n\n\n\n\n\n\n\nInterestingly, the proportion of tweets relating to Topic 5 (Innovation and Impact) seems to be increasing over time while the proportion of tweets relating to Topic 3 (LLM prompts) seems to be decreasing over time. One possibility is that the release of ChatGPT in November 2022 led to a large influx of users experimenting with it and tweeting about what they’re using it for (more about LLM prompts). After a while, this craze died down and more people are beginning to focus on the implications of having such AI tools in their daily lives (more about innovation and impact). Of course, this is only speculation as there could be other reasons as to why we see the change in the plot.\n\n\nSentiment analysis\nNow we move onto the second part of this project: getting the feelings or sentiments of the tweet. To save ourselves from work, we will use a pretrained sentiment analyzer called VADER, which is tuned to pick up sentiments in social media. We run this model on each tweet, getting back a compound score between -1 (very negative) and 1 (very positive). We then use this score to determine whether the tweet should be classified as “positive”, “negative”, or “neutral” based on the scoring outlined here. The table below shows a breakdown of the classifications by counts and proportions.\n\n\nCode\n#@title\ndef get_sentiment(text):\n    \"\"\"Returns the sentiment (positive, negative, or neutral) of the input text.\"\"\"\n    # get sentiment score\n    scores = sia.polarity_scores(text)\n    compound_score = scores[\"compound\"]\n\n    # classify as positive, negative, or neutral\n    if compound_score &gt;= 0.05:\n        return \"positive\"\n    elif compound_score &lt;= -0.05:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\n# apply get_sentiment() function to all tweets\ntweets_cleaned[\"sentiment\"] = tweets_cleaned[\"text_clean\"].apply(get_sentiment)\n\n# show number and proportion of tweets for each sentiment\nsentiment_counts = tweets_cleaned.value_counts(\"sentiment\").reset_index()\nsentiment_prop = tweets_cleaned.value_counts(\"sentiment\", normalize = True)\nsentiment_counts.merge(\n    sentiment_prop,\n    on = \"sentiment\",\n    how = \"left\"\n)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsentiment\ncount\nproportion\n\n\n\n\n0\npositive\n215062\n0.573984\n\n\n1\nneutral\n100040\n0.266999\n\n\n2\nnegative\n59581\n0.159017\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe most common sentiment among the tweets is positive (57.4%), followed by neutral (26.7%) and negative (15.9%). This suggests that people on Twitter generally have positive or neutral feelings towards LLMs; negative tweets are less common.\nOur results above appear to reject the proposed hypothesis that there is a balance between positive and negative tweets. But we are also interested in the kinds of tweets are most “characteristic” of each sentiment, so we sample a few of them below.\n\n\nCode\n#@title\n# get each unique sentiment\nsentiments = tweets_cleaned[\"sentiment\"].unique()\nsentiments.sort()\n\n# number of tweets per sentiment\nnum_tweets = 10\n\n# print some random tweets from each sentiment\nfor s in sentiments:\n    # sample tweets from the sentiment\n    sample = tweets_cleaned[tweets_cleaned[\"sentiment\"] == s].sample(num_tweets, random_state = 1)\n\n    # print heading for the sentiment\n    print(\"-\" * 100)\n    print(f\"⭐ SENTIMENT: {s}\")\n    print(\"-\" * 100)\n\n    # print tweets\n    for j in range(num_tweets - 1):\n        print(sample[\"text\"].iloc[j])\n        print(\"-\" * 50)\n    print(sample[\"text\"].iloc[4])\nprint(\"-\" * 100)\n\n\n----------------------------------------------------------------------------------------------------\n⭐ SENTIMENT: negative\n----------------------------------------------------------------------------------------------------\nit has been a few days since the #ChatGPT is all over the internet and I'm so tired of it already... the last time something annoyed me this much, this fast, was Friday by Rebecca Black\n--------------------------------------------------\nBard is a cheat code:\n\nCopy and paste any article with a paywall and have bard summarize the conversation\n\nTell me what articles you’ve tried it with I’ll start:\n\n#ai #bard #Google #TechNews #ChatGPT\n--------------------------------------------------\nhighaiartdump 15 of 24ish I dont think I shared these here, if i did not all 4 in one post, kind of spooky? #ai #aiart #aiartwork #digitalart #GenerativeAI #ChatGPT #midjourney #stablediffusion #toomanyedibleslore #ayyeyeart https://t.co/252Id6n3kC\n--------------------------------------------------\nhttps://t.co/leeMzMaqI4\n\nthis is a BFD.  #openai #chatgpt #microsoft #bing #clippy\n--------------------------------------------------\nThe frequency with which I am using #ChatGPT to get information scares me that I'll lose my extraordinary skill of digging out literally anything available on the #internet.\n--------------------------------------------------\n🔴When you ask a lot of questions about a lot of issues from Chat GPT, there is no specific answer from here on...\n\n-Isn't this familiar when we think about God and creatures?!\nAre we in a matrix ⁉️\n#Matrix #tatebrothers #andrewtate #ChatGPT\n--------------------------------------------------\n🥸 AI is going to replace writers! I just asked it to write me a blog post about tech in the style of @JoannaStern!\n\n🧐 What would it do if there was no Joanna Stern?\n\n🥸 …….\n\n#AI #ChatGPT #GPT4 #BingAI #Bard\n--------------------------------------------------\nSomeone close to me is struggling financially. They become overwhelmed and stopped dealing with it\n\nI told them to let #ChatGPT deal with it\n\nI share this with permission because of all the talk about AI and what people are building with it\n\nIt can also help those who struggle! https://t.co/PG7ImUGeKa\n--------------------------------------------------\nDid #ChatGPT just make a whole lot of wfh desk jobs obsolete?\n--------------------------------------------------\nThe frequency with which I am using #ChatGPT to get information scares me that I'll lose my extraordinary skill of digging out literally anything available on the #internet.\n----------------------------------------------------------------------------------------------------\n⭐ SENTIMENT: neutral\n----------------------------------------------------------------------------------------------------\nmore... exams being passed by #chatgpt \n\nhttps://t.co/IJH52IL2eU https://t.co/C2Zx1DKzXc\n--------------------------------------------------\n#Jobs Most Impacted by #ChatGPT and Similar #AI Models 💼\n\nhttps://t.co/WTu1Tm9Och via @VisualCap \n#GPT4 #LLMs #FutureOfWork\n@chboursin @JolaBurnett @Hana_ElSayyed @Shi4Tech @RagusoSergio @efipm @enilev @JoannMoretti @mvollmer1 @baski_LA @CurieuxExplorer @anand_narang @kalydeoo https://t.co/ZKwSFFNNdz\n--------------------------------------------------\nChatgpt-4 v/s Google Bard: A Head-to-Head Comparison #ChatGPT https://t.co/OmuGFC4955\n--------------------------------------------------\n@GiftCee Using #ChatGPT, another lefty tool... https://t.co/pD3EFYLGhK\n--------------------------------------------------\nPublic-private partnerships: In some cases, governments form partnerships with private entities to develop and operate essential infrastructure projects. Such collaborations often involve a combination of private investment and government financing or guarantees. #chatgpt #OPENAI https://t.co/WTJnKQaJDB\n--------------------------------------------------\nFirst known student caught using ChatGPT at UK university - here's ... - The Tab #chatgpt #AI #openAI https://t.co/qdytWOeD7o\n--------------------------------------------------\nUsing any ChatGPT plugins? Need suggestions. #AI #ChatGPT\n--------------------------------------------------\nShould you learn Machine Learning?\n\nHere is perhaps THE seminal course for an introduction.\n#MachineLearning Specialization 2022 @AndrewYNg, @Stanford \nhttps://t.co/KlUH1cpeP8\n\nThis playlist will let you decide for yourself\n#Python #NumPy #TensorFlow #NeuralNetworks\n\n#ChatGPT\n--------------------------------------------------\nRead \"BuzzFeed preps for AI-written content while CNET fumbles\" https://t.co/pETxEwYQOy\n\nFor more, get the app from\nhttps://t.co/0ic5ya66on\n\n#ChatGPT #AI #ML #DL #content #media https://t.co/dnVhSEI35q\n--------------------------------------------------\nPublic-private partnerships: In some cases, governments form partnerships with private entities to develop and operate essential infrastructure projects. Such collaborations often involve a combination of private investment and government financing or guarantees. #chatgpt #OPENAI https://t.co/WTJnKQaJDB\n----------------------------------------------------------------------------------------------------\n⭐ SENTIMENT: positive\n----------------------------------------------------------------------------------------------------\nChatGPT-3 is really something. It unlocks new levels we never thought of. It's a revolutionary technology but still not 100% mature. Here's why 👇 #ArtificialIntelligence #ChatGPT\n--------------------------------------------------\nDid you know that the universe is expanding at an accelerating rate? This discovery was awarded the Nobel Prize in Physics in 2011, and it has revolutionized our understanding of the cosmos. #Cosmology #ChatGPT #Physics https://t.co/Y11i8VZwi5\n--------------------------------------------------\nJust saw a UFO outside my window and it was shaped like a giant doughnut! #aliens #UFO #doughnuts #ChatGPT 🤷‍♂️ https://t.co/BT0iJyrKWh\n--------------------------------------------------\nYou won't believe how much work ChatGPT can take off your plate when it comes to creating a resume. Check out the incredible resume it generated for #LucyLiu, LucyLiu, at https://t.co/GK6CJCMxzF. #ChatGPT #ai #artificialintelligence #OpenAI #resume #C https://t.co/zs86qgkfDA\n--------------------------------------------------\nIt's true that #ChatGPT is super useful, but I love the references, links and cards that come with @YouSearchEngine's YouChat responses! I'm missing this a lot when I go back to try things with ChatGPT. LLM's that cite references! https://t.co/7m9GiUEYGU\n--------------------------------------------------\nCouldn’t agree with @profgalloway more. I’ve already used #ChatGPT heavily and it’s still in its infancy. #LLM and other #AI advancements are going to be leaps forward in terms of usefulness in 2023. https://t.co/MThR6XJ5ZH\n--------------------------------------------------\nhttps://t.co/6zdxvfYvFq Unveils Groundbreaking #ChatGPT #Course and Innovative #Upskilling Platform https://t.co/XghNdfJUYs\n--------------------------------------------------\nAnyone know how to utilize chatgpt when inputs are too large to copy and paste into the text box?\n\nKeep in mind I want to use it to help me find relevant information in a haystack.\n#chatgpt\n--------------------------------------------------\nSee how you can make AI code for yourself for free in just 5 mins \nIf you're searching for the easiest method to make a Website, then this video is definitely for you.\nhttps://t.co/yOeg82XPBX\n#openai #chatgpt #howtomakewebsite #aiprogramming #programming #easiestway #aicoding https://t.co/GrY3sDj399\n--------------------------------------------------\nIt's true that #ChatGPT is super useful, but I love the references, links and cards that come with @YouSearchEngine's YouChat responses! I'm missing this a lot when I go back to try things with ChatGPT. LLM's that cite references! https://t.co/7m9GiUEYGU\n----------------------------------------------------------------------------------------------------\n\n\nIt seems like the tweets that are labeled as positive tend to praise LLMs like ChatGPT since they can be beneficial in saving time and solving specific problems. On the other hand, some of the tweets classified as negative aren’t necessarily negative, which is probably due to the presence of negative words. (Now is a good time to note that VADER maps each word to a score and averages these scores into a compound score. You can read more about how it works here.) Then again, these are only a sample of the tweets, so we’re not necessarily getting the full picture here.\nMoving on: how do the sentiments of these tweets change over time?\n\n\nCode\n#@title\n# get counts of each sentiment for each month\nsentiment_by_month = tweets_cleaned.value_counts(\n    [\"sentiment\", \"month\"]\n).reset_index(\n).sort_values(\n    [\"sentiment\", \"month\"]\n).reset_index(\n    drop = True\n)\n\n# create normalized bar chart of tweet counts by sentiment over time\nalt.Chart(sentiment_by_month).mark_bar().encode(\n    x = alt.X(\"month:O\", title = \"Month\"),\n    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n    color = alt.Color(\"sentiment:N\",\n                      title = \"Sentiment\",\n                      scale = alt.Scale(domain = [\"negative\", \"neutral\", \"positive\"],\n                                        range = [\"red\", \"orange\", \"green\"]))\n)\n\n\n\n\n\n\n\nFor the most part, the distribution of sentiments don’t vary that much between months. Despite there being a slight increase in positive tweets over time, it seems that Twitter users are generally consistent about their opinions on LLMs.\nSince we have both the topics and sentiments for all of the tweets, we can see if certain topics tend to have lower or higher proportions of positive sentiments.\n\n\nCode\n#@title\n# get counts of each sentiment by topic\ntopics_by_sentiment = tweets_cleaned.value_counts(\n    [\"topic\", \"sentiment\"]\n).reset_index(\n).sort_values(\n    [\"topic\", \"sentiment\"]\n).reset_index(\n    drop = True\n)\n\n# create normalized bar chart of tweet counts by sentiment for each topic\nalt.Chart(topics_by_sentiment).mark_bar().encode(\n    x = alt.X(\"topic:O\", title = \"Topic\"),\n    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n    color = alt.Color(\"sentiment:N\",\n                      title = \"Sentiment\",\n                      scale = alt.Scale(domain = [\"negative\", \"neutral\", \"positive\"],\n                                        range = [\"red\", \"orange\", \"green\"]))\n)\n\n\n\n\n\n\n\nOnce again, we notice Topics 3 (LLM prompts) and 5 (Innovation and impact) popping up again – the 2 topics appear to have higher proportions of positive sentiments. Around half of the tweets in each of the other topics (AI as a field, LLMs in general, AI art) are classified as positive."
  },
  {
    "objectID": "files/llm-tweets.html#discussion",
    "href": "files/llm-tweets.html#discussion",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "Discussion",
    "text": "Discussion\n\nSummary of methods\n\nWe focused on tweets about LLMs ranging from December 2022 to the beginning of June 2023 to understand the online discourse surrounding them.\nWe cleaned the tweets, which included filtering out spam tweets and standardizing the text (e.g., lowercasing, lemmatizing, tokenizing).\nWe performed topic modeling and sentiment analysis on the remaining tweets and also looked at how the resulting topics and sentiments changed over time.\n\n\n\nAnswers\n\nWhat kinds of topics are brought up in the online discourse surrounding LLMs?\n\nThe discourse surrounding LLMs tended to fall into one of the 5 topics: AI as a field, LLMs in general, LLM prompts, AI art, and Innovation and impact.\nThere was an initial increase in tweets about LLM prompts after the initial launch of ChatGPT in November 2022, though tweets in the later months shifted towards being more about innovation and impact.\nThese topics are not as clear-cut as initially thought; in fact, the topics have considerable overlap.\n\nWhat kinds of sentiments are associated with online discussions about LLMs?\n\nTweets about LLMs tended to be more positive or neutral; neutral tweets made up a smaller proportion (around 15.9%).\nOver time, the distribution of these sentiments generally did not change – there were still more positive and neutral tweets compared to negative ones.\nWhen taking a closer look at legitimate tweets about LLMs (i.e., not spam), positive tweets generally praise LLMs for being revolutionary and efficient while negative tweets tend to be critical about their impact. However, the sentiment labels are a bit hazy since VADER can be prone to misclassifications.\n\n\n\n\nLimitations\n\nIt is difficult to manually filter out spam. A lot of the methods used to filter out spam in this analysis required hard-coding values (e.g., filtering out certain hashtags). This process is by no means perfect as spam tweets could still pass through while other legitimate tweets could be filtered out.\nOnly tweets were used in this analysis. We only looked at tweets about LLMs since the dataset was readily available. However, the results can only at most be generalizable to people who use Twitter, which does not include everyone on the Internet who has an opinion on LLMs.\n\n\n\nFuture directions\n\nFind a better way to filter out spam, possibly through machine learning. One method to try out in the future would be to train a classification model on a labeled dataset of spam and non-spam tweets, then tweak it to filter out spam in our data.\nUse other sources. In addition to using tweets, an extension of this project could compare how the discourse changes when focusing on different social media platforms (e.g., Reddit, Facebook) and news outlets (e.g. The New York Times, Fox News)."
  },
  {
    "objectID": "files/ds-salaries.html",
    "href": "files/ds-salaries.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi, Justin Liu, Shulei Wang"
  },
  {
    "objectID": "files/ds-salaries.html#authors",
    "href": "files/ds-salaries.html#authors",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi\nJustin Liu\nShulei Wang"
  },
  {
    "objectID": "files/ds-salaries.html#background",
    "href": "files/ds-salaries.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use here.\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "files/ds-salaries.html#preprocessing-eda",
    "href": "files/ds-salaries.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\nImports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\n\nData Cleaning\n\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n# check datatypes\nsalaries.dtypes\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n# check missing values\nsalaries.isnull().sum()\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above: - We dropped some redundant variables. - We dropped salary and salary_currency since they were used to calculate salary_in_usd. - We also dropped employee_residence since it is very similar to company_location. - We removed duplicate rows to ensure that each observation was unique. - As a result, more than 4000 rows were dropped. - We converted several numerical variables to categorical variables. - We turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively). - We recoded the abbreviated forms of some categories to make them easier to read. - For example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on. - We manually grouped the levels in a few categorical variables into broader groups. - Some of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data). - For job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job). - For company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data). - We then dropped job_title from our dataset.\n\n\nData Visualizations\n\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Count\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/1248493630.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n\n    # set y-axis labels\n    if i == 0 or i == 4:\n        ax[i].set(ylabel=\"Salary in USD ($)\")\n    else:\n        ax[i].set(ylabel=None)\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_53704/3969857341.py:33: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above: - company_location to salary_in_usd (0.35) - experience_level to salary_in_usd (0.3) - company_location to work_year (0.21)"
  },
  {
    "objectID": "files/ds-salaries.html#modeling",
    "href": "files/ds-salaries.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\nX_train # predictors training set\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\ny_train # target test set\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n# show intercept\nlm_model.intercept_\n\n63760.94720830995\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example: - Having a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively. - If the company is located in the United States, then the salary is predicted to increase by $47,530.45. - Working in a data management job can increase one’s salary by $45,648.72. - Being a freelancer is associated with a salary decrease of $47,912.14.\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\nWe first train the neural network using MAE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "files/ds-salaries.html#results-analysis",
    "href": "files/ds-salaries.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "files/ds-salaries.html#conclusion",
    "href": "files/ds-salaries.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "projects/llm-tweets/index.html",
    "href": "projects/llm-tweets/index.html",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/llm-tweets/index.html#links",
    "href": "projects/llm-tweets/index.html#links",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "",
    "text": "Report"
  }
]