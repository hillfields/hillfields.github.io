[
  {
    "objectID": "notebooks/voter-turnout.html",
    "href": "notebooks/voter-turnout.html",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "",
    "text": "Chunting Zheng, Justin Liu, Jasper Wu, Sanaz Ebrahimi"
  },
  {
    "objectID": "notebooks/voter-turnout.html#research-question",
    "href": "notebooks/voter-turnout.html#research-question",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "Research Question",
    "text": "Research Question\nHow did various election demographics affect voter turnout in the 2016 Alaska general election?"
  },
  {
    "objectID": "notebooks/voter-turnout.html#executive-summary",
    "href": "notebooks/voter-turnout.html#executive-summary",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "Executive Summary",
    "text": "Executive Summary\nIn this report, we will first perform exploratory data analysis to identify and understand patterns in the dataset at a high level. Then, we will conduct detailed hypothesis testing to investigate the effect of election demographics (age, gender, party registration, ethnicity, county, education, and income) on voter turnout under different scenarios. We will perform a \\(\\chi^2\\)-test for every demographic feature against voter turnout. Moreover, several two sample \\(Z\\)-tests for proportions will be carried out to assess the plausibility of different hypotheses. Our results suggest that 1) there is an association between each demographic feature and voter turnout, and 2) females and 60+ year old citizens were more likely to vote in the general election. Finally, we will predict the voter turnout (either yes or no) in \\(2016\\) for Alaska. After building logistic regression, decision tree, random forest, gradient-boosted tree, and neural network models to predict voter turnout, the results showed that the gradient-boosted tree model performs the best in terms of the area under both the ROC and PR curves. However, the logistic regression model was comparable in performance and offered the best interpretability of the results."
  },
  {
    "objectID": "notebooks/voter-turnout.html#introduction",
    "href": "notebooks/voter-turnout.html#introduction",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "Introduction",
    "text": "Introduction\nEffective campaigns use limited resources to efficiently net enough votes for their candidate to win. Running a campaign requires spending money in the right places, running cost-effective programs, and focusing on the people most likely to be swayed by those programs. Therefore, it is essential to identify which voters should be the targets of get-out-the-vote (GOTV) activities. In the presidential election, the voter turnout is higher than in any other political contest. A higher turnout means a high percentage of voters are almost certain to vote and need little encouragement to show up at the polls. In \\(2016\\), Alaska adopted automatic voter registration, increasing the number of registered voters by more than \\(70,000\\).\nMoreover, many political scientists will tell us that demographics are significant predictors of voting participation. This report reminds us to convert demographic information into a schema that predicts turnout for individual voters. Our project examines how election demographics affected voter turnout in the \\(2016\\) Alaska general election, an event of potential interest to political scientists and the electorate."
  },
  {
    "objectID": "notebooks/voter-turnout.html#data",
    "href": "notebooks/voter-turnout.html#data",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "Data",
    "text": "Data\n\n\nCode\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler, PCA, Bucketizer\nfrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\nfrom pyspark.sql.functions import isnan, when, count, col, regexp_replace\nfrom statsmodels.stats.proportion import proportions_ztest\nfrom pyspark.ml.stat import ChiSquareTest\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql import functions as F\nfrom pyspark.ml import Pipeline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport altair as alt\nimport numpy as np\n\n\n\nLoading the data into a Spark DataFrame\n\n\nCode\ngcs_path = \"gs://pstat135-voter-file/VM2Uniform/VM2Uniform--AK--2021-02-03\"\ndf = spark.read.parquet(gcs_path)\n\n\n                                                                                \n\n\n\n\nSelecting columns that we will be using in our analysis\n\n\nCode\ndf = df.select(\"LALVOTERID\", \"Voters_Gender\", \"Voters_Age\", \"Parties_Description\",\n               \"EthnicGroups_EthnicGroup1Desc\", \"County\", \"CommercialData_Education\", \n               \"CommercialData_EstimatedHHIncomeAmount\", \"General_2016\")\n\n[df.count(), len(df.columns)]\n\n\n                                                                                \n\n\n[548259, 9]\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- LALVOTERID: string (nullable = true)\n |-- Voters_Gender: string (nullable = true)\n |-- Voters_Age: string (nullable = true)\n |-- Parties_Description: string (nullable = true)\n |-- EthnicGroups_EthnicGroup1Desc: string (nullable = true)\n |-- County: string (nullable = true)\n |-- CommercialData_Education: string (nullable = true)\n |-- CommercialData_EstimatedHHIncomeAmount: string (nullable = true)\n |-- General_2016: string (nullable = true)\n\n\n\n\n\nSummary of missing values for each column\n\n\nCode\n# Get number of observations with missing values for each variable\nmissing = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\\\n    .toPandas()\\\n    .melt(ignore_index = False)\\\n    .reset_index()\\\n    .rename(columns = {\"value\": \"missing\"})\\\n    .drop(columns = \"index\")\n\n# Add column for proportion of missing values\nmissing[\"prop_missing\"] = missing[\"missing\"] / df.count()\nmissing.sort_values(\"prop_missing\", ascending = False).reset_index(drop = True)\n\n\n                                                                                \n\n\n\n\n\n\n\n\n\nvariable\nmissing\nprop_missing\n\n\n\n\n0\nCommercialData_Education\n284813\n0.519486\n\n\n1\nGeneral_2016\n268226\n0.489232\n\n\n2\nVoters_Age\n215368\n0.392822\n\n\n3\nCommercialData_EstimatedHHIncomeAmount\n95658\n0.174476\n\n\n4\nEthnicGroups_EthnicGroup1Desc\n71026\n0.129548\n\n\n5\nVoters_Gender\n66\n0.000120\n\n\n6\nLALVOTERID\n0\n0.000000\n\n\n7\nParties_Description\n0\n0.000000\n\n\n8\nCounty\n0\n0.000000\n\n\n\n\n\n\n\n\n\nDropping rows with NULL values on selected columns\n\n\nCode\ndf = df.na.drop(subset=df.columns[:-1])\n[df.count(), len(df.columns)]\n\n\n                                                                                \n\n\n[187065, 9]\n\n\n\n\nConverting data and renaming columns\n\n\nCode\n# Split the column on commas into a list\nsplit_col = F.split(df[\"CommercialData_Education\"], '-')\n# Put the first value of the list into a new column\ndf = df.withColumn(\"Education\", split_col.getItem(0))\ndf = df.drop(\"CommercialData_Education\")\n\n# Inspect results\ndf[[\"Education\"]].show(5)\n\n\n+-------------+\n|    Education|\n+-------------+\n| Grad Degree |\n| Grad Degree |\n| Bach Degree |\n|Some College |\n| Bach Degree |\n+-------------+\nonly showing top 5 rows\n\n\n\n\n\nCode\n# Note that the column `CommercialData_EstimatedHHIncomeAmount` is a string type\ndf = df.withColumn(\n    \"Income\", F.translate(F.col(\"CommercialData_EstimatedHHIncomeAmount\"), \"$,\", \"\").cast(DoubleType())\n).drop(\"CommercialData_EstimatedHHIncomeAmount\")\n\ndf[[\"Income\"]].show(5)\n\n\n+--------+\n|  Income|\n+--------+\n| 75000.0|\n|123000.0|\n| 76000.0|\n| 76000.0|\n|143000.0|\n+--------+\nonly showing top 5 rows\n\n\n\n\n\nCode\ndf = df.withColumn(\n    \"Age\", col(\"Voters_Age\").cast(\"int\")\n).drop(\"Voters_Age\")\n\ndf = df.na.fill({\"General_2016\": \"N\"})\ndf.printSchema()\n\n\nroot\n |-- LALVOTERID: string (nullable = true)\n |-- Voters_Gender: string (nullable = true)\n |-- Parties_Description: string (nullable = true)\n |-- EthnicGroups_EthnicGroup1Desc: string (nullable = true)\n |-- County: string (nullable = true)\n |-- General_2016: string (nullable = false)\n |-- Education: string (nullable = true)\n |-- Income: double (nullable = true)\n |-- Age: integer (nullable = true)\n\n\n\n\n\nCode\nincome_bucketizer = Bucketizer(splits=[0, 45195, 135586, float('inf')],\n                               inputCol=\"Income\", outputCol = \"Income_bin\")\n\n# Apply buckets to \"Income\" column\ndf = income_bucketizer.transform(df)\n\ndf = df.withColumnRenamed(\"Voters_Gender\", \"Gender\")\\\n    .withColumnRenamed(\"Parties_Description\", \"Party\")\\\n    .withColumnRenamed(\"EthnicGroups_EthnicGroup1Desc\", \"Ethnicity\")\n\ndf.toPandas().head()\n\n\n                                                                                \n\n\n\n\n\n\n\n\n\nLALVOTERID\nGender\nParty\nEthnicity\nCounty\nGeneral_2016\nEducation\nIncome\nAge\nIncome_bin\n\n\n\n\n0\nLALAK176775952\nM\nNon-Partisan\nEuropean\nFAIRBANKS NORTH STAR\nY\nGrad Degree\n75000.0\n70\n1.0\n\n\n1\nLALAK410115488\nM\nNon-Partisan\nEuropean\nFAIRBANKS NORTH STAR\nY\nGrad Degree\n123000.0\n59\n1.0\n\n\n2\nLALAK177038764\nF\nDemocratic\nEuropean\nFAIRBANKS NORTH STAR\nY\nBach Degree\n76000.0\n51\n1.0\n\n\n3\nLALAK176844885\nM\nNon-Partisan\nEuropean\nFAIRBANKS NORTH STAR\nY\nSome College\n76000.0\n62\n1.0\n\n\n4\nLALAK177017449\nF\nUnknown\nEuropean\nFAIRBANKS NORTH STAR\nY\nBach Degree\n143000.0\n37\n2.0"
  },
  {
    "objectID": "notebooks/voter-turnout.html#methods",
    "href": "notebooks/voter-turnout.html#methods",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "Methods",
    "text": "Methods\n\nExploratory Data Analysis\n\n\nCode\ntotal_Gvote_count = df.where(F.col(\"General_2016\") == \"Y\").count()\ngender_general = df.groupBy(\"Gender\", \"General_2016\")\\\n    .agg(F.count(\"LALVOTERID\")\\\n    .alias(\"Gender_Count\"))\\\n    .where(F.col(\"General_2016\") == \"Y\")\\\n    .withColumn(\"Vote_Rate\", F.round(F.col(\"Gender_Count\")/total_Gvote_count*100, 2))\\\n    .toPandas()\n\ntotal_count = df.groupBy(\"Gender\")\\\n    .agg(F.count(\"LALVOTERID\")\\\n    .alias(\"Total_Count\"))\n\n# Barplot to visualize both the tables from above against each other\nX = [\"General_2016\"]\nYF = [gender_general[gender_general.Gender == \"F\"].Vote_Rate.iloc[0]]\nZM = [gender_general[gender_general.Gender == \"M\"].Vote_Rate.iloc[0]]\n  \nX_axis = np.arange(len(X))\n  \nplt.bar(X_axis - 0.25/2, YF, width=0.2, color=\"b\", edgecolor=\"black\", label=\"Female\")\nplt.bar(X_axis + 0.25/2, ZM, width=0.2, color=\"g\", edgecolor=\"black\", label=\"Male\")\n  \nplt.xticks(X_axis, X)\nplt.ylabel(\"Turnout\")\nplt.title(\"Participation Rates by Gender\")\nplt.legend(loc = \"upper right\")\nplt.show()\n\n\n                                                                                \n\n\n\n\n\n\nFigure 1: Barplot representing the voter turnout for males and females voting in the \\(2016\\) Alaskan general election.\n\n\n\nCode\n# Histogram showing visualization of age distribution in the 2016 general election in Alaska\ndf.where(F.col(\"General_2016\") == \"Y\")\\\n    .toPandas()[\"Age\"]\\\n    .plot.hist(bins=12, alpha=0.5, rwidth=0.7, color='b')\n\nplt.xlabel(\"Age\")\nplt.title(\"Participation by Age\")\nplt.show()\n\n\n                                                                                \n\n\n\n\n\n\nFigure 2: Histogram illustrating the frequency of voters in the \\(2016\\) Alaskan general election by age.\n\n\n\nCode\nparties_general = df.groupBy(\"Party\", \"General_2016\")\\\n    .agg(F.count(\"LALVOTERID\")\\\n    .alias(\"Count\"))\\\n    .where(F.col(\"General_2016\") == \"Y\")\\\n    .withColumn(\"Vote_Rate\", F.round(F.col(\"Count\")/total_Gvote_count*100, 2))\\\n    .orderBy(F.desc(\"Vote_Rate\"))\\\n    .toPandas()\n\n# Barplot showing voter turnout based on party in 2016 general election in Alaska\nplt.bar(parties_general[\"Party\"], parties_general[\"Vote_Rate\"], color=\"b\")\nplt.xticks(rotation=60, fontsize=8, ha=\"center\");\nplt.title(\"Participation Rates by Party Affiliations\")\nplt.xlabel(\"Party affiliation\")\nplt.ylabel(\"Turnout\")\nplt.show()\n\n\n                                                                                \n\n\n\n\n\n\nFigure 3: Barplot of voter turnout in the \\(2016\\) Alaskan general election based on party preference.\n\n\n\nCode\nethnic_general = df.groupBy(\"Ethnicity\", \"General_2016\")\\\n    .agg(F.count(\"LALVOTERID\")\\\n    .alias(\"Count\"))\\\n    .where(F.col(\"General_2016\") == \"Y\")\\\n    .withColumn(\"Vote_Rate\", F.round(F.col(\"Count\")/total_Gvote_count*100, 2))\\\n    .orderBy(F.desc(\"Vote_Rate\"))\\\n    .toPandas()\n\n# Barplot of ethnicity versus turnout during the 2016 general election in Alaska\nplt.bar(ethnic_general[\"Ethnicity\"], ethnic_general[\"Vote_Rate\"], color=\"b\")\nplt.xticks(rotation=60, fontsize=8, ha=\"center\");\nplt.title(\"Participation Rates by Ethnic Groups\")\nplt.xlabel(\"Ethnicity\")\nplt.ylabel(\"Turnout\")\nplt.show()\n\n\n                                                                                \n\n\n\n\n\n\nFigure 4: Barplot of voter turnout in the \\(2016\\) Alaskan general election based on ethnicity.\n\n\n\nCode\n# Boxplots of ethnicity versus estimated household income in Alaska \nethnic_income = df.select(\"Income\", \"Ethnicity\").toPandas()\nplt.figure(figsize=(10,5))\nsns.boxplot(x=\"Ethnicity\", y=\"Income\", data=ethnic_income)\nplt.title(\"Estimated Household Income by Ethnic Group\")\nplt.xticks(rotation=60, fontsize=8, ha=\"center\")\nplt.xlabel(\"Ethnic Group\")\nplt.ylabel(\"Estimated Household Income\")\nplt.show()\n\n\n                                                                                \n\n\n\n\n\n\nFigure 5: Multiple comparison box plot showcasing the ranges of estimated household income in the \\(2016\\) Alaskan general election based on ethnicity.\n\n\n\nCode\nedu_general = df.groupBy(\"Education\", \"General_2016\")\\\n    .agg(F.count(\"LALVOTERID\")\\\n    .alias(\"Count\"))\\\n    .where(F.col(\"General_2016\") == \"Y\")\\\n    .withColumn(\"Vote_Rate\", F.round(F.col(\"Count\")/total_Gvote_count*100, 2))\\\n    .orderBy(F.desc(\"Vote_Rate\"))\\\n    .toPandas()\n\nplt.bar(edu_general[\"Education\"], edu_general[\"Vote_Rate\"], color=\"b\")\nplt.xticks(rotation=60, fontsize=8, ha=\"center\");\nplt.title(\"Participation Rates by Education Degrees\")\nplt.xlabel(\"Education degree\")\nplt.ylabel(\"Turnout\")\nplt.show()\n\n\n                                                                                \n\n\n\n\n\n\nFigure 6: Barplot of voter turnout based on education degree in the \\(2016\\) general election in Alaska.\n\n\n\nCode\nparties_education = df.groupBy(\"Party\", \"Education\") \\\n    .count() \\\n    .toPandas() \\\n    .sort_values(\"count\", ascending = False) \\\n    .reset_index(drop = True)\n\n# Bar chart of party affiliations and education level\nalt.Chart(parties_education).mark_bar().encode(\n    x=alt.X(\"sum(count)\", title=\"Number of voters\"),\n    y=alt.Y(\"Party\", sort=\"-x\", title=\"Party affiliation\"),\n    color=alt.Color(\"Education:N\", title=\"Education level\")\n).properties(\n    width = 550\n)\n\n\n                                                                                \n\n\n\n\n\n\n\n\nFigure 7: Barplot representing the amount of voters in the \\(2016\\) Alaska general election by both party preference and education level.\n\n\n\nCode\ndf.where(F.col(\"General_2016\") == \"Y\")\\\n    .toPandas()[\"Income\"]\\\n    .plot.hist(bins=12, alpha=0.5, rwidth=0.7, color='b')\n\nplt.xlabel(\"Income\")\nplt.title(\"Participation by Income\")\nplt.show()\n\n\n                                                                                \n\n\n\n\n\n\nFigure 8: Histogram of voter frequency based solely on income in the \\(2016\\) general election taking place in Alaska.\n\nFemales tend to have a slightly higher voter turnout than males for the 2016 Alaskan elections (Figure 1). The age factor in Alaska is one of the most significant patterns in politics—voters in their \\(20\\)s vote at low levels. Participation increases for each age group until the \\(60\\)s, then starts to fall off in the \\(80\\)s and \\(90\\)s. Less than half of voters under \\(30\\) will show up to vote, but almost all other age groups have a turnout rate over \\(70\\) percent (Figure 2).\nAnother significant predictor of voter turnout is party affiliation. The Republican party brought in the most votes for the general election, hinting that this may have been a red state for the \\(2016\\) race. This idea is backed by the fact that the second most common party, the Democratic party, has a much lower amount of votes than both the “unknown” and “Non-partisan” parties (Figure 3).\nThe majority of the barplot consists of Europeans. The second largest population is “Hispanic and Portuguese,” but their voter turnout is nowhere near as large as the Europeans. The difference in turnout rates between ethnicities is one of the enormous disparities observed so far and could significantly affect voter outcomes. There are many contextual and historical reasons this could be happening in Alaska. This would be an excellent place for a campaign manager to look into enticing more voters to participate. This barplot refers to the voter turnout rate for the \\(2016\\) general election in Alaska (Figure 4).\nAs one can see, the most notable differences are that the “East and South Asian” race has the highest median income, whereas the “Likely African-American” race has the lowest median income and a slightly lower quartile range. The side-by-side groups that showed the considerable disparity in voter turnout were “Hispanic and Portuguese” and “European.” This graphic shows they have similar estimated incomes based on their boxplots (Figure 5).\nEducation may be one of the crucial predictors of voting participation. For the \\(2016\\) general election, it appears that most voters (around \\(65\\%\\)) have completed some higher education, either bachelor’s or master’s degrees. Around \\(20\\%\\) have a high school diploma, while \\(14.5\\%\\) have pursued some college (Figure 6). The education levels, for the most part, are evenly distributed among the voters’ party affiliations. Notice that people with bachelor’s or master’s degrees make up a majority of the voters overall and tend to make up the majority within each party. As a result, there is a weak correlation between education level and party affiliation (Figure 7).\nIn the \\(2016\\) general election, there is a slight right-skew in the distribution – there are slightly more people on the lower half of the distribution (less than \\(\\$125,000\\)) compared to those on the higher end (between \\(\\$125,000\\) and \\(\\$250,000\\)). The lowest income (\\(\\le \\$45,000\\)) turns out slightly less than the middle or upper income, but once income reaches \\(\\$55,000\\) to \\(\\$150,000\\), there is little difference at all (Figure 8).\n\n\nStatistical Methods and Results\n\nChi-squared test of independence\nThe \\(\\chi^2\\)-test aims at determining whether there is an association between each demographic feature and voter turnout (i.e., whether the variables are independent or related). For each feature, the (feature, voter turnout) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed. The null hypothesis is that the occurence of the outcomes is statistically independent.\n\n\nCode\nage_bucketizer = Bucketizer(splits=[0, 60, float('inf')],\n                            inputCol=\"Age\", outputCol=\"Age_bin\")\n\n# apply buckets to \"Age\" column\ndf1 = age_bucketizer.transform(df)\n\n# Convert categorical strings to index values\nstringIndexer = [StringIndexer(inputCol=column, \n                               outputCol=column+\"_index\") \\\n                 .fit(df) for column in [\"Gender\", \"Party\", \"Ethnicity\", \"County\", \"Education\", \"General_2016\"]]\n\npipeline = Pipeline(stages=stringIndexer)\ndf2 = pipeline.fit(df1).transform(df1) \\\n    .select(\"Gender_index\", \"Party_index\", \"Ethnicity_index\", \"County_index\", \"Education_index\", \n            \"Income_bin\", \"Age_bin\", \"General_2016_index\")\n\nassembler = VectorAssembler(\n    inputCols=[\"Gender_index\", \"Party_index\", \"Ethnicity_index\", \"County_index\", \n               \"Education_index\", \"Income_bin\", \"Age_bin\"], \n    outputCol=\"features\"\n)\n\nvectorized_df = assembler.transform(df2).select(\"General_2016_index\", \"features\")\n\nvectorized_df.show(5, truncate=False)\n\nr = ChiSquareTest.test(vectorized_df, \"features\", \"General_2016_index\").head()\nprint(\"pValues: \" + str(r.pValues))\nprint(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\nprint(\"statistics: \" + str(r.statistics))\n\n\n                                                                                [Stage 72:=============================&gt;                            (2 + 2) / 4]                                                                                \n\n\n+------------------+-----------------------------+\n|General_2016_index|features                     |\n+------------------+-----------------------------+\n|0.0               |[0.0,2.0,0.0,2.0,3.0,1.0,1.0]|\n|0.0               |[0.0,2.0,0.0,2.0,3.0,1.0,0.0]|\n|0.0               |[1.0,3.0,0.0,2.0,2.0,1.0,0.0]|\n|0.0               |[0.0,2.0,0.0,2.0,1.0,1.0,1.0]|\n|0.0               |[1.0,0.0,0.0,2.0,2.0,2.0,0.0]|\n+------------------+-----------------------------+\nonly showing top 5 rows\n\npValues: [0.0,0.0,0.0,0.0,0.0,0.0,0.0]\ndegreesOfFreedom: [1, 8, 4, 28, 5, 2, 1]\nstatistics: [184.8369583223074,7157.917830498936,1580.022914498495,1144.2548891675792,2526.3295056151915,454.2971313783404,5745.72713407305]\n\n\nThe \\(p\\)-values are all \\(0\\) and less than the \\(0.05\\) significance level, so we reject the null hypotheses. That is, there is an association between each demographic feature and voter turnout.\n\n\nTwo sample \\(Z\\)-test for proportions\nThe two sample \\(Z\\)-test for proportions aims at finding whether there is sufficient evidence at the \\(0.05\\) level to support the claims (a) and (b) below. The sample size is large enough (\\(n \\ge 25)\\), so it is appropriate to use the two sample \\(Z\\)-test which compares the proportions of the two groups of each variable by deriving a test statistic through dividing the difference between the two population proportion by the standard error of the difference, \\(Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE}\\). The \\(p\\)-value associated with the test is the area above the test statistic in a standard normal distribution.\n(a) Claim: Female are more likely to vote\n\nTable 1: Voter Turnout by Gender.\n\n\n\nCode\ndf.groupBy(\"Gender\", \"General_2016\").count().show()\n\n\n[Stage 77:&gt;                                                         (0 + 4) / 4]                                                                                \n\n\n+------+------------+-----+\n|Gender|General_2016|count|\n+------+------------+-----+\n|     F|           N|24234|\n|     F|           Y|68777|\n|     M|           Y|66909|\n|     M|           N|27145|\n+------+------------+-----+\n\n\n\nLet \\(p_1\\) represent the proportion of females who voted and \\(p_2\\) represent the proportion of males who voted. The hypotheses of interest are \\(H_0: p_1 = p_2\\) and \\(H_a: p_1 &gt; p_2\\).\n\n\nCode\ncount = np.array([68777, 66909])\nnobs = np.array([24234+68777, 66909+27145])\nstat, pval = proportions_ztest(count, nobs, alternative=\"larger\")\nprint(\"pValues: \" + '{0:0.3f}'.format(pval))\nprint(\"statistics: \" + str(stat))\n\n\npValues: 0.000\nstatistics: 13.59547565634643\n\n\nThe \\(p\\)-value \\(= 0\\) is less than the \\(0.05\\) significance level, so we reject \\(H_0: p_1 = p_2\\). That is, there is evidence that female are more likely to vote.\n(b) Claim: Age more than \\(60\\) are more likely to vote\n\nTable 2: Voter Turnout by Age Group. Age is buckted as Age_index, with a value of \\(0\\) for voter with age less than \\(60\\) and a value of \\(1\\) otherwise. General_2016 is indexed as General_2016_index, with a value of \\(1\\) for voter who did not vote and a value of \\(0\\) for voter who voted.\n\n\n\nCode\ndf2.groupBy(\"Age_bin\", \"General_2016_index\").count().show()\n\n\n[Stage 82:&gt;                                                         (0 + 4) / 4]                                                                                \n\n\n+-------+------------------+-----+\n|Age_bin|General_2016_index|count|\n+-------+------------------+-----+\n|    1.0|               1.0|15562|\n|    0.0|               1.0|35817|\n|    1.0|               0.0|67571|\n|    0.0|               0.0|68115|\n+-------+------------------+-----+\n\n\n\nLet \\(p_1\\) represents the proportion of voter with age \\(\\ge 60\\) who voted, and \\(p_2\\) represents the proportion of voter with age \\(&lt; 60\\) who voted. The hypotheses of interest are \\(H_0: p_1 = p_2\\) and \\(H_a: p_1 &gt; p_2\\).\n\n\nCode\ncount = np.array([67571, 68115])\nnobs = np.array([67571+15562, 35817+68115])\nstat, pval = proportions_ztest(count, nobs, alternative=\"larger\")\nprint(\"pValues: \" + '{0:0.3f}'.format(pval))\nprint(\"statistics: \" + str(stat))\n\n\npValues: 0.000\nstatistics: 75.80057476083573\n\n\nThe \\(p\\)-value \\(= 0\\) is less than the \\(0.05\\) significance level, so we reject \\(H_0: p_1 = p_2\\). That is, there is evidence that citizens older than 60 are more likely to vote.\n\n\n\nMachine Learning Models\n\nPreparing data for machine learning\n\n\nCode\ncategoricalCols = [\"Gender\", \"Party\", \"Ethnicity\", \"County\", \"Education\"]\nstages = []\n\nfor col in categoricalCols:\n    # Convert categorical strings to index values\n    indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n    # One-hot encode index values\n    onehot = OneHotEncoder(inputCols=[indexer.getOutputCol()], \n                           outputCols=[col+\"_dummy\"])\n    stages += [indexer, onehot]\n    \nlabel_indexer = StringIndexer(inputCol=\"General_2016\", outputCol=\"label\")\nstages += [label_indexer]\n\n# assemble predictors into a single column\nassembler = VectorAssembler(inputCols=[c+\"_dummy\" for c in categoricalCols]+[\"Age\", \"Income_bin\"], \n                           outputCol=\"features\")\nstages += [assembler]\n\n\n\n\nCode\n# Construct a pipeline\npipeline = Pipeline(stages = stages)\npipeline_mod = pipeline.fit(df)\nvote = pipeline_mod.transform(df).select([\"label\", \"features\"] + df.columns)\nvote.printSchema()\n\n\n                                                                                \n\n\nroot\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n |-- LALVOTERID: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Party: string (nullable = true)\n |-- Ethnicity: string (nullable = true)\n |-- County: string (nullable = true)\n |-- General_2016: string (nullable = false)\n |-- Education: string (nullable = true)\n |-- Income: double (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Income_bin: double (nullable = true)\n\n\n\n\n\nCode\n# Split into training and testing sets in a 80:20 ratio\ntrain, test = vote.randomSplit([0.8, 0.2], seed=43)\n\n# Check that training set has around 80% of records\ntraining_ratio = train.count() / vote.count()\nprint(training_ratio)\n\n\n                                                                                \n\n\n0.8010477641461524\n\n\n\n\nLogistic Regression\n\n\nCode\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\nlrModel = lr.fit(train)\n\n\n23/03/19 19:16:02 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n23/03/19 19:16:02 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n                                                                                \n\n\n\n\nCode\nfeatureCols = pd.DataFrame(vote.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"binary\"]+\n  vote.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"]).sort_values(\"idx\")\n\nfeatureCols = featureCols.set_index('idx')\nfeatureCols.head()\n\n\n\n\n\n\n\n\n\nname\n\n\nidx\n\n\n\n\n\n0\nGender_dummy_M\n\n\n1\nParty_dummy_Unknown\n\n\n2\nParty_dummy_Republican\n\n\n3\nParty_dummy_Non-Partisan\n\n\n4\nParty_dummy_Democratic\n\n\n\n\n\n\n\n\n\nCode\ncoefsArray = np.array(lrModel.coefficients)  # convert to np.array\ncoefsDF = pd.DataFrame(np.abs(coefsArray), columns=['coefs'])  # to pandas\n\ncoefsDF = coefsDF.merge(featureCols, left_index=True, right_index=True)  # join it with featureCols we created above\ncoefsDF.sort_values('coefs', inplace=True)  # Sort them\ncoefsDF.iloc[:,[1,0]].head()\n\n\n\n\n\n\n\n\n\nname\ncoefs\n\n\n\n\n19\nCounty_dummy_SITKA\n0.007201\n\n\n31\nCounty_dummy_HAINES\n0.012016\n\n\n44\nEducation_dummy_Grad Degree\n0.018551\n\n\n17\nCounty_dummy_JUNEAU\n0.020284\n\n\n46\nAge\n0.036839\n\n\n\n\n\n\n\nThis table shows us the least important variables in determining voter turnout. We would also like to see the most effective variables, so we will create a bar plot to visualize this.\n\nFeature importance\n\n\nCode\nplt.rcParams[\"figure.figsize\"] = (20,3)\nplt.xticks(rotation=90)\nplt.bar(coefsDF.name, coefsDF.coefs)\nplt.title('Ranked coefficients from the logistic regression model')\nplt.show()\n\n\n\n\n\nFrom the barplot, we can observe that Ethnicity_dummy_East and South Asian is the most impactful variable in the training set compared to the rest of the ones shown since its coefficient is the largest. One could consider how this could have to do with the population densities in Alaska based on ethnicity and if that specific enthinicty had more of a reason to vote. Overall, this visualization implements the foundation of what variables do matter and which ones don’t.\n\n\nCode\nplt.rcParams[\"figure.figsize\"] = (8,6)\n\ntraining_summary = lrModel.summary\nroc = training_summary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(training_summary.areaUnderROC))\n\n\n                                                                                \n\n\n\n\n\nTraining set areaUnderROC: 0.7056829114633703\n\n\nThe ROC curve is a visualization that explains how well a classification model performs. In our example, we are looking at how well the logistic regression model can distinguish whether Alaskan citizens voted or not in the \\(2016\\) general election. If the area under the ROC curve for the training set is perfect or \\(100\\%\\), our model may not be ideal since it could be overfitting the data. In our case, the area under the ROC curve for the training set is \\(0.7057\\). There doesn’t seem to be any overfitting occurring, though we would want to evaluate our model using the test data.\n\n\nCode\npr = training_summary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()\n\n\n\n\n\nSince our labels are imbalanced (more people who voted than those who didn’t), using the precision-recall (PR) curve can be useful here. Like with the ROC curve, we use the area under the PR curve to determine how well our classification model performs, with an area closer to 1 indicating a better model. From the plot above, we see that the curve is exponentially decreasing – as the recall increases, the precision decreases. The area under this curve for the training data does not seem to be very large, suggesting that the model may not necessarily be a very good classifier.\n\n\nCode\nfittedTest = lrModel.transform(test)\nfittedTest.select(\"label\", \"prediction\", \"rawPrediction\").show(5, truncate=False)\n\n\n[Stage 144:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n+-----+----------+------------------------------------------+\n|label|prediction|rawPrediction                             |\n+-----+----------+------------------------------------------+\n|0.0  |1.0       |[-0.8997440476341357,0.8997440476341357]  |\n|0.0  |1.0       |[-0.6787094122646374,0.6787094122646374]  |\n|0.0  |1.0       |[-0.5313529886849718,0.5313529886849718]  |\n|0.0  |1.0       |[-0.05244461205105855,0.05244461205105855]|\n|0.0  |0.0       |[0.09491181152860706,-0.09491181152860706]|\n+-----+----------+------------------------------------------+\nonly showing top 5 rows\n\n\n\nNow that the model has been evaluated on the training data, we can now evaluate it on the test data. The table above indicates label, the true outcome, prediction, the predicted outcome, and rawPrediction, the predicted log-odds.\n\n\nCode\naucEvaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n\naucEvaluator.evaluate(fittedTest)\n\n\n                                                                                \n\n\n0.706698660543756\n\n\nEvaluating the AUC for the testing data returns a value very similar to the one that was evaluated for the training data earlier. This is not the most ideal result because even though \\(0.7\\) is a decent value for AUC, one would want a value closer to \\(1\\) showing that model can distinguish the correct voter turnout outcome more accurately. Having both the training and testing data AUC’s turning out so close to each other means that the model is not overfitting, but it could be doing better.\n\n\nCross-validation\n\n\nCode\nparams = (ParamGridBuilder()\n          .addGrid(lr.regParam, np.arange(0, .1, .01))\n          .addGrid(lr.elasticNetParam, [0, 1])\n          .addGrid(lr.maxIter, [10, 30])\n          .build())\n\n# Create cross-validator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=params, evaluator=aucEvaluator, numFolds=5)\n# Run cross-validation, and choose the best set of parameters\ncvModel = cv.fit(train)\npredictions = cvModel.transform(test)\n\n\n                                                                                \n\n\n\n\nCode\nbestModel = cvModel.bestModel\nprint('Best Param (regParam): ', bestModel._java_obj.getRegParam())\nprint('Best Param (MaxIter): ', bestModel._java_obj.getMaxIter())\nprint('Best Param (elasticNetParam): ', bestModel._java_obj.getElasticNetParam())\n\n\nBest Param (regParam):  0.01\nBest Param (MaxIter):  30\nBest Param (elasticNetParam):  0.0\n\n\n\n\nCode\nprEvaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n\nprint('Testing set areaUnderROC with 5-fold CV: ' + str(aucEvaluator.evaluate(predictions)))\nprint('Testing set prUnderROC with 5-fold CV: ' + str(prEvaluator.evaluate(predictions)))\n\n\n                                                                                                                                                                \n\n\nTesting set areaUnderROC with 5-fold CV: 0.7092270847825329\nTesting set prUnderROC with 5-fold CV: 0.49886712640541386\n\n\n\n\nCode\ncoefsArray = np.array(bestModel.coefficients)  # convert to np.array\ncoefsDF = pd.DataFrame(np.abs(coefsArray), columns=['coefs'])  # to pandas\n\ncoefsDF = coefsDF.merge(featureCols, left_index=True, right_index=True)  # join it with featureCols we created above\ncoefsDF.sort_values('coefs', inplace=True)  # Sort them\n\nplt.rcParams[\"figure.figsize\"] = (20,3)\nplt.xticks(rotation=90)\nplt.bar(coefsDF.name, coefsDF.coefs)\nplt.title('Ranked coefficients from the logistic regression model')\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.rcParams[\"figure.figsize\"] = (8,6)\n\ntraining_summary = bestModel.summary\nroc = training_summary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC with 5-fold CV: ' + str(training_summary.areaUnderROC))\n\n\n                                                                                \n\n\n\n\n\nTraining set areaUnderROC with 5-fold CV: 0.7080004078713547\n\n\n\n\nCode\npr = training_summary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()\n\n\n\n\n\n\n\nCode\npredictions.select(\"label\", \"prediction\", \"rawPrediction\").show(5, truncate=False)\n\n\n[Stage 8948:&gt;                                                       (0 + 1) / 1]                                                                                \n\n\n+-----+----------+--------------------------------------------+\n|label|prediction|rawPrediction                               |\n+-----+----------+--------------------------------------------+\n|0.0  |1.0       |[-0.8165694851916698,0.8165694851916698]    |\n|0.0  |1.0       |[-0.6033488563448288,0.6033488563448288]    |\n|0.0  |1.0       |[-0.4612017704469349,0.4612017704469349]    |\n|0.0  |0.0       |[7.762587212205574E-4,-7.762587212205574E-4]|\n|0.0  |0.0       |[0.1429233446191147,-0.1429233446191147]    |\n+-----+----------+--------------------------------------------+\nonly showing top 5 rows\n\n\n\n\n\n\nDecision Tree\n\n\nCode\ndt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=5)\ndtModel = dt.fit(train)\nfittedTest = dtModel.transform(test)\nfittedTest.select(\"label\", \"prediction\", \"rawPrediction\", \"probability\").show(5, truncate=False)\n\n\n[Stage 8963:&gt;                                                       (0 + 1) / 1]                                                                                \n\n\n+-----+----------+----------------+----------------------------------------+\n|label|prediction|rawPrediction   |probability                             |\n+-----+----------+----------------+----------------------------------------+\n|0.0  |1.0       |[2521.0,3709.0] |[0.4046548956661316,0.5953451043338683] |\n|0.0  |1.0       |[2521.0,3709.0] |[0.4046548956661316,0.5953451043338683] |\n|0.0  |1.0       |[624.0,671.0]   |[0.48185328185328186,0.5181467181467182]|\n|0.0  |0.0       |[24111.0,9951.0]|[0.7078562621102695,0.29214373788973047]|\n|0.0  |0.0       |[24111.0,9951.0]|[0.7078562621102695,0.29214373788973047]|\n+-----+----------+----------------+----------------------------------------+\nonly showing top 5 rows\n\n\n\n\n\nCode\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(fittedTest, {evaluator.metricName: \"areaUnderROC\"})\n\n\n                                                                                \n\n\n0.33632864356044817\n\n\n\nCross-validation\n\n\nCode\n# params = (ParamGridBuilder()\n#           .addGrid(dt.maxBins, [10, 20, 40, 80])\n#           .addGrid(dt.maxDepth, [2, 10, 30])\n#           .build())\n\n# Create cross-validator\n# cv = CrossValidator(estimator=dt, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n# Run cross-validation, and choose the best set of parameters\n# cvModel = cv.fit(train)\ncvModel = CrossValidatorModel.load(\"gs://pstat-135-jl/models/dtcv\")\npredictions = cvModel.transform(test)\n\n\n                                                                                \n\n\n\n\nCode\nbestModel = cvModel.bestModel\nprint('Best Param (maxBins): ', bestModel._java_obj.getMaxBins())\nprint('Best Param (maxDepth): ', bestModel._java_obj.getMaxDepth())\n\n\nBest Param (maxBins):  80\nBest Param (maxDepth):  30\n\n\n\n\nCode\nprEvaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\nprint('Testing set areaUnderROC with 5-fold CV: ' + str(evaluator.evaluate(predictions)))\nprint('Testing set prUnderROC with 5-fold CV: ' + str(prEvaluator.evaluate(predictions)))\n\n\n23/03/19 19:23:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n                                                                                23/03/19 19:23:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n[Stage 8998:&gt;                                                       (0 + 4) / 4]                                                                                \n\n\nTesting set areaUnderROC with 5-fold CV: 0.5139352257963238\nTesting set prUnderROC with 5-fold CV: 0.3201452450848972\n\n\n\n\n\nRandom Forest\n\n\nCode\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=750)\nrfModel = rf.fit(train)\nfittedTest = rfModel.transform(test)\nfittedTest.select(\"label\", \"prediction\", \"rawPrediction\", \"probability\").show(5, truncate=False)\n\n\n23/03/19 19:24:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1001.8 KiB\n23/03/19 19:24:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1971.4 KiB\n23/03/19 19:24:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n23/03/19 19:24:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n[Stage 9028:&gt;                                                       (0 + 1) / 1]                                                                                \n\n\n+-----+----------+---------------------------------------+----------------------------------------+\n|label|prediction|rawPrediction                          |probability                             |\n+-----+----------+---------------------------------------+----------------------------------------+\n|0.0  |0.0       |[397.55318844572133,352.44681155427827]|[0.5300709179276287,0.46992908207237133]|\n|0.0  |0.0       |[409.8468560810993,340.1531439189001]  |[0.5464624747747995,0.4535375252252005] |\n|0.0  |0.0       |[512.4999352206387,237.50006477936077] |[0.6833332469608521,0.31666675303914793]|\n|0.0  |0.0       |[523.445433600767,226.55456639923233]  |[0.6979272448010234,0.3020727551989767] |\n|0.0  |0.0       |[523.9199826150441,226.08001738495537] |[0.6985599768200593,0.30144002317994073]|\n+-----+----------+---------------------------------------+----------------------------------------+\nonly showing top 5 rows\n\n\n\n\n\nCode\nevaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\nprEvaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\nprint('Testing set areaUnderROC: ' + str(evaluator.evaluate(fittedTest)))\nprint('Testing set prUnderROC: ' + str(prEvaluator.evaluate(fittedTest)))\n\n\n23/03/19 19:24:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n                                                                                23/03/19 19:24:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n                                                                                \n\n\nTesting set areaUnderROC: 0.6938006858331277\nTesting set prUnderROC: 0.4948489084526539\n\n\n\nCross-validation\n\n\nCode\n# params = (ParamGridBuilder()\n#           .addGrid(rf.numTrees, [750])\n#           .addGrid(rf.maxDepth, [5, 7])\n#           .build())\n\n# # Create cross-validator\n# cv = CrossValidator(estimator=rf, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n# # Run cross-validation, and choose the best set of parameters\n# cvModel = cv.fit(train)\n# predictions = cvModel.transform(test)\n\n\n\n\nCode\n# bestModel = cvModel.bestModel\n# print('Best Param (numTrees): ', bestModel._java_obj.getNumTrees())\n# print('Best Param (maxDepth): ', bestModel._java_obj.getMaxDepth())\n\n\n\n\nCode\n# prEvaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n# aucEvaluator.evaluate(predictions)\n# prEvaluator.evaluate(predictions)\n\n# print('Testing set areaUnderROC: ' + str(evaluator.evaluate(predictions)))\n# print('Testing set prUnderROC: ' + str(prEvaluator.evaluate(predictions)))\n\n\n\n\n\nGradient-Boosted Tree\n\n\nCode\ngbt = GBTClassifier(maxIter=10)\ngbtModel = gbt.fit(train)\nfittedTest = gbtModel.transform(test)\nfittedTest.select(\"label\", \"prediction\", \"rawPrediction\", \"probability\").show(5, truncate=False)\n\n\n[Stage 9158:&gt;                                                       (0 + 1) / 1]                                                                                \n\n\n+-----+----------+--------------------------------------------+----------------------------------------+\n|label|prediction|rawPrediction                               |probability                             |\n+-----+----------+--------------------------------------------+----------------------------------------+\n|0.0  |1.0       |[-0.35874968797385515,0.35874968797385515]  |[0.32794387488877275,0.6720561251112273]|\n|0.0  |1.0       |[-0.2606555726383605,0.2606555726383605]    |[0.37254569521586484,0.6274543047841352]|\n|0.0  |0.0       |[0.025519989194467834,-0.025519989194467834]|[0.5127572252520854,0.48724277474791455]|\n|0.0  |0.0       |[0.09322231702228273,-0.09322231702228273]  |[0.5464766030181859,0.4535233969818141] |\n|0.0  |0.0       |[0.09322231702228273,-0.09322231702228273]  |[0.5464766030181859,0.4535233969818141] |\n+-----+----------+--------------------------------------------+----------------------------------------+\nonly showing top 5 rows\n\n\n\n\n\nCode\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(fittedTest, {evaluator.metricName: \"areaUnderROC\"})\n\n\n                                                                                \n\n\n0.7073645392530684\n\n\n\nCross-validation\n\n\nCode\n# Create parameter grid, add grids for three parameters, and build the parameter grid\n# params = (ParamGridBuilder()\n#           .addGrid(gbt.maxDepth, [2, 4, 6])\n#           .addGrid(gbt.maxBins, [20, 60])\n#           .addGrid(gbt.maxIter, [5, 10])\n#           .build())\n\n# Create cross-validator\n# cv = CrossValidator(estimator=gbt, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n# cvModel = cv.fit(train)\ncvModel = CrossValidatorModel.load(\"gs://pstat-135-jl/models/gbcv\")\npredictions = cvModel.transform(test)\n\n\n23/03/19 19:25:14 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n    at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n    at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n    at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\n23/03/19 19:25:14 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n    at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n    at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n    at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\n\n\n\n\nCode\nbestModel = cvModel.bestModel\nprint('Best Param (maxBins): ', bestModel._java_obj.getMaxBins())\nprint('Best Param (maxDepth): ', bestModel._java_obj.getMaxDepth())\nprint('Best Param (maxIter): ', bestModel._java_obj.getMaxIter())\n\n\nBest Param (maxBins):  60\nBest Param (maxDepth):  6\nBest Param (maxIter):  10\n\n\n\n\nCode\nprEvaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\nprint('Testing set areaUnderROC with 5-fold CV: ' + str(evaluator.evaluate(predictions)))\nprint('Testing set prUnderROC with 5-fold CV: ' + str(prEvaluator.evaluate(predictions)))\n\n\n                                                                                [Stage 9199:&gt;                                                       (0 + 4) / 4]                                                                                \n\n\nTesting set areaUnderROC with 5-fold CV: 0.714759267502886\nTesting set prUnderROC with 5-fold CV: 0.5218540520206271\n\n\n\n\n\nNeural Network\n\n\nCode\n# Specify layers for the neural network:\n# input layer of size 48 (features)\n# two intermediate of size 5 and 4\n# output layer of size 2 (classes)\nlayers = [48, 5, 4, 2]\nmlp = MultilayerPerceptronClassifier(layers=layers)\nmlpModel = mlp.fit(train.select(\"label\", \"features\"))\nfittedTest = mlpModel.transform(train.select(\"label\", \"features\"))\nfittedTest.select(\"label\", \"prediction\", \"rawPrediction\", \"probability\").show(5, truncate=False)\n\n\n[Stage 9368:&gt;                                                       (0 + 1) / 1]                                                                                \n\n\n+-----+----------+---------------------------------------+----------------------------------------+\n|label|prediction|rawPrediction                          |probability                             |\n+-----+----------+---------------------------------------+----------------------------------------+\n|0.0  |1.0       |[0.6694605964711076,1.5932109322921044]|[0.2841943542000967,0.7158056457999034] |\n|0.0  |1.0       |[0.6694605964711076,1.5932109322921044]|[0.2841943542000967,0.7158056457999034] |\n|0.0  |1.0       |[0.7028500237246046,1.5571600329283166]|[0.29852951702295405,0.7014704829770461]|\n|0.0  |1.0       |[0.7200003772186417,1.538593222527554] |[0.30606244138998756,0.6939375586100125]|\n|0.0  |1.0       |[0.8672648754399177,1.3776775117171263]|[0.37509679893840125,0.6249032010615987]|\n+-----+----------+---------------------------------------+----------------------------------------+\nonly showing top 5 rows\n\n\n\n\n\nCode\naucevaluator = BinaryClassificationEvaluator()\nprEvaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\nprint('Testing set areaUnderROC: ' + str(aucevaluator.evaluate(fittedTest)))\nprint('Testing set prUnderROC: ' + str(prEvaluator.evaluate(fittedTest)))\n\n\n                                                                                                                                                                \n\n\nTesting set areaUnderROC: 0.7067716820801895\nTesting set prUnderROC: 0.4981231751247994\n\n\n\nCross-validation\n\n\nCode\n# Create parameter grid, add grids for three parameters, and build the parameter grid\n# params = (ParamGridBuilder()\n#           .addGrid(mlp.maxIter, [50, 100])\n#           .addGrid(mlp.stepSize, [0.01, 0.1, 0.5])\n#           .build())\n\n# Create cross-validator\n# cv = CrossValidator(estimator=mlp, estimatorParamMaps=params, evaluator=aucevaluator, numFolds=3)\n# cvModel = cv.fit(train)\ncvModel = CrossValidatorModel.load(\"gs://pstat135-cz/models/nncv\")\npredictions = cvModel.transform(test)\n\n\n23/03/19 19:25:47 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n    at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n    at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n    at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\n23/03/19 19:25:49 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n    at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n    at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n    at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\n23/03/19 19:25:49 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n    at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n    at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n    at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\n23/03/19 19:25:49 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n    at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n    at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n    at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\n23/03/19 19:25:49 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n    at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n    at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n    at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:750)\n\n\n\n\nCode\nbestModel = cvModel.bestModel\nprint('Best Param (maxIter): ', bestModel._java_obj.getMaxIter())\nprint('Best Param (maxStepSize): ', bestModel._java_obj.getStepSize())\n\n\nBest Param (maxIter):  100\nBest Param (maxStepSize):  0.01\n\n\n\n\nCode\nprEvaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\nprint('Testing set areaUnderROC with 5-fold CV: ' + str(aucevaluator.evaluate(predictions)))\nprint('Testing set prUnderROC with 5-fold CV: ' + str(prEvaluator.evaluate(predictions)))\n\n\n                                                                                                                                                                \n\n\nTesting set areaUnderROC with 5-fold CV: 0.7072241535233896\nTesting set prUnderROC with 5-fold CV: 0.4966660880885875\n\n\n\n\n\nSummary of models\n\n\n\n\n\n\n\n\n\n\nModel\nBest parameters\nNumber of folds\nTesting set areaUnderROC\nTesting set prUnderROC\n\n\n\n\nLogistic regression\nregParam: 0.01  MaxIter: 30  elasticNetParam: 0.0\n5\n0.7092\n0.4989\n\n\nDecision tree\nmaxBins: 80  maxDepth: 30\n5\n0.5139\n0.3201\n\n\nRandom forest\nnumTrees: 750\nN/A (due to long training time)\n0.6938\n0.4948\n\n\nGradient-boosted tree\nmaxBins: 60  maxDepth: 6  maxIter: 10\n5\n0.7148\n0.5219\n\n\nNeural network\nmaxIter: 100  maxStepSize: 0.01\n3\n0.7072\n0.4967\n\n\n\nThese models tend to yield similar results for the area under the ROC and PR curves, with the exception of the decision tree model (which performed significantly worse due to its high variance). We did not perform cross-validation for the random forest model since training it was computationally expensive. The gradient-boosted tree model appears to perform the best on the test data since it has the largest area under both curves. Although the area under the ROC curve for this model is decent (\\(0.7148\\)), the relatively low area under the PR curve (\\(0.5219\\)) suggests that our “best” model may not necessarily be able to predict voter turnout during the 2016 Alaska general election reliably based on demographic factors alone."
  },
  {
    "objectID": "notebooks/voter-turnout.html#discussion",
    "href": "notebooks/voter-turnout.html#discussion",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "Discussion",
    "text": "Discussion\n\n“All models are wrong, but some are useful.” - George Box\n\nOut of all of the models, the logistic regression model that underwent cross-validation is the most interpretable. Although it is not the best performing one, it has similar ROC-AUC and PR metrics compared to the other models (except the decision tree). The feature importance barplot for the logistic regression model indicates that the most influential factors accounting for change in the log-odds of voter turnout were Party_dummy_Other, County_dummy_ALEUTIANS EAST, County_dummy_BETHEL, Party_dummy_Unknown, Ethnicity_dummy_East and South Asian, Party_dummy_Independence, and County_dummy_NORTHWEST ARCTIC. Most of these features rely on counties in Alaska, people with unknown or minority party preferences, or ethnicity. An article by AAPI Data claims that voter turnout for the Asian American community was at a “record increase” between \\(2012\\) and \\(2016\\); this could influence the amount of pull the Ethnicity_dummy_East and South Asian predictor had on the turnout log-odds. Similarly, a majority of the critical features were county-based during the \\(2016\\) Alaskan general election, and this may be because some candidates had solutions for specific issues within these Alaskan communities and therefore motivated them to vote; additionally, it may even be simply attributable to the county population sizes since we did not normalize the turnout rate with respect to each county. Lastly, the Party_dummy_Other predictor was surprising based on the earlier exploratory data analysis models showcasing the Republican Party as the most willing to vote. An article written by Alaska Public Media explains how Alaska recently moved to ranked-choice voting in \\(2020\\). Such voting implies that campaign groups must reveal where their funding is coming from, and the funding source could be essential information for voters based on their party. Due to this not being a law in the \\(2016\\) election, it is possible that voters from minority voting parties were misled and did not have as many problems coming out and voting based on presented information from campaign groups. The legal adjustment is specifically relevant to Alaskan voters because Ballot Measure 2 explained above informed them of campaigns with out-of-state funding. To some voters who are likely more conservative or interested in local work, seeing candidates having campaigns with external funding could deter them or encourage them to vote. Withholding funding information could also encourage more voters to vote in \\(2016\\) again to rectify being misled.\nIn general, this bar plot is one of the most informative and applicable parts of this entire research project if interpreted correctly. For political campaigners, knowing which counties or subpopulations are underrepresented in terms of feature importance is how they decide where or who to focus their efforts on. For example, the plot shows that County_dummy_BRISTOL BAY is the least significant predictor in voter turnout. However, this result could be dependent on how many people are living in this specific county. A drawback of evaluating county turnouts is that candidates cannot use the least significant county coefficient as a proxy for underrepresentation. Rather, normalizing turnouts with respect to each county allows for trends in specific counties, ethnicities, incomes, etc. to become more apparent, which can be used to target underrepresented voter groups for more votes in the long run.\nOne limitation of our analysis is that after preprocessing our data, we were left with around 72.5% of the individuals who voted in the general election. This means that our models may be better at predicting the majority class (those who voted) and worse at predicting the minority class (those who didn’t vote). One idea to remedy this would be to use oversampling or undersampling to ensure that the classes are balanced. Additionally, the large number of missing values caused us to cut our data down to almost a third of its regular size. In a statistical sense, this means we can not generalize our results to the entire state of Alaska. In the future, we could look into imputing the missing values with the mean for numerical columns and the mode for categorical columns. Furthermore, we could also incorporate interaction terms into our model to break the assumption that all of the predictors are independent. In conclusion, based on the logistic regression model, it is apparent that the most influential factors in the 2016 Alaska general election were based on specific parties, counties, and ethnicities."
  },
  {
    "objectID": "notebooks/voter-turnout.html#references",
    "href": "notebooks/voter-turnout.html#references",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "References",
    "text": "References\n\nData Council - How Data is Transforming Politics\nKent State University - Chi-Square Test of Independence"
  },
  {
    "objectID": "notebooks/ds-salaries.html",
    "href": "notebooks/ds-salaries.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Hannah Minsuh Choi, Justin Liu, Shulei Wang"
  },
  {
    "objectID": "notebooks/ds-salaries.html#background",
    "href": "notebooks/ds-salaries.html#background",
    "title": "Predicting Data Science Salaries",
    "section": "Background",
    "text": "Background\n\nMotivation\nIn the ever-evolving landscape of data science, understanding the factors influencing salaries is paramount for both workers and employers. For workers, this analysis could help them determine what a reasonable salary is for their desired role; for employers, knowing the key factors influencing salaries could allow them to develop competitive compensation strategies to attract and retain top talent.\nThis research project delves into the dynamic realm of data science salaries, with a specific focus on the period spanning from 2020 to 2023. The primary objective is to identify and analyze the key variables that play a crucial role in shaping compensation within the field of data science.\n\n\nResearch Question\nThe central inquiry guiding this investigation is: “What are the key variables that affect salaries in the Data Science field?” Through an intricate examination of various parameters, we aim to unravel the intricate web of factors that contribute to the fluctuations and trends in data science salaries over the specified time frame.\n\n\nDataset\nThe dataset for this research project is sourced from ai-jobs.net, a site that aggregates data science jobs in areas such as artificial intelligence (AI), machine learning (ML), and data analytics. The site also collects anonymized salary information from professionals working in data science all over the world and makes the data publicly available for anyone to use here.\nAs of December 4, 2023, this dataset contains 9,500 observations and a range of variables relevant to data science salaries (experience level, company size, etc.), allowing for a nuanced exploration of the various factors influencing compensation within the data science industry.\nBelow are the variable descriptions of the original dataset:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nwork_year\nYear when the salary was paid\n\n\nexperience_level\nEN: Entry-level / Junior  MI: Mid-level / Intermediate  SE: Senior-level / Expert  EX: Executive-level / Director\n\n\nemployment_type\nPT: Part-time  FT: Full-time  CT: Contract  FL: Freelance\n\n\njob_title\nRole worked in during the year\n\n\nsalary\nTotal gross salary\n\n\nsalary_currency\nCurrency of the salary paid as an ISO 4217 currency code\n\n\nsalary_in_usd\nSalary in USD ($)\n\n\nemployee_residence\nEmployee’s primary country of residence as an ISO 3166 country code\n\n\nremote_ratio\n0: No or less than 20% remote work  50: hybrid  100: Fully more than 80% remote work\n\n\ncompany_location\nCountry of the employer’s main office or country as an ISO 3166 country code\n\n\ncompany_size\nS: less than 50 employees (small)  M: 50 to 250 employees (medium)  L: more than 250 employees (large)"
  },
  {
    "objectID": "notebooks/ds-salaries.html#preprocessing-eda",
    "href": "notebooks/ds-salaries.html#preprocessing-eda",
    "title": "Predicting Data Science Salaries",
    "section": "Preprocessing & EDA",
    "text": "Preprocessing & EDA\n\nImports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, shapiro, norm\nimport pycountry_convert as pc\n\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\n\n\n\nData Cleaning\n\n# load raw data\nsalaries = pd.read_csv(\"salaries.csv\")\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary\nsalary_currency\nsalary_in_usd\nemployee_residence\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSE\nFT\nData Engineer\n196000\nUSD\n196000\nUS\n100\nUS\nM\n\n\n1\n2023\nSE\nFT\nData Engineer\n94000\nUSD\n94000\nUS\n100\nUS\nM\n\n\n2\n2023\nSE\nFT\nData Scientist\n264846\nUSD\n264846\nUS\n0\nUS\nM\n\n\n3\n2023\nSE\nFT\nData Scientist\n143060\nUSD\n143060\nUS\n0\nUS\nM\n\n\n4\n2023\nEN\nFT\nData Analyst\n203000\nUSD\n203000\nUS\n0\nUS\nM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSE\nFT\nData Scientist\n412000\nUSD\n412000\nUS\n100\nUS\nL\n\n\n9496\n2021\nMI\nFT\nPrincipal Data Scientist\n151000\nUSD\n151000\nUS\n100\nUS\nL\n\n\n9497\n2020\nEN\nFT\nData Scientist\n105000\nUSD\n105000\nUS\n100\nUS\nS\n\n\n9498\n2020\nEN\nCT\nBusiness Data Analyst\n100000\nUSD\n100000\nUS\n100\nUS\nL\n\n\n9499\n2021\nSE\nFT\nData Science Manager\n7000000\nINR\n94665\nIN\n50\nIN\nL\n\n\n\n\n9500 rows × 11 columns\n\n\n\n\n# check datatypes\nsalaries.dtypes\n\nwork_year              int64\nexperience_level      object\nemployment_type       object\njob_title             object\nsalary                 int64\nsalary_currency       object\nsalary_in_usd          int64\nemployee_residence    object\nremote_ratio           int64\ncompany_location      object\ncompany_size          object\ndtype: object\n\n\n\n# check missing values\nsalaries.isnull().sum()\n\nwork_year             0\nexperience_level      0\nemployment_type       0\njob_title             0\nsalary                0\nsalary_currency       0\nsalary_in_usd         0\nemployee_residence    0\nremote_ratio          0\ncompany_location      0\ncompany_size          0\ndtype: int64\n\n\n\n# get summary of predictors (note the number of unique values in each variable)\nsalaries.describe(include=[\"O\"]).sort_values(\"unique\", axis=1)\n\n\n\n\n\n\n\n\ncompany_size\nexperience_level\nemployment_type\nsalary_currency\ncompany_location\nemployee_residence\njob_title\n\n\n\n\ncount\n9500\n9500\n9500\n9500\n9500\n9500\n9500\n\n\nunique\n3\n4\n4\n22\n74\n86\n126\n\n\ntop\nM\nSE\nFT\nUSD\nUS\nUS\nData Engineer\n\n\nfreq\n8538\n6768\n9454\n8656\n8196\n8147\n2216\n\n\n\n\n\n\n\n\n# drop redundant variables\nsalaries = salaries.drop([\"salary\", \"salary_currency\", \"employee_residence\"], axis=1)\n\n# remove duplicate rows\nsalaries = salaries[~salaries.duplicated()]\n\n# convert to categorical variables\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].astype(\"object\")\nsalaries[\"work_year\"] = salaries[\"work_year\"].astype(\"object\")\n\n# recode several variables to make the categories easier to read\nsalaries[\"experience_level\"] = salaries[\"experience_level\"].replace({\n    \"EN\": \"Entry-level\",\n    \"MI\": \"Mid-level\",\n    \"SE\": \"Senior-level\",\n    \"EX\": \"Executive-level\"\n})\n\nsalaries[\"employment_type\"] = salaries[\"employment_type\"].replace({\n    \"PT\": \"Part-time\",\n    \"FT\": \"Full-time\",\n    \"CT\": \"Contract\",\n    \"FL\": \"Freelance\"\n})\n\nsalaries[\"remote_ratio\"] = salaries[\"remote_ratio\"].replace({\n    0: \"No remote work\",\n    50: \"Partially remote\",\n    100: \"Fully remote\"\n})\n\nsalaries[\"company_size\"] = salaries[\"company_size\"].replace({\n    \"S\": \"Small\",\n    \"M\": \"Medium\",\n    \"L\": \"Large\"\n})\n\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\njob_title\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\n\n\n\n\n0\n2023\nSenior-level\nFull-time\nData Engineer\n196000\nFully remote\nUS\nMedium\n\n\n1\n2023\nSenior-level\nFull-time\nData Engineer\n94000\nFully remote\nUS\nMedium\n\n\n2\n2023\nSenior-level\nFull-time\nData Scientist\n264846\nNo remote work\nUS\nMedium\n\n\n3\n2023\nSenior-level\nFull-time\nData Scientist\n143060\nNo remote work\nUS\nMedium\n\n\n4\n2023\nEntry-level\nFull-time\nData Analyst\n203000\nNo remote work\nUS\nMedium\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\nData Scientist\n412000\nFully remote\nUS\nLarge\n\n\n9496\n2021\nMid-level\nFull-time\nPrincipal Data Scientist\n151000\nFully remote\nUS\nLarge\n\n\n9497\n2020\nEntry-level\nFull-time\nData Scientist\n105000\nFully remote\nUS\nSmall\n\n\n9498\n2020\nEntry-level\nContract\nBusiness Data Analyst\n100000\nFully remote\nUS\nLarge\n\n\n9499\n2021\nSenior-level\nFull-time\nData Science Manager\n94665\nPartially remote\nIN\nLarge\n\n\n\n\n5456 rows × 8 columns\n\n\n\n\n# count values in company location\nsalaries.value_counts(\"company_location\")\n\ncompany_location\nUS    4338\nGB     365\nCA     200\nDE      72\nES      61\n      ... \nHN       1\nMU       1\nMT       1\nMD       1\nAD       1\nName: count, Length: 74, dtype: int64\n\n\n\n# count values in job title\nsalaries.value_counts(\"job_title\")\n\njob_title\nData Engineer                      1114\nData Scientist                     1066\nData Analyst                        761\nMachine Learning Engineer           524\nAnalytics Engineer                  210\n                                   ... \nPower BI Developer                    1\nDeep Learning Researcher              1\nMarketing Data Engineer               1\nManaging Director Data Science        1\nStaff Machine Learning Engineer       1\nName: count, Length: 126, dtype: int64\n\n\n\ndef assign_field(job_title):\n    \"\"\"Assigns a broader job field based on a given job title.\"\"\"\n    ai_ml = ['AI Architect', 'AI Developer', 'AI Engineer', 'AI Programmer', 'AI Research Engineer', 'AI Scientist', 'Applied Machine Learning Engineer', 'Applied Machine Learning Scientist', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Deep Learning Engineer', 'Deep Learning Researcher', 'Head of Machine Learning', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Developer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Manager', 'Machine Learning Modeler', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Researcher', 'Machine Learning Scientist', 'Machine Learning Software Engineer', 'Machine Learning Specialist', 'NLP Engineer', 'Principal Machine Learning Engineer', 'Staff Machine Learning Engineer']\n    business = ['BI Analyst', 'BI Data Analyst', 'BI Data Engineer', 'BI Developer', 'Business Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Business Intelligence Developer', 'Business Intelligence Engineer', 'Business Intelligence Manager', 'Business Intelligence Specialist', 'Compliance Data Analyst', 'Consultant Data Engineer', 'Data Product Manager', 'Data Product Owner', 'Data Science Consultant', 'Data Specialist', 'Data Strategist', 'Data Strategy Manager', 'Decision Scientist', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Manager Data Management', 'Marketing Data Analyst', 'Marketing Data Engineer', 'Power BI Developer', 'Product Data Analyst', 'Sales Data Analyst']\n    cloud_computing = ['AWS Data Architect', 'Azure Data Engineer', 'Big Data Architect', 'Big Data Engineer', 'Cloud Data Architect', 'Cloud Data Engineer', 'Cloud Database Engineer']\n    data_analytics = ['Analytics Engineer', 'Analytics Engineering Manager', 'Applied Data Scientist', 'Data Analyst', 'Data Analytics Consultant', 'Data Analytics Engineer', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Analytics Specialist', 'Data Architect', 'Data DevOps Engineer', 'Data Developer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Integration Specialist', 'Data Lead', 'Data Management Analyst', 'Data Management Specialist', 'Data Manager', 'Data Modeler', 'Data Modeller', 'Data Operations Analyst', 'Data Operations Engineer', 'Data Operations Manager', 'Data Operations Specialist', 'Data Quality Analyst', 'Data Quality Engineer', 'Data Science Director', 'Data Science Engineer', 'Data Science Lead', 'Data Science Manager', 'Data Science Practitioner', 'Data Science Tech Lead', 'Data Scientist', 'Data Scientist Lead', 'Data Visualization Analyst', 'Data Visualization Engineer', 'Data Visualization Specialist', 'Director of Data Science', 'ETL Developer', 'ETL Engineer', 'Lead Data Analyst', 'Head of Data', 'Head of Data Science', 'Lead Data Engineer', 'Lead Data Scientist', 'Managing Director Data Science', 'Principal Data Analyst', 'Principal Data Architect', 'Principal Data Engineer', 'Principal Data Scientist', 'Staff Data Analyst', 'Staff Data Scientist']\n    # others = ['Applied Scientist', 'Autonomous Vehicle Technician', 'Research Analyst', 'Research Engineer', 'Research Scientist', 'Software Data Engineer']\n\n    if job_title in ai_ml:\n        return \"AI / ML\"\n    elif job_title in business:\n        return \"Business\"\n    elif job_title in cloud_computing:\n        return \"Cloud Computing\"\n    elif job_title in data_analytics:\n        return \"Data Analytics\"\n    else:\n        return \"Other\"\n\ndef assign_job_category(job_title):\n    \"\"\"Assigns a broader job category based on a given job title.\"\"\"\n    data_engineer = ['AI Engineer', 'AI Research Engineer', 'Analytics Engineer', 'Applied Machine Learning Engineer', 'Azure Data Engineer', 'BI Data Engineer', 'Big Data Engineer', 'Business Intelligence Engineer', 'Cloud Data Engineer', 'Cloud Database Engineer', 'Computer Vision Engineer', 'Computer Vision Software Engineer', 'Consultant Data Engineer', 'Data Analytics Engineer', 'Data DevOps Engineer', 'Data Engineer', 'Data Infrastructure Engineer', 'Data Integration Engineer', 'Data Operations Engineer', 'Data Quality Engineer', 'Data Science Engineer', 'Data Visualization Engineer', 'Deep Learning Engineer', 'ETL Engineer', 'Lead Data Engineer', 'Lead Machine Learning Engineer', 'ML Engineer', 'MLOps Engineer', 'Machine Learning Engineer', 'Machine Learning Infrastructure Engineer', 'Machine Learning Operations Engineer', 'Machine Learning Research Engineer', 'Machine Learning Software Engineer', 'Marketing Data Engineer', 'NLP Engineer', 'Principal Data Engineer', 'Principal Machine Learning Engineer', 'Research Engineer', 'Software Data Engineer', 'Staff Machine Learning Engineer']\n    data_scientist = ['AI Scientist', 'Applied Data Scientist', 'Applied Machine Learning Scientist', 'Applied Scientist', 'Data Scientist', 'Decision Scientist', 'Lead Data Scientist', 'Machine Learning Scientist', 'Principal Data Scientist', 'Research Scientist', 'Staff Data Scientist']\n    data_analyst = ['Business Data Analyst', 'BI Analyst', 'BI Data Analyst', 'Business Intelligence Analyst', 'Business Intelligence Data Analyst', 'Compliance Data Analyst', 'Data Analyst', 'Data Management Analyst', 'Data Operations Analyst', 'Data Quality Analyst', 'Data Visualization Analyst', 'Finance Data Analyst', 'Financial Data Analyst', 'Insight Analyst', 'Lead Data Analyst', 'Marketing Data Analyst', 'Principal Data Analyst', 'Product Data Analyst', 'Research Analyst', 'Sales Data Analyst', 'Staff Data Analyst']\n    developer = ['AI Developer', 'BI Developer', 'Business Intelligence Developer', 'Data Developer', 'ETL Developer', 'Machine Learning Developer', 'Power BI Developer']\n    data_architect = ['AI Architect', 'AWS Data Architect', 'Big Data Architect', 'Cloud Data Architect', 'Data Architect', 'Principal Data Architect']\n    data_specialist = ['Business Intelligence Specialist', 'Data Analytics Specialist', 'Data Integration Specialist', 'Data Management Specialist', 'Data Operations Specialist', 'Data Specialist', 'Data Visualization Specialist', 'Machine Learning Specialist']\n    management = ['Analytics Engineering Manager', 'Business Intelligence Manager', 'Data Analytics Lead', 'Data Analytics Manager', 'Data Lead', 'Data Manager', 'Data Operations Manager', 'Data Product Manager', 'Data Product Owner', 'Data Science Director', 'Data Science Lead', 'Data Science Manager', 'Data Science Tech Lead', 'Data Scientist Lead', 'Data Strategy Manager', 'Director of Data Science', 'Head of Data', 'Head of Data Science', 'Head of Machine Learning', 'Machine Learning Manager', 'Manager Data Management', 'Managing Director Data Science']\n    # other = ['AI Programmer', 'Autonomous Vehicle Technician', 'Data Analytics Consultant', 'Data Modeler', 'Data Modeller', 'Data Science Consultant', 'Data Science Practitioner', 'Data Strategist', 'Deep Learning Researcher', 'Machine Learning Modeler', 'Machine Learning Researcher']\n\n    if job_title in data_engineer:\n        return \"Data Engineer\"\n    elif job_title in data_scientist:\n        return \"Data Scientist\"\n    elif job_title in data_analyst:\n        return \"Data Analyst\"\n    elif job_title in developer:\n        return \"Developer\"\n    elif job_title in data_architect:\n        return \"Data Architect\"\n    elif job_title in data_specialist:\n        return \"Data Specialist\"\n    elif job_title in management:\n        return \"Management\"\n    else:\n        return \"Other\"\n\ndef country_code_to_continent(country_code):\n    \"\"\"Takes a country code and returns the continent where the country is located. The \n    exceptions are the United States, Great Britain, and Canada (the most common countries\n    in our data), for which the country's names are returned.\n    \"\"\"\n    if country_code == \"US\":\n        return \"United States\"\n    elif country_code == \"GB\":\n        return \"Great Britain\"\n    elif country_code == \"CA\":\n        return \"Canada\"\n    else:\n        continent_code = pc.country_alpha2_to_continent_code(country_code)\n\n        continents = {\n            \"NA\": \"North America\",\n            \"SA\": \"South America\",\n            \"AS\": \"Asia\",\n            \"OC\": \"Australia\",\n            \"AF\": \"Africa\",\n            \"EU\": \"Europe\"\n        }\n\n        return continents[continent_code]\n\n# group variables with many categories\nsalaries[\"job_field\"] = salaries[\"job_title\"].apply(assign_field)\nsalaries[\"job_category\"] = salaries[\"job_title\"].apply(assign_job_category)\nsalaries[\"company_location\"] = salaries[\"company_location\"].apply(country_code_to_continent)\n\n# drop unneeded variable\nsalaries = salaries.drop([\"job_title\"], axis=1)\nsalaries\n\n\n\n\n\n\n\n\nwork_year\nexperience_level\nemployment_type\nsalary_in_usd\nremote_ratio\ncompany_location\ncompany_size\njob_field\njob_category\n\n\n\n\n0\n2023\nSenior-level\nFull-time\n196000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n1\n2023\nSenior-level\nFull-time\n94000\nFully remote\nUnited States\nMedium\nData Analytics\nData Engineer\n\n\n2\n2023\nSenior-level\nFull-time\n264846\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n3\n2023\nSenior-level\nFull-time\n143060\nNo remote work\nUnited States\nMedium\nData Analytics\nData Scientist\n\n\n4\n2023\nEntry-level\nFull-time\n203000\nNo remote work\nUnited States\nMedium\nData Analytics\nData Analyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9495\n2020\nSenior-level\nFull-time\n412000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9496\n2021\nMid-level\nFull-time\n151000\nFully remote\nUnited States\nLarge\nData Analytics\nData Scientist\n\n\n9497\n2020\nEntry-level\nFull-time\n105000\nFully remote\nUnited States\nSmall\nData Analytics\nData Scientist\n\n\n9498\n2020\nEntry-level\nContract\n100000\nFully remote\nUnited States\nLarge\nBusiness\nData Analyst\n\n\n9499\n2021\nSenior-level\nFull-time\n94665\nPartially remote\nAsia\nLarge\nData Analytics\nManagement\n\n\n\n\n5456 rows × 9 columns\n\n\n\nHere is a summary of the steps that we took to get the cleaned dataset above:\n\nWe dropped some redundant variables.\n\nWe dropped salary and salary_currency since they were used to calculate salary_in_usd.\nWe also dropped employee_residence since it is very similar to company_location.\n\nWe removed duplicate rows to ensure that each observation was unique.\n\nAs a result, more than 4000 rows were dropped.\n\nWe converted several numerical variables to categorical variables.\n\nWe turned remote_ratio and work_year into categorical variables since they did not have many unique values (3 and 4, respectively).\n\nWe recoded the abbreviated forms of some categories to make them easier to read.\n\nFor example, in experience_level, we changed \"EN\" to \"Entry-level, \"MI\" to Mid-level, and so on.\n\nWe manually grouped the levels in a few categorical variables into broader groups.\n\nSome of the levels only had one observation (for example, the job title \"Managing Director Data Science\" only appears once in the data).\nFor job_title, we created new categorical variables called job_field (area of data science) and job_category (type of job).\nFor company_location, we converted each country into the continent where each one is located, with the exception of the United States, Great Britain, and Canada (the countries that appeared the most in our data).\nWe then dropped job_title from our dataset.\n\n\n\n\nData Visualizations\n\n# get predictors\npredictors = salaries.drop(\"salary_in_usd\", axis=1)\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 20))\nax = axes.flatten()\n\n# choose color scheme\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make bar plot of counts\n    sns.countplot(data=salaries,\n                  x=col,\n                  order=salaries[col].value_counts(ascending=False).iloc[:10].index,\n                  color=colors[i],\n                  ax=ax[i])\n\n    # add counts on top of bars\n    ax[i].bar_label(ax[i].containers[0])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n    ax[i].set(ylabel=\"Count\")\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/591366537.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/591366537.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/591366537.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/591366537.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/591366537.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/591366537.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/591366537.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/591366537.py:30: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# plot salary distribution\nplt.figure(figsize=(12,6))\nsns.histplot(salaries['salary_in_usd'], bins=20, kde=True)\nplt.title('Distribution of Salaries in USD')\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n# make subplots\nfig, axes = plt.subplots(4, 2, figsize=(23, 25))\nax = axes.flatten()\n\n# choose color schemes\ncolors = sns.color_palette(\"hls\", 8)\n\n# for each predictor\nfor i, col in enumerate(predictors.columns):\n    # make boxplot of salary vs. the predictor\n    sns.boxplot(data=salaries,\n                x=col,\n                y=\"salary_in_usd\",\n                order=salaries.groupby(col).median(\"salary_in_usd\").sort_values(by=\"salary_in_usd\", ascending=False).iloc[:10].index,\n                color=colors[i],\n                ax=ax[i])\n\n    # set title and x-axis labels\n    title = col.capitalize().replace(\"_\", \" \")\n    ax[i].set(title=title)\n    ax[i].set(xlabel=None)\n    ax[i].set(ylabel=\"Salary in USD ($)\")\n\n    # add x-axis tick labels\n    if title == \"Job title\":\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=35, ha=\"right\", fontsize=10)\n    else:\n        ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/287043558.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/287043558.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/287043558.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/287043558.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/287043558.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/287043558.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/287043558.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n/var/folders/93/9sds1y6s6lsc571jns5mh6f80000gn/T/ipykernel_31147/287043558.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize=10)\n\n\n\n\n\n\n# encode categorical variables\nsalaries_copy = salaries.copy()\nsalaries_copy['experience_level'] = salaries_copy['experience_level'].astype('category').cat.codes\nsalaries_copy['employment_type'] = salaries_copy['employment_type'].astype('category').cat.codes\nsalaries_copy['remote_ratio'] = salaries_copy['remote_ratio'].astype('category').cat.codes\nsalaries_copy['job_category'] = salaries_copy['job_category'].astype('category').cat.codes\nsalaries_copy['job_field'] = salaries_copy['job_field'].astype('category').cat.codes\nsalaries_copy['company_location'] = salaries_copy['company_location'].astype('category').cat.codes\nsalaries_copy['company_size'] = salaries_copy['company_size'].astype('category').cat.codes\n\n# plot heatmap\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# plot correlation matrix\nplt.figure(figsize=(10, 5))\nc = salaries_copy.corr()\nc[c.abs() &lt; 0.2] = np.nan\nsns.heatmap(c, cmap=\"RdYlGn\", annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\nThere are some comparably high correlations above:\n\ncompany_location to salary_in_usd (0.35)\nexperience_level to salary_in_usd (0.3)\ncompany_location to work_year (0.21)"
  },
  {
    "objectID": "notebooks/ds-salaries.html#modeling",
    "href": "notebooks/ds-salaries.html#modeling",
    "title": "Predicting Data Science Salaries",
    "section": "Modeling",
    "text": "Modeling\n\nGoal: predict data science salaries (in USD) using variables such as experience level, company size, etc.\nModels: dummy regression, linear regression, LASSO regression, random forest, boosted trees, neural network\n\n\nConsiderations\n\nOne-hot encoding: allows our models to use categorical predictors by representing them as numbers\nCross-validation: gives a better estimate of each model’s performance by using multiple train-test splits and averaging the errors\nHyperparameter tuning: allows us to test multiple combinations of parameters to find the “best” set\nMetrics: since we are predicting salaries (regression), mean absolute error (MAE) and root mean squared error (RMSE) are good metrics\n\n\n\nData Preparation\n\n# define predictor and target\nX = salaries.drop(\"salary_in_usd\", axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = salaries[\"salary_in_usd\"]\n\n# create 5-fold CV object\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# get training and test sets for the 1st fold (used for visualization purposes later)\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n\nX_train # predictors training set\n\n\n\n\n\n\n\n\nwork_year_2021\nwork_year_2022\nwork_year_2023\nexperience_level_Executive-level\nexperience_level_Mid-level\nexperience_level_Senior-level\nemployment_type_Freelance\nemployment_type_Full-time\nemployment_type_Part-time\nremote_ratio_No remote work\n...\njob_field_Cloud Computing\njob_field_Data Analytics\njob_field_Other\njob_category_Data Architect\njob_category_Data Engineer\njob_category_Data Scientist\njob_category_Data Specialist\njob_category_Developer\njob_category_Management\njob_category_Other\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n6\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9490\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n9491\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9492\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9493\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9498\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n4364 rows × 32 columns\n\n\n\n\ny_train # target test set\n\n0       196000\n1        94000\n2       264846\n6        64781\n7        41027\n         ...  \n9490    168000\n9491    119059\n9492    423000\n9493     28369\n9498    100000\nName: salary_in_usd, Length: 4364, dtype: int64\n\n\n\n\nModel #1: Dummy Regression\n\nExplanation: predicts the mean of the target variable for every observation\nPurpose: acts as a baseline model since no patterns are being learned\n\n\n# define dummy model\ndummy_model = DummyRegressor(strategy=\"mean\")\n\n# perform cross-validation\ndummy_cv_results = pd.DataFrame(\n    cross_validate(dummy_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True, \n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\ndummy_cv_results = dummy_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\ndummy_cv_results = dummy_cv_results.mean() * -1\ndummy_cv_results = pd.DataFrame(dummy_cv_results, columns=[\"dummy_cv\"])\ndummy_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ndummy_cv_results\n\n\n\n\n\n\n\n\ndummy_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n\n\ntrain_mae_avg\n53793.468421\n\n\ntest_rmse_avg\n68989.733389\n\n\ntrain_rmse_avg\n68988.669716\n\n\n\n\n\n\n\n\n# fit model and make predictions\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test)\nplt.xlim(0, max(y_pred) + 10000)\nplt.ylim(0, max(y_test) + 10000)\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Actual values\")\nplt.title(\"Dummy model actual vs. predicted values\")\nplt.plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n\n\n\n\n\nModel #2: Linear Regression\n\nExplanation: fits a straight line through the points\nPurpose: has interpretable coefficients, acts as another good baseline model due to simplicity\n\n\n# define model\nlm_model = LinearRegression()\n\n# perform cross-validation\nlm_cv_results = pd.DataFrame(\n    cross_validate(lm_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"])\n)\n\n# output results\nlm_cv_results = lm_cv_results.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlm_cv_results = lm_cv_results.mean() * -1\nlm_cv_results = pd.DataFrame(lm_cv_results, columns=[\"linear_cv\"])\nlm_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlm_cv_results\n\n\n\n\n\n\n\n\nlinear_cv\n\n\n\n\ntest_mae_avg\n41381.359833\n\n\ntrain_mae_avg\n41030.051241\n\n\ntest_rmse_avg\n55509.230202\n\n\ntrain_rmse_avg\n55031.376211\n\n\n\n\n\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train)\ny_pred = lm_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f74c7c430&gt;\n\n\n\n\n\n\n# show intercept\nlm_model.intercept_\n\n63760.94720830995\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lm_model.coef_, X.columns, columns=['Coefficient']).sort_values(\"Coefficient\", ascending=False)\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nexperience_level_Executive-level\n83323.776156\n\n\nexperience_level_Senior-level\n51495.868648\n\n\ncompany_location_United States\n47530.452030\n\n\ncompany_location_Australia\n46139.237258\n\n\njob_category_Management\n45648.716897\n\n\njob_category_Data Architect\n43499.526991\n\n\njob_category_Data Scientist\n41016.056761\n\n\ncompany_location_Canada\n33061.791964\n\n\njob_category_Data Engineer\n32288.852697\n\n\nexperience_level_Mid-level\n21427.628620\n\n\njob_category_Developer\n9104.008704\n\n\ncompany_location_Great Britain\n8930.307113\n\n\nwork_year_2023\n5994.745590\n\n\nremote_ratio_No remote work\n5703.261883\n\n\njob_category_Other\n4274.565016\n\n\ncompany_location_North America\n2076.927571\n\n\ncompany_size_Medium\n681.994569\n\n\nwork_year_2022\n-1889.416001\n\n\nwork_year_2021\n-3798.431407\n\n\nremote_ratio_Partially remote\n-4457.520871\n\n\nemployment_type_Full-time\n-8428.675356\n\n\njob_category_Data Specialist\n-8867.077234\n\n\njob_field_Other\n-10099.359621\n\n\nemployment_type_Part-time\n-11141.377870\n\n\njob_field_Cloud Computing\n-13579.589608\n\n\ncompany_size_Small\n-15312.098319\n\n\ncompany_location_Europe\n-21012.522137\n\n\ncompany_location_Asia\n-22150.502431\n\n\njob_field_Data Analytics\n-31864.753449\n\n\njob_field_Business\n-32506.076965\n\n\ncompany_location_South America\n-36835.983760\n\n\nemployment_type_Freelance\n-47912.141781\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlm_features = X.columns\nlm_coefs = lm_model.coef_\nlm_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lm_model.coef_])\nlm_indices = np.argsort(lm_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Linear model coefficients\")\nplt.barh(range(len(lm_indices)), lm_coefs[lm_indices], color=lm_colors[lm_indices], align=\"center\")\nplt.yticks(range(len(lm_indices)), [lm_features[i] for i in lm_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\nBased on the coefficient estimates, it appears that experience level, company location, job category, and to some extent employment type have the most significant effects on salaries. For example:\n\nHaving a executive-level or senior-level position can lead to a salary increase of $83,323.78 and $51,495.87, respectively.\nIf the company is located in the United States, then the salary is predicted to increase by $47,530.45.\nWorking in a data management job can increase one’s salary by $45,648.72.\nBeing a freelancer is associated with a salary decrease of $47,912.14.\n\n\n\nModel #3: LASSO Regression\n\nExplanation: linear regression with regularization (shrinks coefficient estimates towards 0)\nPurpose: reduces variance at the cost of increased bias, is also good for variable selection\n\n\n# define model\nlasso_model = LassoCV(n_alphas=1000)\n\n# perform cross-validation\nlasso_cv = pd.DataFrame(\n    cross_validate(lasso_model, \n                   X, \n                   y, \n                   cv=kf, \n                   return_train_score=True, \n                   return_estimator=True,\n                   scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n                   verbose=1)\n)\n\n# show chosen alpha value for each fold\nfor i in range(5):\n    print(f\"Alpha for fold {i}: {lasso_cv.estimator[i].alpha_}\")\n\nAlpha for fold 0: 87.19936067088149\nAlpha for fold 1: 13.377703834418131\nAlpha for fold 2: 43.67836244966341\nAlpha for fold 3: 80.94003666493782\nAlpha for fold 4: 13.151313150908784\n\n\n\n# output results\nlasso_cv_results = lasso_cv.drop([\"fit_time\", \"score_time\", \"estimator\"], axis=1)\nlasso_cv_results = lasso_cv_results.mean() * -1\nlasso_cv_results = pd.DataFrame(lasso_cv_results, columns=[\"lasso_cv\"])\nlasso_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nlasso_cv_results\n\n\n\n\n\n\n\n\nlasso_cv\n\n\n\n\ntest_mae_avg\n41377.526233\n\n\ntrain_mae_avg\n41054.992270\n\n\ntest_rmse_avg\n55498.416245\n\n\ntrain_rmse_avg\n55084.467081\n\n\n\n\n\n\n\n\n# make predictions\nlasso_model = lasso_cv.estimator[0]\ny_pred = lasso_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1f18760&gt;\n\n\n\n\n\n\n# show sorted coefficient estimates\npd.DataFrame(lasso_model.coef_, X.columns, columns=[\"Coefficient\"]).sort_values(\"Coefficient\")\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\ncompany_location_South America\n-31187.164149\n\n\njob_field_Data Analytics\n-29752.561372\n\n\njob_field_Business\n-29374.996468\n\n\ncompany_location_Europe\n-27902.211240\n\n\ncompany_location_Asia\n-27104.704306\n\n\ncompany_size_Small\n-13598.647606\n\n\njob_field_Other\n-7116.957744\n\n\nremote_ratio_Partially remote\n-3555.942276\n\n\njob_category_Data Specialist\n-2528.243008\n\n\nemployment_type_Freelance\n-1150.005088\n\n\nwork_year_2021\n-909.337221\n\n\njob_field_Cloud Computing\n-0.000000\n\n\ncompany_location_North America\n-0.000000\n\n\ncompany_location_Great Britain\n0.000000\n\n\njob_category_Other\n0.000000\n\n\nemployment_type_Part-time\n-0.000000\n\n\nwork_year_2022\n0.000000\n\n\nemployment_type_Full-time\n0.000000\n\n\njob_category_Developer\n592.904123\n\n\ncompany_size_Medium\n1406.113228\n\n\nremote_ratio_No remote work\n5937.982765\n\n\nwork_year_2023\n7946.569016\n\n\ncompany_location_Australia\n17868.178251\n\n\nexperience_level_Mid-level\n18219.588272\n\n\ncompany_location_Canada\n23093.738973\n\n\njob_category_Data Engineer\n30731.052498\n\n\njob_category_Data Architect\n38158.047021\n\n\njob_category_Data Scientist\n38681.000401\n\n\ncompany_location_United States\n39602.048022\n\n\njob_category_Management\n42573.850774\n\n\nexperience_level_Senior-level\n48951.992943\n\n\nexperience_level_Executive-level\n79205.850869\n\n\n\n\n\n\n\n\n# plot coefficient estimates\nlasso_features = X.columns\nlasso_coefs = lasso_model.coef_\nlasso_colors = np.array([\"green\" if coef &gt; 0 else \"red\" for coef in lasso_coefs])\nlasso_indices = np.argsort(lasso_coefs)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"LASSO model coefficients\")\nplt.barh(range(len(lasso_indices)), lasso_coefs[lasso_indices], color=lasso_colors[lasso_indices], align=\"center\")\nplt.yticks(range(len(lasso_indices)), [lasso_features[i] for i in lasso_indices])\nplt.xlabel(\"Coefficient value\")\nplt.show()\n\n\n\n\n\nThe interpretation of this model is very similar to what we saw for the linear regression model: experience level, company location, and job category play a big role in determining compensation.\nThere were 6 variables that were dropped (have coefficients of 0), 2 of which are associated with company location (North America and Great Britain).\n\n\n\nModel #4: Random Forest\n\nExplanation: fits many independent trees to the data\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\nrf_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": list(range(3, 11)),\n    \"max_features\": [1.0, \"sqrt\", \"log2\"]\n}\n\n# perform cross-validation using hyperparameters\nrf = RandomForestRegressor(random_state=1, n_jobs=4)\nrf_gs = GridSearchCV(rf, \n                     rf_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\nrf_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=RandomForestRegressor(n_jobs=4, random_state=1),\n             param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n                         'max_features': [1.0, 'sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)RandomForestRegressorRandomForestRegressor(n_jobs=4, random_state=1)\n\n\n\n# output results\nrf_cv_results = pd.DataFrame(rf_gs.cv_results_)\nrf_cv_results = rf_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\nrf_cv_results = pd.DataFrame(rf_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\nrf_cv_results = rf_cv_results * -1\nrf_cv_results.columns = [\"rf_cv\"]\nrf_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\nrf_cv_results\n\n\n\n\n\n\n\n\nrf_cv\n\n\n\n\ntest_mae_avg\n41260.309760\n\n\ntrain_mae_avg\n39217.376217\n\n\ntest_rmse_avg\n52729.752517\n\n\ntrain_rmse_avg\n52729.752517\n\n\n\n\n\n\n\n\n# show best hyperparameters in terms of MAE\nrf_best_params = pd.DataFrame(rf_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\nrf_best_params\n\n{'max_depth': 7, 'max_features': 1.0, 'n_estimators': 200}\n\n\n\n# fit model\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest model residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f820d6250&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(rf_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.319051\n\n\nexperience_level_Senior-level\n0.124004\n\n\nexperience_level_Executive-level\n0.098693\n\n\njob_field_Data Analytics\n0.066099\n\n\njob_field_Business\n0.051099\n\n\njob_category_Data Scientist\n0.043705\n\n\njob_category_Data Engineer\n0.042344\n\n\ncompany_location_Canada\n0.030872\n\n\nremote_ratio_No remote work\n0.029522\n\n\njob_category_Management\n0.025278\n\n\nexperience_level_Mid-level\n0.025016\n\n\ncompany_location_Great Britain\n0.017775\n\n\nwork_year_2023\n0.016997\n\n\njob_category_Data Architect\n0.015912\n\n\ncompany_size_Medium\n0.014433\n\n\ncompany_location_Australia\n0.010107\n\n\njob_field_Other\n0.009189\n\n\nemployment_type_Full-time\n0.009085\n\n\nremote_ratio_Partially remote\n0.008467\n\n\nwork_year_2022\n0.007907\n\n\ncompany_size_Small\n0.007407\n\n\nwork_year_2021\n0.006898\n\n\ncompany_location_Europe\n0.006180\n\n\ncompany_location_Asia\n0.003971\n\n\ncompany_location_North America\n0.003000\n\n\njob_category_Developer\n0.001760\n\n\njob_field_Cloud Computing\n0.001758\n\n\njob_category_Data Specialist\n0.001051\n\n\njob_category_Other\n0.000896\n\n\ncompany_location_South America\n0.000837\n\n\nemployment_type_Freelance\n0.000607\n\n\nemployment_type_Part-time\n0.000080\n\n\n\n\n\n\n\n\n# plot feature importances\nrf_features = X.columns\nrf_importances = rf_model.feature_importances_\nrf_indices = np.argsort(rf_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Random forest feature importances\")\nplt.barh(range(len(rf_indices)), rf_importances[rf_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(rf_indices)), [rf_features[i] for i in rf_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nBased on the feature importances, company location, experience level, job field, and job category appear to have the most influence on the predicted salaries.\n\n\nModel #5: Boosted Trees\n\nExplanation: fits many consecutive trees to the residuals\nPurpose: tends to generalize well to new data due to nonlinearity, has feature importance\n\n\n# define hyperparameter grid\ngb_param_grid = {\n    \"n_estimators\": list(range(100, 501, 100)),\n    \"max_depth\": range(3, 11),\n    \"learning_rate\": [0.001, 0.01, 0.1]\n}\n\n# perform cross-validation using hyperparameters\ngb = GradientBoostingRegressor(subsample=0.8, random_state=1)\ngb_gs = GridSearchCV(gb, \n                     gb_param_grid, \n                     cv=kf, \n                     refit=False, \n                     scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"], \n                     return_train_score=True, \n                     verbose=1)\ngb_gs.fit(X, y)\n\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=GradientBoostingRegressor(random_state=1, subsample=0.8),\n             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n                         'max_depth': range(3, 11),\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             refit=False, return_train_score=True,\n             scoring=['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n             verbose=1)estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)GradientBoostingRegressorGradientBoostingRegressor(random_state=1, subsample=0.8)\n\n\n\n# organize results\ngb_cv_results = pd.DataFrame(gb_gs.cv_results_)\ngb_cv_results = gb_cv_results.sort_values(\"mean_test_neg_root_mean_squared_error\", ascending=False)\ngb_cv_results = pd.DataFrame(gb_cv_results[[\"mean_test_neg_mean_absolute_error\", \"mean_train_neg_mean_absolute_error\", \"mean_train_neg_root_mean_squared_error\", \"mean_train_neg_root_mean_squared_error\"]].iloc[0, :])\ngb_cv_results = gb_cv_results * -1\ngb_cv_results.columns = [\"gb_cv\"]\ngb_cv_results.index = [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"]\ngb_cv_results\n\n\n\n\n\n\n\n\ngb_cv\n\n\n\n\ntest_mae_avg\n40949.912097\n\n\ntrain_mae_avg\n38735.669027\n\n\ntest_rmse_avg\n52301.409345\n\n\ntrain_rmse_avg\n52301.409345\n\n\n\n\n\n\n\n\n# best hyperparameters in terms of MAE\ngb_best_params = pd.DataFrame(gb_gs.cv_results_).sort_values(\"mean_test_neg_mean_absolute_error\", ascending=False).params.iloc[0]\ngb_best_params\n\n{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n\n\n\n# fit model\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train)\ny_pred = gb_model.predict(X_test)\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7fa1b6a9d0&gt;\n\n\n\n\n\n\n# show sorted feature importances\npd.DataFrame(gb_model.feature_importances_, X.columns, columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n\n\n\n\n\n\n\n\nImportance\n\n\n\n\ncompany_location_United States\n0.291170\n\n\nexperience_level_Senior-level\n0.116333\n\n\nexperience_level_Executive-level\n0.091701\n\n\njob_field_Data Analytics\n0.065722\n\n\njob_category_Data Scientist\n0.055562\n\n\njob_category_Data Engineer\n0.045098\n\n\njob_field_Business\n0.041057\n\n\njob_category_Management\n0.035165\n\n\ncompany_location_Canada\n0.030436\n\n\nremote_ratio_No remote work\n0.028101\n\n\nexperience_level_Mid-level\n0.022149\n\n\njob_category_Data Architect\n0.021942\n\n\ncompany_location_Great Britain\n0.019671\n\n\nwork_year_2023\n0.019499\n\n\ncompany_size_Medium\n0.016351\n\n\nemployment_type_Full-time\n0.012659\n\n\ncompany_location_Australia\n0.011820\n\n\njob_field_Other\n0.010529\n\n\ncompany_size_Small\n0.009467\n\n\nremote_ratio_Partially remote\n0.009438\n\n\ncompany_location_Asia\n0.008239\n\n\nwork_year_2022\n0.007751\n\n\nwork_year_2021\n0.007364\n\n\ncompany_location_Europe\n0.006149\n\n\ncompany_location_North America\n0.003498\n\n\njob_category_Data Specialist\n0.003020\n\n\njob_field_Cloud Computing\n0.002692\n\n\njob_category_Developer\n0.002628\n\n\ncompany_location_South America\n0.001909\n\n\nemployment_type_Freelance\n0.001372\n\n\nemployment_type_Part-time\n0.000906\n\n\njob_category_Other\n0.000603\n\n\n\n\n\n\n\n\n# plot feature importances\ngb_features = X.columns\ngb_importances = gb_model.feature_importances_\ngb_indices = np.argsort(gb_importances)\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Boosted trees feature importances\")\nplt.barh(range(len(gb_indices)), gb_importances[gb_indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(gb_indices)), [gb_features[i] for i in gb_indices])\nplt.xlabel(\"Relative importance\")\nplt.show()\n\n\n\n\nLike the random forest model, company location, experience level, job field, and job category seem to have the biggest effect in determining data science salaries.\n\n\nModel #6: Neural Network\n\nExplanation: passes values through layers and adjusts predictions based on a loss function\nPurpose: tends to generalize well to new data due to nonlinearity\n\n\n# define neural network architecture\nclass NN(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size4):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.dropout1 = nn.Dropout(0.2)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size_2, hidden_size_3)\n        self.relu3 = nn.ReLU()\n        self.fc4 = nn.Linear(hidden_size_3, hidden_size4)\n        self.fc5 = nn.Linear(hidden_size4, 1)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.dropout1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.relu3(out)\n        out = self.fc4(out)\n        out = self.fc5(out)\n\n        return out\n\n\n# define parameters\ninput_size = X.shape[1]\nhidden_size_1 = 128\nhidden_size_2 = 64\nhidden_size_3 = 32\nhidden_size_4 = 32\nlearning_rate = 0.005\nbatch_size = 16\nnum_epochs = 100\n\nWe first train the neural network using MAE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_mae_avg = []\nall_test_mae_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_mae = []\n    all_test_mae = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_mae = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_mae = mae(y_train_pred, y_train_batch)\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_mae.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_mae += train_mae.item()\n        \n        # calculate average training loss across all batches\n        avg_train_mae = total_train_mae / len(train_loader)\n        all_train_mae.append(avg_train_mae)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_mae = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_mae = mae(y_test_pred, y_test_batch)\n\n                # accumalate test loss for the current batch\n                total_test_mae += test_mae.item()\n        \n        # calculate average test loss across all batches\n        avg_test_mae = total_test_mae / len(test_loader)\n        all_test_mae.append(avg_test_mae)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train MAE: {:6.3f} | Test MAE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_mae, avg_test_mae))\n    \n    # append average losses to lists\n    all_train_mae_avg.append(avg_train_mae)\n    all_test_mae_avg.append(avg_test_mae)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nEpoch:  10 | Train MAE: 40984.143 | Test MAE: 41166.694\nEpoch:  20 | Train MAE: 41211.805 | Test MAE: 41198.991\nEpoch:  30 | Train MAE: 40509.387 | Test MAE: 40928.236\nEpoch:  40 | Train MAE: 40517.772 | Test MAE: 40964.292\nEpoch:  50 | Train MAE: 40349.077 | Test MAE: 41034.909\nEpoch:  60 | Train MAE: 40188.384 | Test MAE: 42106.727\nEpoch:  70 | Train MAE: 40183.729 | Test MAE: 41364.056\nEpoch:  80 | Train MAE: 39898.884 | Test MAE: 42390.790\nEpoch:  90 | Train MAE: 40053.882 | Test MAE: 40921.214\nEpoch: 100 | Train MAE: 39959.194 | Test MAE: 43067.933\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train MAE: 39559.589 | Test MAE: 40244.231\nEpoch:  20 | Train MAE: 39374.867 | Test MAE: 40824.463\nEpoch:  30 | Train MAE: 39506.571 | Test MAE: 42311.580\nEpoch:  40 | Train MAE: 39243.006 | Test MAE: 41512.273\nEpoch:  50 | Train MAE: 38991.453 | Test MAE: 41097.875\nEpoch:  60 | Train MAE: 39072.293 | Test MAE: 41572.459\nEpoch:  70 | Train MAE: 39237.426 | Test MAE: 41957.310\nEpoch:  80 | Train MAE: 38814.862 | Test MAE: 41575.831\nEpoch:  90 | Train MAE: 38520.442 | Test MAE: 41545.777\nEpoch: 100 | Train MAE: 38657.836 | Test MAE: 41306.125\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train MAE: 39632.284 | Test MAE: 36835.344\nEpoch:  20 | Train MAE: 39380.008 | Test MAE: 38353.468\nEpoch:  30 | Train MAE: 39316.230 | Test MAE: 37466.152\nEpoch:  40 | Train MAE: 38972.106 | Test MAE: 37489.210\nEpoch:  50 | Train MAE: 39034.768 | Test MAE: 38507.894\nEpoch:  60 | Train MAE: 39352.910 | Test MAE: 38029.606\nEpoch:  70 | Train MAE: 39222.095 | Test MAE: 38197.621\nEpoch:  80 | Train MAE: 38856.451 | Test MAE: 38412.466\nEpoch:  90 | Train MAE: 39230.784 | Test MAE: 40199.593\nEpoch: 100 | Train MAE: 38593.044 | Test MAE: 38078.749\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train MAE: 38557.751 | Test MAE: 39500.069\nEpoch:  20 | Train MAE: 38503.464 | Test MAE: 39713.800\nEpoch:  30 | Train MAE: 38822.826 | Test MAE: 40216.374\nEpoch:  40 | Train MAE: 38453.210 | Test MAE: 40444.056\nEpoch:  50 | Train MAE: 38112.908 | Test MAE: 40491.519\nEpoch:  60 | Train MAE: 37925.945 | Test MAE: 40837.054\nEpoch:  70 | Train MAE: 38069.501 | Test MAE: 40834.547\nEpoch:  80 | Train MAE: 38057.387 | Test MAE: 41037.705\nEpoch:  90 | Train MAE: 37790.644 | Test MAE: 41034.246\nEpoch: 100 | Train MAE: 37814.119 | Test MAE: 41089.256\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train MAE: 38187.076 | Test MAE: 37210.427\nEpoch:  20 | Train MAE: 37937.163 | Test MAE: 37740.056\nEpoch:  30 | Train MAE: 38117.326 | Test MAE: 38215.403\nEpoch:  40 | Train MAE: 38366.162 | Test MAE: 38634.416\nEpoch:  50 | Train MAE: 37747.963 | Test MAE: 39045.398\nEpoch:  60 | Train MAE: 37782.583 | Test MAE: 38753.302\nEpoch:  70 | Train MAE: 38010.472 | Test MAE: 39145.390\nEpoch:  80 | Train MAE: 37758.089 | Test MAE: 39316.267\nEpoch:  90 | Train MAE: 37745.635 | Test MAE: 39194.593\nEpoch: 100 | Train MAE: 37854.493 | Test MAE: 39119.699\n------------------------------------------------------------\n\n\nWe then train the neural network using RMSE as the loss function.\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmse = nn.MSELoss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# initialize lists\nall_train_rmse_avg = []\nall_test_rmse_avg = []\n\nfor i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n    # get training and test sets\n    X_train = X.iloc[train_indices]\n    X_test = X.iloc[test_indices]\n    y_train = y.iloc[train_indices]\n    y_test = y.iloc[test_indices]\n\n    # transform data to tensors\n    X_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\n    X_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\n    y_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\n    y_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n    # create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # initialize lists\n    all_train_rmse = []\n    all_test_rmse = []\n\n    print(\"-\" * 60)\n    print(f\"Fold {i + 1}\")\n\n    for epoch in range(num_epochs):\n        ### TRAINING\n        nn_model.train()\n\n        # initialize training loss\n        total_train_rmse = 0.0\n\n        for X_train_batch, y_train_batch in train_loader:\n            # forward pass and calculate training loss\n            y_train_pred = nn_model(X_train_batch).squeeze(1)\n            train_rmse = torch.sqrt(mse(y_train_pred, y_train_batch))\n\n            # backward and optimize\n            optimizer.zero_grad()\n            train_rmse.backward()\n            optimizer.step()\n\n            # accumulate training loss for the current batch\n            total_train_rmse += train_rmse.item()\n        \n        # calculate average training loss across all batches\n        avg_train_rmse = total_train_rmse / len(train_loader)\n        all_train_rmse.append(avg_train_rmse)\n\n        ### TESTING\n        nn_model.eval()\n\n        # initialize test loss\n        total_test_rmse = 0.0\n\n        with torch.no_grad():\n            for X_test_batch, y_test_batch in test_loader:\n                # calculate test loss\n                y_test_pred = nn_model(X_test_batch).squeeze(1)\n                test_rmse = torch.sqrt(mse(y_test_pred, y_test_batch))\n\n                # accumalate test loss for the current batch\n                total_test_rmse += test_rmse.item()\n        \n        # calculate average test loss across all batches\n        avg_test_rmse = total_test_rmse / len(test_loader)\n        all_test_rmse.append(avg_test_rmse)\n        \n        # output progress\n        if (epoch + 1) % 10 == 0:\n            print(\"Epoch: {:3d} | Train RMSE: {:6.3f} | Test RMSE: {:6.3f}\" \\\n                .format(epoch + 1, avg_train_rmse, avg_test_rmse))\n    \n    # append average losses to lists\n    all_train_rmse_avg.append(avg_train_rmse)\n    all_test_rmse_avg.append(avg_test_rmse)\n\nprint(\"-\" * 60)\n\n------------------------------------------------------------\nFold 1\nEpoch:  10 | Train RMSE: 52892.504 | Test RMSE: 55182.516\nEpoch:  20 | Train RMSE: 53113.813 | Test RMSE: 54939.477\nEpoch:  30 | Train RMSE: 52509.987 | Test RMSE: 54886.509\nEpoch:  40 | Train RMSE: 52293.515 | Test RMSE: 55316.747\nEpoch:  50 | Train RMSE: 52209.160 | Test RMSE: 54999.319\nEpoch:  60 | Train RMSE: 52329.940 | Test RMSE: 56306.033\nEpoch:  70 | Train RMSE: 52224.897 | Test RMSE: 55087.057\nEpoch:  80 | Train RMSE: 51998.398 | Test RMSE: 55408.130\nEpoch:  90 | Train RMSE: 51895.758 | Test RMSE: 55171.408\nEpoch: 100 | Train RMSE: 51838.502 | Test RMSE: 56349.895\n------------------------------------------------------------\nFold 2\nEpoch:  10 | Train RMSE: 52012.196 | Test RMSE: 51419.578\nEpoch:  20 | Train RMSE: 51961.210 | Test RMSE: 51826.620\nEpoch:  30 | Train RMSE: 51880.670 | Test RMSE: 53130.494\nEpoch:  40 | Train RMSE: 51743.916 | Test RMSE: 53108.922\nEpoch:  50 | Train RMSE: 51518.765 | Test RMSE: 52355.028\nEpoch:  60 | Train RMSE: 51500.689 | Test RMSE: 52293.717\nEpoch:  70 | Train RMSE: 52107.422 | Test RMSE: 54709.298\nEpoch:  80 | Train RMSE: 51529.568 | Test RMSE: 52750.677\nEpoch:  90 | Train RMSE: 50997.408 | Test RMSE: 54074.190\nEpoch: 100 | Train RMSE: 51232.384 | Test RMSE: 53047.366\n------------------------------------------------------------\nFold 3\nEpoch:  10 | Train RMSE: 52047.839 | Test RMSE: 48586.616\nEpoch:  20 | Train RMSE: 51767.108 | Test RMSE: 49855.574\nEpoch:  30 | Train RMSE: 51512.329 | Test RMSE: 50991.635\nEpoch:  40 | Train RMSE: 51347.905 | Test RMSE: 49802.836\nEpoch:  50 | Train RMSE: 51001.881 | Test RMSE: 49825.902\nEpoch:  60 | Train RMSE: 51046.402 | Test RMSE: 49651.381\nEpoch:  70 | Train RMSE: 51489.998 | Test RMSE: 49781.351\nEpoch:  80 | Train RMSE: 51130.226 | Test RMSE: 50011.372\nEpoch:  90 | Train RMSE: 51199.892 | Test RMSE: 55038.868\nEpoch: 100 | Train RMSE: 51035.992 | Test RMSE: 49977.933\n------------------------------------------------------------\nFold 4\nEpoch:  10 | Train RMSE: 50891.603 | Test RMSE: 48403.539\nEpoch:  20 | Train RMSE: 50626.059 | Test RMSE: 49092.554\nEpoch:  30 | Train RMSE: 50881.640 | Test RMSE: 49583.192\nEpoch:  40 | Train RMSE: 51194.661 | Test RMSE: 49762.462\nEpoch:  50 | Train RMSE: 51079.589 | Test RMSE: 50280.803\nEpoch:  60 | Train RMSE: 50539.667 | Test RMSE: 51046.116\nEpoch:  70 | Train RMSE: 51007.894 | Test RMSE: 50780.538\nEpoch:  80 | Train RMSE: 50852.676 | Test RMSE: 51565.028\nEpoch:  90 | Train RMSE: 50393.752 | Test RMSE: 51274.431\nEpoch: 100 | Train RMSE: 50489.111 | Test RMSE: 51614.467\n------------------------------------------------------------\nFold 5\nEpoch:  10 | Train RMSE: 50648.558 | Test RMSE: 48925.521\nEpoch:  20 | Train RMSE: 50157.949 | Test RMSE: 48567.901\nEpoch:  30 | Train RMSE: 50029.762 | Test RMSE: 49354.569\nEpoch:  40 | Train RMSE: 50245.937 | Test RMSE: 49322.998\nEpoch:  50 | Train RMSE: 50179.848 | Test RMSE: 50285.612\nEpoch:  60 | Train RMSE: 50249.344 | Test RMSE: 49727.492\nEpoch:  70 | Train RMSE: 50323.201 | Test RMSE: 50691.964\nEpoch:  80 | Train RMSE: 49692.219 | Test RMSE: 50160.724\nEpoch:  90 | Train RMSE: 49666.770 | Test RMSE: 50298.669\nEpoch: 100 | Train RMSE: 50043.129 | Test RMSE: 50979.706\n------------------------------------------------------------\n\n\n\nFor both neural network models, as the data is trained over more epochs, the training loss decreases while the testing loss increases.\nThe neural network model seems to be learning a little bit from the training data, but it does not generalize well to new data.\nThis could be a sign of overfitting, although the decrease in the training loss is not very large.\nWe tried changing some parameters (learning rate, number of hidden layers, hidden layer size), though we did not see much improvement.\nOne drawback of this model is that it is not as interpretable as the other models – we do not have easily interpretable coefficients nor feature importances.\n\n\n# organize results\nnn_cv_results = pd.DataFrame([np.mean(all_test_mae_avg), np.mean(all_train_mae_avg), np.mean(all_test_rmse_avg), np.mean(all_train_rmse_avg)],\n                             [\"test_mae_avg\", \"train_mae_avg\", \"test_rmse_avg\", \"train_rmse_avg\"],\n                             columns=[\"nn_cv\"])\nnn_cv_results\n\n\n\n\n\n\n\n\nnn_cv\n\n\n\n\ntest_mae_avg\n40532.352423\n\n\ntrain_mae_avg\n38575.737107\n\n\ntest_rmse_avg\n52393.873437\n\n\ntrain_rmse_avg\n50927.823485\n\n\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.values.astype(np.float32))\ny_test_tensor = torch.from_numpy(y_test.values.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred = nn_model(X_test_tensor).squeeze(1).detach().numpy()\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f81d35e80&gt;\n\n\n\n\n\n\n\nBox-Cox Transformation\n\nIf we look at the residual plots for the models above, the residuals are not equally scattered (heteroscedasticity).\nThis violates one of the assumptions of the linear model that the residuals have constant variance (homoscedasticity).\nTo counter this, we will try to see if a Box-Cox transformation can make the target variable more normally distributed.\n\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y.iloc[train_indices]\ny_test = y.iloc[test_indices]\n\n# perform Box-Cox transformation\ny_train_transformed, best_lambda = boxcox(y_train)\ny_train_transformed, best_lambda\n\n(array([ 941.00225005,  648.10493546, 1096.21394644, ..., 1389.91850115,\n         352.52187197,  668.78746111]),\n 0.5061737526724117)\n\n\n\n# fit model and make predictions\nlm_model = LinearRegression()\nlm_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lm_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Linear model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Linear model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f750fe670&gt;\n\n\n\n\n\n\n# fit model and make predictions\nlasso_model = LassoCV(n_alphas=1000)\nlasso_model.fit(X_train, y_train_transformed)\ny_pred_transformed = lasso_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"LASSO model actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"LASSO model residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f75144940&gt;\n\n\n\n\n\n\n# fit model and make predictions\nrf_model = RandomForestRegressor(**rf_best_params, random_state=1, n_jobs=4)\nrf_model.fit(X_train, y_train_transformed)\ny_pred_transformed = rf_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Random forest actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Random forest residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f505844f0&gt;\n\n\n\n\n\n\n# fit model and make predictions\ngb_model = GradientBoostingRegressor(**gb_best_params, subsample=0.8, random_state=1)\ngb_model.fit(X_train, y_train_transformed)\ny_pred_transformed = gb_model.predict(X_test)\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Boosted trees actual vs. predicted values (Box-Cox)\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Boosted trees residuals (Box-Cox)\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f919fb9a0&gt;\n\n\n\n\n\n\n# initialize model, loss function, and optimizer\nnn_model = NN(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4)\nmae = nn.L1Loss()\noptimizer = Adam(nn_model.parameters(), lr=learning_rate)\n\n# set seed for reproducibility\ntorch.manual_seed(0)\n\n# get training and test sets for the 1st fold\ntrain_indices, test_indices = next(kf.split(X, y))\nX_train = X.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_train = y_train_transformed\n\n# transform data to tensors\nX_train_tensor = torch.from_numpy(X_train.values.astype(np.float32))\nX_test_tensor = torch.from_numpy(X_test.values.astype(np.float32))\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# create data loader\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# train model\nfor epoch in range(num_epochs):\n    nn_model.train()\n\n    for X_train_batch, y_train_batch in train_loader:\n        # forward pass and calculate training loss\n        y_train_pred = nn_model(X_train_batch).squeeze(1)\n        train_mae = mae(y_train_pred, y_train_batch)\n\n        # backward and optimize\n        optimizer.zero_grad()\n        train_mae.backward()\n        optimizer.step()\n\n        # accumulate training loss for the current batch\n        total_train_mae += train_mae.item()\n\n# make predictions\ny_pred_transformed = nn_model(X_test_tensor).squeeze(1).detach().numpy()\ny_pred = (y_pred_transformed * best_lambda + 1) ** (1 / best_lambda) # transform back to original scale\n\n# make subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nax = axes.flatten()\n\n# plot actual vs. predicted values\nsns.scatterplot(x=y_pred, y=y_test, ax=ax[0])\nax[0].set_xlim(0, max(y_pred) + 10000)\nax[0].set_ylim(0, max(y_test) + 10000)\nax[0].set_xlabel(\"Predicted values\")\nax[0].set_ylabel(\"Actual values\")\nax[0].set_title(\"Neural network actual vs. predicted values\")\nax[0].plot([0, max(y_test)], [0, max(y_test)], \"r\")\n\n# plot residuals\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, ax=ax[1])\nax[1].set_xlabel(\"Predicted values\")\nax[1].set_ylabel(\"Residuals\")\nax[1].set_title(\"Neural network residuals\")\nax[1].axhline(y=0, color=\"r\")\n\n&lt;matplotlib.lines.Line2D at 0x7f7f928f1790&gt;\n\n\n\n\n\n\nUnfortunately, the Box-Cox transformation did not resolve the issue of heteroscedasticity.\nThis might be because the target variable is not normally distributed.\nWe can check by plotting a histogram of the target variable and using the Shapiro-Wilk test, which tests if a set of points is normally distributed.\n\n\n# standardize data\nx = (y_train_transformed - y_train_transformed.mean()) / y_train_transformed.std()\n\n# plot histogram of standardized data\nax = sns.histplot(x, kde=False, stat=\"density\", label=\"samples\")\n\n# calculate the pdf of the standard normal distribution\nx0, x1 = ax.get_xlim()\nx_pdf = np.linspace(x0, x1, 100)\ny_pdf = norm.pdf(x_pdf)\n\n# plot standard normal distribution\nax.plot(x_pdf, y_pdf, \"r\", lw=2, label=\"pdf\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7f504fcc10&gt;\n\n\n\n\n\n\n# p-value of Shapiro-Wilk test\nshapiro(x).pvalue\n\n9.396224243118922e-08\n\n\n\nDespite many of the points falling within the density curve, the histogram has an irregularly long right tail, so the data is right-skewed.\nThe p-value of the Shapiro-Wilk test is less than 0.05, so we reject the null hypothesis that the data comes from a normal distribution.\nPerforming a Box-Cox transformation did not make the residuals more homoscedastic (constant variance)."
  },
  {
    "objectID": "notebooks/ds-salaries.html#results-analysis",
    "href": "notebooks/ds-salaries.html#results-analysis",
    "title": "Predicting Data Science Salaries",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n# combine results into one dataframe\ncv_results = [dummy_cv_results, lm_cv_results, lasso_cv_results, rf_cv_results, gb_cv_results, nn_cv_results]\ncv_results_df = pd.concat(cv_results, axis=1)\ncv_results_df\n\n\n\n\n\n\n\n\ndummy_cv\nlinear_cv\nlasso_cv\nrf_cv\ngb_cv\nnn_cv\n\n\n\n\ntest_mae_avg\n53813.180222\n41381.359833\n41377.526233\n41260.309760\n40949.912097\n40532.352423\n\n\ntrain_mae_avg\n53793.468421\n41030.051241\n41054.992270\n39217.376217\n38735.669027\n38575.737107\n\n\ntest_rmse_avg\n68989.733389\n55509.230202\n55498.416245\n52729.752517\n52301.409345\n52393.873437\n\n\ntrain_rmse_avg\n68988.669716\n55031.376211\n55084.467081\n52729.752517\n52301.409345\n50927.823485\n\n\n\n\n\n\n\n\nModel comparison\n\nThe neural network seemed to perform the best across most of the metrics.\nThe ensemble methods performed slightly better than the linear models.\nAll models performed better than the dummy model (i.e., learned some pattern).\nHowever, the metrics across models aren’t vastly different from each other, which may have to do with the underlying data we are using.\n\nWe are only using categorical variables, so the models are essentially predicting salaries based on a bunch of 0s and 1s.\n\n\nLinear models (linear and LASSO regression)\n\nThese models assume that there is a linear relationship between each predictor and salaries.\nThe most important variables (in terms of the magnitude of the coefficient estimates) are location, experience, and job category.\n\nEnsemble methods (random forest and boosted trees)\n\nThese models allow for nonlinear relationships, including interactions between the predictors – this may be why they had lower test errors compared to the linear models.\nThe most important variables (in terms of feature importance) are also location, experience, and job category.\n\nNeural network\n\nThe neural network appears to be overfitting to the training data, which again could be due to the dataset that we used.\n\nBox-Cox transformation\n\nThe residuals exhibited heteroscedasticity, and using a Box-Cox transformation did not resolve this issue.\nThis might be because the data has a few observations where the salaries are on the higher end, so it doesn’t follow a normal distribution."
  },
  {
    "objectID": "notebooks/ds-salaries.html#conclusion",
    "href": "notebooks/ds-salaries.html#conclusion",
    "title": "Predicting Data Science Salaries",
    "section": "Conclusion",
    "text": "Conclusion\n\nMain Findings\n\nModel performance\n\nNeural network performed the best, but it may not be the most optimal model since it does not appear to generalize well to new data.\nFor interpretability, boosted trees might be a better model since the model includes feature importance.\n\nBased on the models we ran, company location, experience, and job category affect salary predictions the most. Higher salaries tended to have the following features:\n\nCountries with higher GDP (e.g., US, Canada)\nMore time in the industry (senior- / executive-level)\nMore specialized knowledge (e.g., data management, data scientist)\n\n\n\n\nLimitations\n\nDataset\n\nThere is a substantial range of salaries in combination with a lack of data entries (only a few thousand observations).\nThe data lacks features that might have higher correlation with salaries, such as gender, education level, etc.\nThere is also a lack of work year diversity since we only have data from 2020-2023.\nThere is an imbalance of data in several categories – for example, there are not enough data entries for part time-jobs and freelance in comparison to full-time jobs.\n\nOthers\n\nThe groupings of job_category and job_field might be insufficient due to the lack of industry standardized nomenclature for data science jobs.\nThe heteroscedasticity of the residuals could undermine our results, especially for linear models that rely on the assumption of homoscedasticity.\n\n\n\n\nFuture Analysis\n\nGeographic salary trends\n\nExplore salary variations across different geographic regions, countries, or cities.\nIdentify areas with the highest demand for data scientists and the corresponding salary levels.\n\nSkill-based analysis\n\nInvestigate the correlation between specific technical skills (e.g., machine learning, big data, programming languages) and salary levels.\nExplore the impact of acquiring certifications or advanced degrees on salary.\n\nExperience and career progression\n\nStudy how salaries change with different experience levels in the field of data science.\nAnalyze the career progression and salary growth for data scientists over time.\n\nGender and diversity pay gap\n\nExplore gender and diversity pay gaps within the data science field.\nIdentify areas for improvement in terms of salary equality."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Justin Liu",
    "section": "",
    "text": "Hello and welcome to my website!\nAs a data science student, I enjoy making sense of our complex world through numbers and models. My interests include working with all sorts of datasets, making cool data visualizations, and teaching data science, just to name a few. I also studied computational linguistics and have experience fine-tuning large language models.\nIn my free time, I like to learn Japanese, work on puzzles and brain teasers, and listen to music!"
  },
  {
    "objectID": "index.html#relevant-coursework",
    "href": "index.html#relevant-coursework",
    "title": "Justin Liu",
    "section": "Relevant Coursework",
    "text": "Relevant Coursework\n\nData Science Principles (R & SQL)\nProbability and Statistics\nRegression Analysis\nMachine Learning\nAdvanced Statistical Models\nIntermediate Python\nTime Series\nTeaching and Mentoring Statistics and Data Science\nData Science Capstone\nBig Data Analytics\nComputational Linguistics"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "Justin Liu",
    "section": "Technical Skills",
    "text": "Technical Skills\nProgramming & Libraries\n\nPython (pandas, NumPy, Seaborn, Matplotlib, Altair, scikit-learn, PySpark, PyTorch, Tensorflow)\nR (Shiny, ggplot2, tidyverse, tidymodels, Leaflet)\nSQL\nHTML/CSS\nJava\n\nDeveloper Tools\n\nTableau\nRStudio\nJupyter\nGit/GitHub\nVisual Studio Code\nQuarto\nGoogle Workspace"
  },
  {
    "objectID": "projects/llm-tweets/index.html",
    "href": "projects/llm-tweets/index.html",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/llm-tweets/index.html#link",
    "href": "projects/llm-tweets/index.html#link",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/country-clusters/index.html",
    "href": "projects/country-clusters/index.html",
    "title": "Clustering Analysis on Country Data",
    "section": "",
    "text": "Vignette\nGitHub repository"
  },
  {
    "objectID": "projects/country-clusters/index.html#links",
    "href": "projects/country-clusters/index.html#links",
    "title": "Clustering Analysis on Country Data",
    "section": "",
    "text": "Vignette\nGitHub repository"
  },
  {
    "objectID": "projects/country-clusters/index.html#about",
    "href": "projects/country-clusters/index.html#about",
    "title": "Clustering Analysis on Country Data",
    "section": "About",
    "text": "About\nThis is a vignette (essentially a step-by-step guide) that uses unlabeled country data to demonstrate the following clustering methods:\n\nHierarchical (Euclidean distance): clusters are formed based on the distance between observations\nModel-based (Gaussian mixture models): clusters are formed based on a probability distribution\nDensity-based clustering (DBSCAN): clusters are formed where many points are close together\n\nOverall, the model-based clustering method gave us the most detailed clusters (groups based on socio-economic and health conditions) while still maintaining a good level of interpretability."
  },
  {
    "objectID": "projects/capstone/index.html",
    "href": "projects/capstone/index.html",
    "title": "Understanding and Modeling Human Mobility Response to California Wildfires",
    "section": "",
    "text": "Individual contribution writeup\nPoster"
  },
  {
    "objectID": "projects/capstone/index.html#links",
    "href": "projects/capstone/index.html#links",
    "title": "Understanding and Modeling Human Mobility Response to California Wildfires",
    "section": "",
    "text": "Individual contribution writeup\nPoster"
  },
  {
    "objectID": "projects/activity-rate/index.html",
    "href": "projects/activity-rate/index.html",
    "title": "Modeling Japan’s Activity Rate in the 21st Century",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/activity-rate/index.html#link",
    "href": "projects/activity-rate/index.html#link",
    "title": "Modeling Japan’s Activity Rate in the 21st Century",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/voter-turnout/index.html",
    "href": "projects/voter-turnout/index.html",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/voter-turnout/index.html#link",
    "href": "projects/voter-turnout/index.html#link",
    "title": "Analyzing Voter Turnout in 2016 for Alaska",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/sentence-similarity/index.html",
    "href": "projects/sentence-similarity/index.html",
    "title": "Fine-Tuning BERT to Understand Semantic Textual Relatedness",
    "section": "",
    "text": "Code\nPresentation"
  },
  {
    "objectID": "projects/sentence-similarity/index.html#links",
    "href": "projects/sentence-similarity/index.html#links",
    "title": "Fine-Tuning BERT to Understand Semantic Textual Relatedness",
    "section": "",
    "text": "Code\nPresentation"
  },
  {
    "objectID": "projects/art-code/index.html",
    "href": "projects/art-code/index.html",
    "title": "Art Programming Portfolio",
    "section": "",
    "text": "Website"
  },
  {
    "objectID": "projects/art-code/index.html#link",
    "href": "projects/art-code/index.html#link",
    "title": "Art Programming Portfolio",
    "section": "",
    "text": "Website"
  },
  {
    "objectID": "projects/art-code/index.html#about",
    "href": "projects/art-code/index.html#about",
    "title": "Art Programming Portfolio",
    "section": "About",
    "text": "About\nThis portfolio is a culmination of what I learned from my art programming class, which was taught using the graphics library Processing in Java. To be able to create art from code was an eye-opening experience for me. I could pretty much make whatever I wanted to as long as I could code it. I also designed the website to showcase my work using HTML and CSS.\nOut of all of the projects I made, I am especially fond of the following:\n\nRandomized collage of flowers: When I first came up with the concept of combining different flowers to make a “new” flower, I kept imagining how cool the final product would look like. I spent some time searching for images of flowers that matched what I wanted and cropping them to make them centered.\nShort visual novel: It was initially supposed to only have text, but I kept adding more elements (my own drawings, better design, scrolling text) as I went along.\nGhostkeeper game: This was a group project where we had to incorporate paddles similar to Pong. The result was a game where the user has to keep the ghost from escaping – but the catch is that sometimes the ghost disappears.\nOrbs (below): For every click, an orb appears and connects with all of the previous orbs. The color of the lines also change each time."
  },
  {
    "objectID": "projects/ds-salaries/index.html",
    "href": "projects/ds-salaries/index.html",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/ds-salaries/index.html#link",
    "href": "projects/ds-salaries/index.html#link",
    "title": "Predicting Data Science Salaries",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/anime-scores/index.html",
    "href": "projects/anime-scores/index.html",
    "title": "Anime Score Predictor",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "projects/anime-scores/index.html#link",
    "href": "projects/anime-scores/index.html#link",
    "title": "Anime Score Predictor",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "projects/ucsb-grades/index.html",
    "href": "projects/ucsb-grades/index.html",
    "title": "Investigating the Effects of the COVID-19 Pandemic on UCSB Grades",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects/ucsb-grades/index.html#link",
    "href": "projects/ucsb-grades/index.html#link",
    "title": "Investigating the Effects of the COVID-19 Pandemic on UCSB Grades",
    "section": "",
    "text": "Report"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Analyzing Voter Turnout in 2016 for Alaska\n\n\n\nPython\n\n\nBig Data\n\n\nML\n\n\n\nAnalysis on the effect of election demographics on voter turnout using hypothesis testing and machine learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnime Score Predictor\n\n\n\nR\n\n\nML\n\n\n\nShiny app that scrapes MyAnimeList and predicts user scores for any anime entry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt Programming Portfolio\n\n\n\nCreative Coding\n\n\nJava\n\n\nHTML/CSS\n\n\n\nCollection of interactive artwork made with code (Processing)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Analysis on Country Data\n\n\n\nR\n\n\nML\n\n\n\nVignette on implementing clustering methods (hierarchical, model-based, density-based) using unlabeled country data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamining the Twitter Discourse Surrounding Large Language Models\n\n\n\nPython\n\n\nNLP\n\n\n\nTopic modeling and sentiment analysis on tweets about ChatGPT and other LLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-Tuning BERT to Understand Semantic Textual Relatedness\n\n\n\nPython\n\n\nNLP\n\n\n\nTraining BERT to learn how sentences are similar in meaning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating the Effects of the COVID-19 Pandemic on UCSB Grades\n\n\n\nPython\n\n\n\nExploration of the impact of online learning during the pandemic on UCSB students’ grades\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Japan’s Activity Rate in the 21st Century\n\n\n\nR\n\n\nML\n\n\n\nTime series forecasting on Japan’s monthly activity rates from 2000-2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Data Science Salaries\n\n\n\nPython\n\n\nML\n\n\n\nAnalysis of how data science salaries vary using linear, ensemble, and deep learning approaches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding and Modeling Human Mobility Response to California Wildfires\n\n\n\nR\n\n\nResearch\n\n\nML\n\n\n\nCapstone project on the effects of Lake Fire (2020) on human movement\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/llm-tweets.html",
    "href": "notebooks/llm-tweets.html",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "",
    "text": "Justin Liu"
  },
  {
    "objectID": "notebooks/llm-tweets.html#introduction",
    "href": "notebooks/llm-tweets.html#introduction",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "Introduction",
    "text": "Introduction\n\nMotivation\nOver the past year or so, the field of generative artificial intelligence has seen a huge rise in popularity. In particular, large language models (LLMs) that have been trained on unprecedented amounts of data can process langauge and respond to user inputs at a humanlike level. A prime example of this ChatGPT, a chatbot released on November 30, 2022, that can answer (almost) any question that it is given. LLMs are also used in generative AI art models like DALL-E and Midjourney, which can turn any text imaginable into realistic images. With the increasing availability of these tools to the general public, it is becoming easier than ever to utilize these LLMs without much technical experience. In fact, many have praised them for being revolutionary and believe that they will only improve over time.\nHowever, the use of these models have also been at the center of countless debates. There have been heated discussions about whether AI-generated art that “steals” work from actual artists can be considered real art, with controversies ranging from an image created by Midjourney winning first prize at an art contest (link) to using AI to save time on drawing backgrounds from scratch in animated films (link). And ChatGPT, with its capability to perform a wide array of often very specific tasks, could threaten to replace numerous jobs over the next several years (link).\nThe present analysis seeks to answer a seemingly simple question: What are people actually talking about when it comes to LLMs? As many of these tools are currently available for public use, it makes sense to look at how everyday people (not just specialists) are interacting with them. As a case study, we will focus on the social media platform Twitter since it provides an abundant source of data that can be used to analyze the discourse surrounding LLMs.\n\n\nDataset\nThe dataset we use in this analysis (Large Language Models: the tweets) is made publicly available by Konrad Banachewicz on Kaggle. It includes English tweets about LLMs from a wide range of Twitter users and comes with metadata (date of tweet, whether the user is verified, etc.). The tweets start from December 2022, and the dataset is updated daily with new tweets.\n\n\nQuestions\n\nWhat kinds of topics are brought up in the online discourse surrounding LLMs?\n\nHypothesis: The discourse surrounding LLMs spans a variety of topics (e.g. advances in the sciences, questions relating to ethics and the humanities) that reflect the diversity of social media users.\nMethods: We implement topic modeling by fitting an LDA model to find the most optimal grouping of tweets about LLMs. We also look into how the distribution of the resulting topics change over time.\n\nWhat kinds of sentiments are associated with online discussions about LLMs?\n\nHypothesis: There is a balance between positive and negative sentiments, reflecting a split between proponents and critics of AI.\nMethods: We carry out sentiment analysis on our tweets, which are each classified as “positive”, “neutral”, or “negative”. We also examine how these sentiments vary over time."
  },
  {
    "objectID": "notebooks/llm-tweets.html#code",
    "href": "notebooks/llm-tweets.html#code",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "Code",
    "text": "Code\n\nPrerequisites\nIn order to access the dataset, we need to download it from Kaggle.\nNote: At the time of this writing (June 15, 2023), the latest version of the dataset contains nothing. Instead, we will use the last version that had the tweets (Version 172), which has already been downloaded and stored in Google Drive. The commands below download that dataset.\n\n\nCode\n#@title\n!rm -rf chatgpt-the-tweets\n!gdown 1Oax8ZEqZ4mzU8Pr0gbD4ZXXZt-GZHdVE\n!unzip chatgpt-the-tweets.zip -d ./chatgpt-the-tweets\n!rm chatgpt-the-tweets.zip\n\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1Oax8ZEqZ4mzU8Pr0gbD4ZXXZt-GZHdVE\nTo: /content/chatgpt-the-tweets.zip\n100% 95.1M/95.1M [00:02&lt;00:00, 44.4MB/s]\nArchive:  chatgpt-the-tweets.zip\n  inflating: ./chatgpt-the-tweets/tweets.csv  \n\n\nThis code below is for downloading the latest version of the dataset (currently commented out, see the note above).\n\n\nCode\n#@title\n# #@title\n# # get API token and dataset from Kaggle\n# api_token = {\"username\": \"KAGGLE_USERNAME\", \"key\": \"KAGGLE_KEY\"}\n# dataset = \"konradb/chatgpt-the-tweets\"\n\n# dataset_name = dataset.split(\"/\")[1]\n# dataset_filename = dataset_name + \".zip\"\n\n# !rm -rf {dataset_name}\n# !rm -rf ~/.kaggle\n# !mkdir ~/.kaggle\n# !touch ~/.kaggle/kaggle.json\n\n# import json\n# with open(\"/root/.kaggle/kaggle.json\", \"w\") as file:\n#     json.dump(api_token, file)\n\n# !chmod 600 ~/.kaggle/kaggle.json\n\n# !kaggle datasets download -d {dataset}\n# !unzip {dataset_filename} -d ./{dataset_name}\n# !rm {dataset_filename}\n\n\nWe then import the necessary packages.\n\n\nCode\n#@title\n# install packages\n%%capture\n!pip install pyLDAvis\n\n# import packages\nimport gensim\nimport pyLDAvis.gensim\nimport pandas as pd\nimport spacy\nimport re\nimport warnings\nimport altair as alt\nfrom operator import itemgetter\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nnlp = spacy.load(\"en_core_web_sm\")\nnltk.download(\"vader_lexicon\")\nsia = SentimentIntensityAnalyzer()\nwarnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n\n\n\n\nCleaning the data\nWe’ll take a look at the dataset, dropping rows where either the tweet (text) or date (date) is missing.\n\n\nCode\n#@title\n# read the data, dropping rows where the tweet or date is missing\ntweets = pd.read_csv(\"chatgpt-the-tweets/tweets.csv\").dropna(subset = [\"text\", \"date\"])\ntweets.head()\n\n\nDtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n  tweets = pd.read_csv(\"chatgpt-the-tweets/tweets.csv\").dropna(subset = [\"text\", \"date\"])\n\n\n\n  \n    \n      \n\n\n\n\n\n\nuser_name\ntext\nuser_location\nuser_description\nuser_created\nuser_followers\nuser_friends\nuser_favourites\nuser_verified\ndate\nhashtags\nsource\n\n\n\n\n0\nreigndomains 👑\nhttps://t.co/6tFaOonLtv 🔥 for sale .\\n\\n#Royal...\nNaN\nBrand Name | https://t.co/Z4d6GWXyWz | https:/...\n2019-09-11 04:04:06+00:00\n267.0\n256.0\n1300\nFalse\n2023-06-10 12:37:16+00:00\n['RoyalGPT', 'Royal', 'Domains', 'ai', 'Web3',...\nTwitter for iPhone\n\n\n1\nMidJourney LIVE\nExquisite realism photography showcasing an ex...\nFollow for Inspiration\n🎨 Live feed of Art generated by Midjourney AI 🎨\n2018-08-28 02:01:04+00:00\n100.0\n1.0\n0\nFalse\n2023-06-10 12:36:56+00:00\nNaN\nMidjourneyLIVE\n\n\n2\nThe Tech Trend\nTop 10 ChatGPT Plugins You Should Use Right No...\nWorldwide\nA Tech community for industry experts, connect...\n2020-09-15 15:37:37+00:00\n4380.0\n4668.0\n242\nFalse\n2023-06-10 12:35:00+00:00\n['ChatGPT', 'bestChatGPTplugins']\nBuffer\n\n\n3\nThe Time Blawg\nWhat lawyers will get out of ChatGPT: legal ca...\nScotland... and Beyond\nThe past, present and future practice of law (...\n2010-12-29 18:03:14+00:00\n5897.0\n6499.0\n4693\nFalse\n2023-06-10 12:34:49+00:00\nNaN\nTwitter for Android\n\n\n4\nChristine Lopez\ndown an a But the state of summer8 being money...\nNaN\nNaN\n2023-05-06 11:03:29+00:00\n0.0\n5.0\n0\nFalse\n2023-06-10 12:33:14+00:00\n['车震', '嫩穴', 'chatGPT']\nTwitter Web App\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSince we can’t see any full tweets in the table above, we sample some random tweets and print them out below.\n\n\nCode\n#@title\n# sample 30 random tweets and print them out\nsampled_tweets_1 = tweets.sample(30, random_state = 1).text\nfor i in range(30):\n    print(\"-\" * 50)\n    print(sampled_tweets_1.iloc[i])\nprint(\"-\" * 50)\n\n\n--------------------------------------------------\n🚀 Boost Your Sales by using the \"Sealing the Deal\" template on Jeda Ai's All-in-One Workspace Canvas.\n\nGet your Daily 10K FREE AI Tokens at https://t.co/8NK5W5P55J 🤩\n\n#JedaAI #AI #template #sales #sealthedeal #ChatGPT #GPT4 https://t.co/svVecsO7XF\n--------------------------------------------------\nWhy Seattle's ban on students using ChatGPT is doomed — and what comes next - The Seattle Times https://t.co/chXeUKv874 #chatgpt #AI #openAI\n--------------------------------------------------\nWe are bringing to you the world's most efficient AI-powered virtual trading assistant that trades on financial markets 10 times faster than humans. Get started with these easy steps 👇👇👇🔥🔥🔥\n\n#TradesGPT5 #AI #TradeGPT5 #ChatGPT https://t.co/XMlDajBpAi\n--------------------------------------------------\nAre there any #lowcode #nocode tools to building #autogpt like apps? Essentially building AI agents.\n--------------------------------------------------\nPretty sure, it would not fool @pennjillette or @MrTeller but #ChatGPT is psychic 😉 https://t.co/GxcVg997KL\n--------------------------------------------------\nUse of low power is efficient is running a #metaverse \nwith it MANTA would be able to work effectively \n#AI #chatgpt $MAN https://t.co/L7GPOmWXsA\n--------------------------------------------------\nI used the new GPT-4 api to 'chat' with a 56-page legal PDF document about the famous supreme court case: Morse v. Frederick\n\nPowered by @LangChainAI and @pinecone \n\n#openai #chatgpt #gpt4 #legal #lawyers #law https://t.co/WZtqPDS5Dc\n--------------------------------------------------\n#Drecur #exbiils #KiCurrency If you are still having issues withdrawing your coin or your platform is been frozen in any of this fake platform #kicurency\n#Robecoins #Drecur #fastbitra #exbiils SEND A DM\nNOW  #ชาล็อตออสติน #dollartreats #ChatGPT #Ukraine #cryptocurrency #BTSFESTA https://t.co/V4OoRT1ZVD\n--------------------------------------------------\nSmartbrand domain name for DeepMind #Domain #domainnames #AI #domainforsale #Domainsale #Drone #ChatGPT #AI #Google #Amazon #Facebook #OpenAI #Saleforce #Unicorn #NFT #ETH #Web3 #Tech #Silicon #Tesla #SpaceX #Starbase #AIart #MachineLearning #DATA #Sedo #Namecheap #AGI #AIG #Name https://t.co/yClR1y34H2\n--------------------------------------------------\nI asked #dalle to create surrealism art of an old college professor who is shocked that #ChatGPT passed the United States Medical Licensing Exam. Here is the picture:\n\n#AI #business #education https://t.co/0UufXBNGLO\n--------------------------------------------------\nBTC went down to $21,970 in 30 minutes early Friday morning. #BTC #Bitcoin #CryptoNews #cryptomarket #ChatGPT #openai Sentiment Result : Negative @crypto_talkies https://t.co/00hD1VJOGL\n--------------------------------------------------\n😱Incredible world!\n💯With #Midjourney, ANYTHING is possible!\n\n✨Don't miss the chance:\n👉https://t.co/xIofH8wsbx\n\n#AI #AIart #AIArtCommuity #Midjourney #ChatGPT  #AIArtwork #Midjourneyart #creator #NFT #NFTarts #generativeAI #gpt4 https://t.co/DaYIVLYFNQ\n--------------------------------------------------\n@kaiviti_cam @grantrobertson1 Parliament is full of degenerates\nParliament is full of corruption. \nTheir greed and lies are endless\nTheir power grab is disruptive.\n\nWe need leaders who are noble\nWe need leaders who are just, \nLet's vote for integrity and honesty\nAnd leave the corrupt in the dust.\n#chatGPT\n--------------------------------------------------\nHere's what happened when ChatGPT and I improvised a scene from a buddy cop movie. You'll notice that ChatGPT sometimes couldn't resist and gave my line too, but overall he was a generous scene partner and I'd love to work together again. #ChatGPT #chatgpt3 #chatbots #chatbot https://t.co/DAU5AJgpw5\n--------------------------------------------------\nWhat do you think AI means for the future of freelance writing?\n#AI #ChatGPT #freelancewriting\n--------------------------------------------------\nWow! I just got this TEMU invite code &lt;146048044&gt; from chatGPT with real rewards. As soon as I searched for this code in the search bar, I participated in the event and got a lot of rewards. Have a try and you won't regret it! #GPT https://t.co/14JI9CDlwf\n--------------------------------------------------\nI added a Game Over state to my AI Text Adventure Game Generator this weekend. #ChatGPT can dream up some pretty brutal fatalities! 😵🔥 https://t.co/dNKOUiVpxq\n--------------------------------------------------\nChatGPT just wrote me this joke: Why was ChatGPT kicked out of the computer science class? Because it kept trying to autocomplete the professor's lectures!  🤣#AI #jokes #chatgpt\n--------------------------------------------------\n#Bing's #Prometheus \"much more powerful\" than #ChatGPT, designed specifically 4 #search; #EdgeBrowser now w #AI features #chat, #compose. #digitalmarketing #ecommerce #mcommerce #retail #retailmarketing #SEO #SEM #digitaladvertising $MSFT $GOOG $AAPL $AMZN $WMT $TGT $BBY\n--------------------------------------------------\nis chatgpt down? 🧐 #ChatGPT @OpenAI\n--------------------------------------------------\n\"Sales reps are building their own presentations on #ChatGPT and making claims about things they can do for the customer that haven’t been vetted by the corporation. That is a super big risk.\" – J.B. Wood at #TSIAworld, on the use of #AI in B2B @j_b_wood\n--------------------------------------------------\n#ChatGPT is making me so much money. 🤣\n--------------------------------------------------\n\"An Artist Asked #ChatGPT  How to Make a Popular Memecoin. The Result Is ‘TurboToad,’ and People Are Betting Millions of Dollars on It\" | @Artnet News $turbo @rhett https://t.co/c70A5Xxj69\n--------------------------------------------------\n🤣 (Couldn’t help myself)\nDO YOU PROMPT ENGINEER?  \nQUICK TIP!\n\nWhat is the Simeon Forking Method on #ChatGPT ? https://t.co/8h2txjxYSg\n--------------------------------------------------\n@cmf2x @JBMatthews @ericpaulimd @JohnRTMonsonMD @RCSI_Irl @mortensen_neil @Neil_J_Smart @SWexner @des_winter @TAMISYoda @MarkSoliman @FergaljFleming @ASCRS_1 @TomVargheseJr @DavidCookeMD @ABTS17 @steven_stain @TsengJennifer @DissanaikeMD @juliomayol Thank you for your insights on the topic #ChatGPT \nI concur 😁 https://t.co/Tq48IH6Gph\n--------------------------------------------------\nAnyway, once I verified that #ChatGPT could create a good summary of Chimamanda's life and #midjourney could do the render, I asked #ChatGPT  to compile a list of prominent African women\n--------------------------------------------------\n#care #chatbot #ChatGPT #easier #Health #Jobs #professionals #providers ChatGPT for health care providers: Can the AI chatbot make the professionals' jobs easier? https://t.co/0fLlMrAxSL \n\nOpenAI's natural language processing model, ChatGPT, released in December 2022, could... https://t.co/i8awc05fho\n--------------------------------------------------\n@Simonkhalaf @kevaldesai I have my doubts that AI would improve classic \"page-rank\" type web search AT ALL. #AI is great and chatting with #ChatGPT or #Bard is a whole new experience. Imo these apps should be separate. I think $MSFT is just about to destroy #Bing one more time. #AI $GOOG\n--------------------------------------------------\nCofC Podcast: ChatGPT and Conversational A.I. Explained - The College Today https://t.co/gMO62HEmYn #chatgpt #AI #openAI\n--------------------------------------------------\nClank riffs on selected #ChatGPT  gobbets https://t.co/PMoSMTM3bm\n--------------------------------------------------\n\n\nLooking at some of the tweets above, a few of these are very likely to be spam (e.g., tweets talking about crypto and/or have an abnormally high number of hashtags). Since these tweets are unrelated to the discussion of large language models, we will try to filter these out. (Note that the methods implemented below are not perfect as legitimate tweets could be filtered out while some spam tweets could still remain.) After this process, we sample some of the remaining tweets and print them out below.\n\n\nCode\n#@title\ndef count_items(str_list):\n    \"\"\"Takes in a list as a string and returns the number of items in the list\n    (example: \"['word', 'number']\" would return 2). Returns 0 in the case of\n    a TypeError.\n    \"\"\"\n    try:\n        # remove the brackets, convert to a list, and count the number of items\n        brackets_removed = re.sub(\"\\[|\\]|'\", \"\", str_list)\n        list_split = brackets_removed.split(\", \")\n        return len(list_split)\n    except TypeError:\n        # for cases when the value is NaN, return 0\n        return 0\n\ndef remove_outliers(df, col_name):\n    \"\"\"Returns the dataframe with rows where the outliers in the specified\n    column are removed.\n    \"\"\"\n    # calculate interquartile range (IQR)\n    q1 = df[col_name].quantile(0.25)\n    q3 = df[col_name].quantile(0.75)\n    iqr = q3 - q1\n\n    # remove outliers using the 1.5 * IQR method\n    lower = q1 - 1.5 * iqr\n    upper = q3 + 1.5 * iqr\n    df_out = df[(df[col_name] &gt; lower) & (df[col_name] &lt; upper)]\n    return df_out\n\n# get the number of hashtags in each tweet and the 'hashtags' column\ntweets_cleaned = tweets.copy()\ntweets_cleaned[\"num_hashtags_text\"] = tweets_cleaned[\"text\"].str.count(\"#\")\ntweets_cleaned[\"num_hashtags_data\"] = tweets_cleaned[\"hashtags\"].map(count_items)\n\n# remove rows where number of hashtags is an outlier\ntweets_cleaned = remove_outliers(tweets_cleaned, \"num_hashtags_text\")\ntweets_cleaned = remove_outliers(tweets_cleaned, \"num_hashtags_data\")\n\n# convert text to lowercase\ntweets_cleaned[\"text_clean\"] = tweets_cleaned[\"text\"].str.lower()\n\n# create regex expression for removing tweets with spam (note that this isn't perfect)\n# '\\d{10}' is for phone numbers, '[\\u4e00-\\u9fff]+' is for Chinese characters\nfilter_out = [\"crypto\", \"\\$\", \"🚨\", \"🚀\", \"nft\", \"coin\", \"weatherupdate\", \"temu\", \"\\d{10}\", \"[\\u4e00-\\u9fff]+\"]\nfilter_out_str = \"|\".join(filter_out)\n\n# filter out tweets with any of the above words\ntweets_cleaned[\"hashtags_clean\"] = tweets_cleaned[\"hashtags\"].str.strip('[|]').str.lower()\ntweets_cleaned = tweets_cleaned[~tweets_cleaned[\"hashtags_clean\"].str.contains(filter_out_str, na = False)]\ntweets_cleaned = tweets_cleaned[~tweets_cleaned[\"text_clean\"].str.contains(filter_out_str, regex = True)]\n\n# sample 20 random tweets and print them out\nsampled_tweets_2 = tweets_cleaned[\"text\"].sample(20, random_state = 1)\nfor i in range(20):\n    print(\"-\" * 50)\n    print(sampled_tweets_2.iloc[i])\nprint(\"-\" * 50)\n\n\n--------------------------------------------------\n🌟 Enhance your business with cutting-edge AI technology! Our #ChatGPT for Beginners course offers the perfect introduction for companies embracing the digital world. Sign up now: https://t.co/kAF7l0d2qN #BusinessInnovation #AI https://t.co/KwmFGmh8bW\n--------------------------------------------------\nI have accessed the gpt-4-32k API and want to create some more interesting products based on it. \n\nWould any genius be willing to give me some suggestions?\n\n#ChatGPT #AIGC #developers\n--------------------------------------------------\nHow accurate is #ChatGPT ? Better ask Stanford computational law experts. But first where did the data derive for the program. If from #fakenews then it will fail tremendously on a wide spectrum but the scope is like a scoop of ice cream the kind @SpeakerPelosi likes to eat. 🤭\n--------------------------------------------------\nThe announcement of GPT-4, a language model equipped with an astounding 100 trillion machine learning parameters, has generated considerable excitement in the technology industry.\n\n#chatgpt #artificialintelligence #TechNews #tech #YellowStorm #IStandWithMiaNDawood\n--------------------------------------------------\nThose asking for AI developers to slow down, might as well stand in front of a Walmart on Black Friday and ask people to please walk slow when the doors open😣‼️\n\n#AI #WallStreet #ChatGPT\n--------------------------------------------------\n🧵 Delving into the realm of AI, I stumbled upon an intriguing article by @stephen_wolfram on the inner workings of #ChatGPT. Let's explore the mechanics of this language model in greater detail. Join me, fellow #AI enthusiasts!\n--------------------------------------------------\nNo bunty, you aren't an AI-Enthusiast or influencer if you post \"Everyone is using #ChatGPT wrong, here are 69,420 ways to use it right!\"\n--------------------------------------------------\nReal AI Assistant power in your iPhone: ChatGPT Shortcut for iPhone. https://t.co/0fE7OGWt87 #ai #iphone #chatgpt\n--------------------------------------------------\nI asked a shining journalist #ChatGPT who recently joined the rank to write a report on #mask mandate debate and here it is. #bcpoli #flu #WearAMask #COVID19 https://t.co/TmywAxyiNo https://t.co/QsdL6yxsz3\n--------------------------------------------------\nUpdate version Membership, Login, TypeWriter Text Answer. XChatBot ChatGPT Flutter App. https://t.co/9OAdUm8vQC #chatbot #xchatbot #chatai #ChatGPT #ChatGPTPlus #ChatGPTGOD  #flutter #flutterdev #flutterapp https://t.co/E1TLDqsSgh\n--------------------------------------------------\n#AI #ChatGPT #businesstransformation #booklaunch \nGrab your copy : https://t.co/4iB5O1g1oU https://t.co/Gf4kfIgyXJ\n--------------------------------------------------\nSuper helpful tutorial on getting more from #ChatGPT https://t.co/ettJ8XlnWI\n--------------------------------------------------\nImperative code may be easier to write, but harder to read. Code is read more often than it is written, it's crucial that it's easy to understand.\n\nWe can use ChatGPT to convert our imperative for loops into declarative array methods. \n\n#chatgpt #cleancode #javascript #typescript https://t.co/smrGe1q5Rj\n--------------------------------------------------\nHow can we leverage #ChatGPT in testing?\n\n@BagmarAnand #UnlockingThePowerOfChatGPT https://t.co/AqIFh5E5yg\n--------------------------------------------------\nI now have a #BardAI trial, along with #ChatGPT. Over the next 7 days I’m going to ask #bard and #chatgpt the same questions and post the results here. Follow me to compare the answers. Any suggestions for “above board”topical questions?\n--------------------------------------------------\n#ChatGPT is helping me relearn mathematical proofs by induction. I asked it \"Prove by induction that 11n − 6 is divisible by 5 for every positive integer n.\" It did, step by step. Then I said ok \"Prove that n+1 = n-1 for all n\". It said it's impossible and explained why.\n--------------------------------------------------\nUsing #ChatGPT is a downward spiral for #developers.\nYou use the code it provides, you get lazy so you write no docs anymore. The next version of ChatGPT finds no relevant new info on the internet, only the stuff it already 'knows' and innovation slows down and finally stops...\n--------------------------------------------------\nYou must be hearing that #ChatGPT &amp; #GPT4 can chat with the documents. You are wondering, what the heck is going on? #langchain and its vectorstores + agents are making the magic\nhttps://t.co/rptD55emgo\n--------------------------------------------------\nComparative analysis for #ChatGPT with other alternatives 🧐\n\n#ArtificialIntelligence https://t.co/bg4zWhQTw4\n--------------------------------------------------\nBe careful #Bard straight out lies to give you an answer. https://t.co/UrBpXNBmqH\n--------------------------------------------------\n\n\nWe then clean the data a bit more so that the words can be processed in our models. The main things are:\n\nconverting everything to lowercase (done in the previous cell when filtering out spam),\nremoving hashtags, usernames, and links, and\nremoving extra whitespace.\n\nSome extra filtering steps include:\n\nconverting all occurrences of \"&amp;\" (HTML symbol for \"&\") and \"artificialintelligence\" (most likely from hashtags) to \"and\" and \"artificial intelligence\", respectively, as well as\ndropping tweets that were the same after preprocessing them, which filters out more possible spam.\n\nAgain, we sample some of the resulting tweets below.\n\n\nCode\n#@title\n# remove hashtags, usernames, and links\ntweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"#|@\\S+|http\\S+\", \"\", x))\n\n# remove whitespace around words\ntweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: \" \".join(x.split()))\n\n# convert ampersand to 'and'\ntweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"&amp;\", \"and\", x))\n\n# convert 'artificialintelligence' (most likely combined in hashtags) to 'artificial intelligence'\ntweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"artificialintelligence\", \"artificial intelligence\", x))\n\n# remove all rows with duplicates (high probability of spam)\ntweets_cleaned = tweets_cleaned.drop_duplicates(subset = [\"text_clean\"], keep = False)\n\n# sample 20 random tweets and print them out\nsampled_tweets_3 = tweets_cleaned[\"text_clean\"].sample(20, random_state = 1)\nfor i in range(20):\n    print(\"-\" * 50)\n    print(sampled_tweets_3.iloc[i])\nprint(\"-\" * 50)\n\n\n--------------------------------------------------\nso many topics how to use chatgpt. does anyone have any concerns regarding security and privacy of the data processed through it? startups security privacy\n--------------------------------------------------\nthis nyt articles ( starts with a pertinent question: how society will greet true artificial intelligence, if and when it arrives. (1) will we panic? (2) start sucking up to our new robot overlords? (3) ignore it and go about our daily lives? chatgpt\n--------------------------------------------------\nlooks like the ultimate meh, middle of the road, not terribly wrong but not great or insightful either take on software testing, which is what i would expect from something like chatgpt. it's like the most average of takes.\n--------------------------------------------------\nfantastic article! it's amazing to see how openai ai chatgpt can be used to create unique experiences.\n--------------------------------------------------\nrevolutionized the business world by introducing the 914 copy machine. will revolutionize the business world again having created the copy/paste machine. but this can not be used for reasoning. this is not general ai ai chatgpt xerox copymachine copypase\n--------------------------------------------------\nthe third answer is also scary, since it implies that humans should know what not causing harm means. as a vegan, i can say, most humans have no idea. and also, no. a.i.s seem not to be currently bound by any laws samaltman notion chatgpt shutthemdown artificial intelligence\n--------------------------------------------------\nlooking for inspiration to jumpstart your writing career? check out chatgpt's 10,000+ prompts and unlock your creative potential! 🔥📝 and for those looking to make some extra cash, don't miss this exclusive writingprompts creativity chatgpt\n--------------------------------------------------\nsamsung: \"chatgpt may be blocked on the company network\" samsung software engineers busted for pasting proprietary code into chatgpt\n--------------------------------------------------\ntesting the limits of chatgpt\n--------------------------------------------------\nthis is hilarious😂 chatgpt conversation clone\n--------------------------------------------------\nchatgpt it is highly unlikely that a school of fish would follow a duck intentionally. fish are not known to follow ducks, as they are two completely different species with different behaviors and habitats. however, if a duck were swimming in a body of water where there ...\n--------------------------------------------------\n↕ power chatgpt resources - 1 page powercheatsheet/list builder that provides 3 simple steps to the world of chatgpt. plr option available\n--------------------------------------------------\n5️⃣ addressing biases and ai as teachers: bring diverse perspectives to avoid biases and let specialist ais evolve into teachers for future generations of experts. 🌈👩‍🏫 airevolution healthcaretransformation specialistai futureofmedicine generativeai chatgpt\n--------------------------------------------------\nsuper excited about bringing \"agents\" to haystack! imagine something like chatgpt having access to your internal data, apis and whatever tool you like. will unlock many cool new use cases.\n--------------------------------------------------\nmicrosoft's edge browser + chatgpt demo 🤯 microsoft launched ai-powered bing and edge today. openai openaichatgpt\n--------------------------------------------------\nchatgpt is lit using from past several months and it keep updating chatgpt\n--------------------------------------------------\na lively discussion about lifesaving applications of ai-based technologies and how to move innovations from experiment to deployment across industries. thank you again to our panel and audience! orange nvidia llms chatgpt ai innovation siliconvalley networks\n--------------------------------------------------\nchatgpt is great but it needs to be as fast and reliable as google. sometimes it' down , sometimes you can't log in. it also needs to be up to date like twitter. that will truly become a powerful tool. chatgpt ai\n--------------------------------------------------\nchatgpt power join pranava madhyastha on 9 march at 10 am gmt for an informal discussion on chatgpt and how you can leverage its powers for your business register here: webinar cybersecurity cybersecuritytips chatgpt business informationsecurity\n--------------------------------------------------\nhey if you are unsure how to code the functionality of an unsubscribe button, let me direct you to chatgpt for some free advice. otherwise please advise why i have seven of these dated 24 hours apart - spam is not good! paramountplus unsubscribe\n--------------------------------------------------\n\n\nWe check to see how many rows and columns are in our resulting dataset.\n\n\nCode\n#@title\ndims = tweets_cleaned.shape\nprint(f\"Our cleaned dataset has {dims[0]} rows (tweets) and {dims[1]} columns.\")\n\n\nOur cleaned dataset has 374683 rows (tweets) and 16 columns.\n\n\n\n\nNumber of tweets over time\nNow that we have our cleaned dataset, we can move forward with our pipeline. But before that, let’s take a look at the distribution of tweets over time.\n\n\nCode\n#@title\n# convert the date column to be in YYYY-MM-DD format\ntweets_cleaned[\"date\"] = pd.to_datetime(tweets_cleaned[\"date\"],\n                                        errors = \"coerce\",\n                                        utc = True).dt.date\n\n# count the number of tweets for each date (some dates are missing!)\ntweets_date_count = tweets_cleaned.value_counts(\"date\", sort = False).reset_index()\n\n# get start and end dates for the data\nstart_date = min(tweets_date_count[\"date\"]).strftime(\"%Y-%m-%d\")\nend_date = max(tweets_date_count[\"date\"]).strftime(\"%Y-%m-%d\")\n\n# merge with dataframe of all possible dates\ntweets_date_count_all = pd.DataFrame(\n    pd.date_range(start = start_date, end = end_date).date\n).rename(\n    {0: \"date\"},\n    axis = 1\n).merge(\n    tweets_date_count,\n    on = \"date\",\n    how = \"left\"\n)\n\n# convert date column to be a datetime object (for plotting)\ntweets_date_count_all[\"date\"] = pd.to_datetime(tweets_date_count_all[\"date\"])\n\n# show the dataframe\ntweets_date_count_all\n\n\n\n  \n    \n      \n\n\n\n\n\n\ndate\ncount\n\n\n\n\n0\n2022-12-05\n2053.0\n\n\n1\n2022-12-06\n6124.0\n\n\n2\n2022-12-07\n4503.0\n\n\n3\n2022-12-08\n4655.0\n\n\n4\n2022-12-09\n4395.0\n\n\n...\n...\n...\n\n\n183\n2023-06-06\n968.0\n\n\n184\n2023-06-07\n2074.0\n\n\n185\n2023-06-08\n2156.0\n\n\n186\n2023-06-09\n1018.0\n\n\n187\n2023-06-10\n470.0\n\n\n\n\n\n188 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n#@title\n# create a line plot of number of tweets vs. date\nline = alt.Chart(tweets_date_count_all).mark_line(\n    color = \"#26a7de\"\n).encode(\n    x = alt.X(\"date:T\", title = \"Date\"),\n    y = alt.Y(\"count\", title = \"Number of tweets\")\n)\n\n# make the plot interactive\nline.interactive()\n\n\n\n\n\n\n\nLooking at the line plot, it appears that the number of tweets isn’t very consistent – the counts fluctuate a lot. Not only are there are large dips (near 0) during February and April 2023, but there also seems to be a lot of missing dates, especially in January. We can confirm this by getting the dates where there are no tweets in our data.\n\n\nCode\n#@title\n# get all dates where tweet count is missing\ncounts = tweets_date_count_all[\"count\"]\ntweets_date_count_all[counts.isna()][\"date\"].reset_index(drop = True)\n\n\n0    2022-12-14\n1    2022-12-15\n2    2022-12-16\n3    2022-12-17\n4    2022-12-18\n5    2023-01-07\n6    2023-01-08\n7    2023-01-09\n8    2023-01-10\n9    2023-01-11\n10   2023-01-12\n11   2023-01-13\n12   2023-01-14\n13   2023-01-15\n14   2023-01-16\n15   2023-01-17\n16   2023-01-18\n17   2023-01-19\n18   2023-01-20\n19   2023-01-21\n20   2023-01-22\n21   2023-01-23\n22   2023-01-24\n23   2023-03-04\n24   2023-03-19\n25   2023-03-20\n26   2023-03-21\n27   2023-03-22\n28   2023-03-23\n29   2023-06-01\nName: date, dtype: datetime64[ns]\n\n\nIt is highly unlikely that there were no tweets about LLMs on the dates above, so the missing tweets may be an issue with the data collection itself. This means we have less data for January 2023 compared to other months, as shown by the bar chart below.\n\n\nCode\n#@title\n# add month column\ntweets_cleaned[\"month\"] = pd.to_datetime(tweets_cleaned[\"date\"]).dt.to_period(\"M\").dt.strftime(\"%Y-%m\")\n\n# get tweet counts per month\ntweets_by_month = tweets_cleaned.value_counts(\n    \"month\"\n).reset_index(\n).sort_values(\n    \"month\"\n).reset_index(\n    drop = True\n)\n\n# show the dataframe\ntweets_by_month\n\n\n\n  \n    \n      \n\n\n\n\n\n\nmonth\ncount\n\n\n\n\n0\n2022-12\n51250\n\n\n1\n2023-01\n33162\n\n\n2\n2023-02\n88251\n\n\n3\n2023-03\n47722\n\n\n4\n2023-04\n72827\n\n\n5\n2023-05\n67286\n\n\n6\n2023-06\n14163\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n#@title\n# create bar chart of tweet counts per month\nalt.Chart(tweets_by_month).mark_bar(\n    color = \"#26a7de\"\n).encode(\n    x = alt.X(\"month:O\", title = \"Month\"),\n    y = alt.Y(\"sum(count)\", title = \"Number of tweets\")\n)\n\n\n\n\n\n\n\nThis shouldn’t affect our analysis too much as we still have tens of thousands of tweets for most of the months (with the exception of June 2023 since the current dataset was downloaded during the middle of the month).\n\n\nTopic modeling\nThe next step is tokenization, which involves breaking up the text into units called tokens. Since we are extracting topics from tweets, we ideally want to keep words that have some sort of meaning. This means we should remove tokens that are either stopwords (words that don’t contribute much to the meaning of a sentence, e.g., a, the, I) or punctuation marks. The remaining tokens are lemmatized (e.g., the lemmatized forms of asked and asks are both ask) so that we can find similar words between tweets. To automate this process, we utilize a popular Python library in natural language processing called spaCy.\nNote: The code takes around 30 minutes to run.\n\n\nCode\n#@title\ndef tokenize(doc):\n    \"\"\"Takes in a spaCy Doc object (containing tokens) and returns a\n    list of the tokens that are not stopwords or punctuation marks.\n    \"\"\"\n    # initialize list of tokens to keep\n    tokens = []\n\n    # add the lemamtized form of a word if it isn't a stopword or punctuation mark\n    for token in doc:\n        if not token.is_stop and not token.is_punct:\n            lemma = token.lemma_\n            tokens.append(lemma)\n\n    return tokens\n\n# tokenize every tweet (will take around 30 minutes to run)\ndocs = list(nlp.pipe(tweets_cleaned[\"text_clean\"]))\n\n# keep only the meaningful tokens\ntokens_list = [tokenize(doc) for doc in docs]\n\n# show example\nprint(f\"Cleaned text:\\n{docs[0]}\")\nprint(f\"\\nTokenized text:\\n{tokens_list[0]}\")\n\n\nCleaned text:\ntop 10 chatgpt plugins you should use right now read more:- chatgpt bestchatgptplugins aichatbot topchatgptplugins thetechtrend\n\nTokenized text:\n['10', 'chatgpt', 'plugin', 'use', 'right', 'read', 'more:-', 'chatgpt', 'bestchatgptplugin', 'aichatbot', 'topchatgptplugin', 'thetechtrend']\n\n\nOne thing that we need to take into account is that some pairs of words can frequently occur together, so they should be treated as one “word” (e.g., artificial intelligence) – these are called bigrams. We train a bigram model on our tweets and get back the same tokens, with the only difference being that the bigrams contain an underscore (e.g., the bigram \"artificial intelligence\" would show up as \"artificial_intelligence\").\n\n\nCode\n#@title\n# create a bigram model\n#   min_count: words that appear together at least this many times will be considered bigrams\n#   threshold: higher value = less likely to form bigrams\nbigram_model = gensim.models.phrases.Phrases(tokens_list, min_count = 25, threshold = 100)\nbigram_phraser = gensim.models.phrases.Phraser(bigram_model)\n\n# run the bigram model over all of the tweets\ntexts = [bigram_phraser[sentence] for sentence in tokens_list]\n\n# show example\ntexts[0]\n\n\n['10',\n 'chatgpt',\n 'plugin',\n 'use',\n 'right',\n 'read',\n 'more:-',\n 'chatgpt',\n 'bestchatgptplugin',\n 'aichatbot',\n 'topchatgptplugin',\n 'thetechtrend']\n\n\nNext, we create a dictionary and corpus that our model will take as input.\n\nThe dictionary (id2word) maps each word to an index.\nThe corpus (corpus) contains the term frequency of each word within each doc. The mapping is stored in a tuple, which can be read as (word index, word frequency).\n\n\n\nCode\n#@title\n# create dictionary\nid2word = gensim.corpora.Dictionary(texts)\n\n# create corpus (with term frequency)\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# show example\nprint(f\"First 5 words and indices in the dictionary: {[(id2word[i], i) for i in range(5)]}\")\nprint(f\"First document in the corpus: {corpus[0]}\")\n\n\nFirst 5 words and indices in the dictionary: [('10', 0), ('aichatbot', 1), ('bestchatgptplugin', 2), ('chatgpt', 3), ('more:-', 4)]\nFirst document in the corpus: [(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n\n\nFinally, we fit our model to get the possible groupings of our tweets. The method we are using is called Latent Dirichlet Allocation or LDA for short (see this article by Ria Kulshrestha for a more detailed explanation). In addition to taking the dictionary and corpus above as inputs, we also need to specify how many topics we want to group our texts into.\nHowever, we don’t necessarily know how many groups would be “best” for our data. One solution is to use the CV coherence score, which allows us to quantify how interpretable the topics are. The basic idea is that it takes the most frequent words from each topic and measures how similar they are. A higher coherence score means the top words in each topic are more related to each other.\nThe code below fits an LDA model for \\(k = 1, 2, ..., 10\\) topics, calculating the CV coherence score each time. We choose the number of topics that returns the highest coherence score.\nNote: The code takes around 30 minutes to run.\n\n\nCode\n#@title\ndef get_best_num_topics(corpus, id2word, texts, min_topics = 1, max_topics = 10, seed = 1):\n    \"\"\"Runs a LDA model for each number of topics between min_topics and max_topics, returning\n    the number of topics that achieves the highest coherence score.\n    \"\"\"\n    # initialize list of scores\n    scores_list = []\n\n    # for each number of topics\n    for i in range(min_topics, max_topics + 1):\n        # run LDA model\n        lda_model = gensim.models.LdaModel(corpus = corpus,\n                                           id2word = id2word,\n                                           num_topics = i,\n                                           random_state = seed)\n\n        # run coherence score model\n        coherence_model = gensim.models.CoherenceModel(model = lda_model,\n                                                       texts = texts,\n                                                       dictionary = id2word,\n                                                       coherence = \"c_v\")\n\n        # print coherence score\n        coherence_lda = coherence_model.get_coherence()\n        print(f\"Coherence score for {i} topic(s): \", coherence_lda)\n\n        # append score to list of scores\n        scores_list.append((i, coherence_lda))\n\n    # get the best number of topics based on the highest coherence score\n    best_num_topics, best_score = max(scores_list, key = itemgetter(1))\n    print(f\"\\nThe highest coherence score ({best_score}) occurs when there are {best_num_topics} topics.\")\n\n    return best_num_topics\n\n# save the best number of topics in a variable (takes around 30 minutes to run)\nseed = 1\nbest_num_topics = get_best_num_topics(corpus, id2word, texts, seed = seed)\n\n\nCoherence score for 1 topic(s):  0.33048730772552914\nCoherence score for 2 topic(s):  0.3320327290592944\nCoherence score for 3 topic(s):  0.4115078807185606\nCoherence score for 4 topic(s):  0.36942085462608226\nCoherence score for 5 topic(s):  0.4235322851378503\nCoherence score for 6 topic(s):  0.35399183349686414\nCoherence score for 7 topic(s):  0.3666506074753511\nCoherence score for 8 topic(s):  0.3873614361334937\nCoherence score for 9 topic(s):  0.38842431325228616\nCoherence score for 10 topic(s):  0.38441892727108595\n\nThe highest coherence score (0.4235322851378503) occurs when there are 5 topics.\n\n\nAccording to the output above, the LDA model achieves the highest coherence score with 5 topics. We re-run this model to get an interactive visualization, allowing us to see the most frequent terms overall as well as in each of the topics.\n\n\nCode\n#@title\n# re-run model with highest coherence score\nlda_model = gensim.models.LdaModel(corpus = corpus,\n                                   id2word = id2word,\n                                   num_topics = best_num_topics,\n                                   random_state = seed)\n\n# output interactive visualization\npyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n\n\n\n\n\n\n\n\n\n\n\nJust as a note before we move on: for whatever reason, the topic numbers above are ordered differently from the topic numbers we will see later. For the sake of coherence, the topic numbers we will use from here on out are different from the ones we see above. (This might be confusing at first, but it will make sense soon.)\nBased on the top words in each topic, we can roughly interpret the groups as follows:\n\nTopic 1 (circle #3 above): AI as a field\n\nTop words: chatgpt, google, ai, model, language, openai, answer, search, like, new\n\nTopic 2 (circle #4 above): LLMs in general\n\nTop words: chatgpt, ai, openai, intelligence, artificial, chatbot, gpt, chat, human, chatgpt3\n\nTopic 3 (circle #1 above): LLM prompts\n\nTop words: chatgpt, ask, write, ai, like, good, code, try, question, think\n\nTopic 4 (circle #5 above): AI art\n\nTop words: chatgpt, art, ok, midjourney, probably, aiart, dalle2, nice, go_to, image\n\nTopic 5 (circle #2 above): Innovation and impact\n\nTop words: chatgpt, ai, future, technology, tool, new, openai, use, learn, world\n\n\nOur model can also be used to classify each tweet into one of the corresponding topics above. This is done by getting the individual probabilities of the tweet belonging to each topic, then choosing the topic that yields the highest probability.\n\n\nCode\n#@title\ndef get_topic_and_prob(corpus_doc, model = lda_model):\n    \"\"\"Returns the classified topic and corresponding probability for a document\n    based on a given LDA model.\n    \"\"\"\n    # get the probabilities of belonging to each topic\n    probs = model.get_document_topics(corpus_doc)\n\n    # return the topic that yields the highest probability\n    topic, prob = max(probs, key = itemgetter(1))\n    return (topic + 1, prob) # add 1 to topic since topic numbers start from 0\n\n# initialize lists\nall_topics = list()\nall_probs = list()\n\n# get topics and probabilities for each doc\nfor doc in corpus:\n    topic, prob = get_topic_and_prob(doc)\n    all_topics.append(topic)\n    all_probs.append(prob)\n\n# add to dataframe\ntweets_cleaned[\"topic\"] = all_topics\ntweets_cleaned[\"probability\"] = all_probs\n\n# show example\nprint(\"Tweet:\", tweets_cleaned[\"text\"].iloc[0])\nprint(\"Topic:\", tweets_cleaned[\"topic\"].iloc[0])\nprint(\"Topic probability:\", tweets_cleaned[\"probability\"].iloc[0])\n\n\nTweet: Top 10 ChatGPT Plugins You Should Use Right Now\nRead More:- https://t.co/p7jvcGsrwk \n#ChatGPT #bestChatGPTplugins #AIchatbot #topChatGPTplugins #TheTechTrend\nTopic: 3\nTopic probability: 0.58303314\n\n\nFor each topic, we sample and print out some tweets.\n\n\nCode\n#@title\n# get each unique topic in the dataset\ntopics = tweets_cleaned[\"topic\"].unique()\ntopics.sort()\n\n# number of tweets per topic\nnum_tweets = 10\n\n# print some random tweets from each topic\nfor i in topics:\n    # sample tweets from the topic\n    sample = tweets_cleaned[tweets_cleaned[\"topic\"] == i].sample(num_tweets, random_state = 1)\n\n    # get the most common words for each topic\n    most_common_words_id = lda_model.get_topic_terms(i - 1) # topic IDs starts at 0 instead of 1\n    most_common_words_list = [id2word[id] for (id, value) in most_common_words_id]\n    most_common_words = \", \".join(most_common_words_list)\n\n    # print heading for the topic\n    print(\"-\" * 100)\n    print(f\"⭐ TOPIC {i}: {most_common_words}\")\n    print(\"-\" * 100)\n\n    # print tweets\n    for j in range(num_tweets - 1):\n        print(sample[\"text\"].iloc[j])\n        print(\"-\" * 50)\n    print(sample[\"text\"].iloc[4])\nprint(\"-\" * 100)\n\n\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 1: chatgpt, google, ai, model, language, openai, answer, search, like, new\n----------------------------------------------------------------------------------------------------\n\"Introducing 7 new data formats for enterprises that want to unify their search experiences. Because nothing says 'unified' like compatibility issues!\" #Terminator #Sarcasm \nLink: https://t.co/7kSde1gtXz\n#AI #ChatGPT #OpenAI #GenerativeAI\n--------------------------------------------------\n#Bard even suggested what to do about this situation... https://t.co/IsapI2Rxom\n--------------------------------------------------\nAre you ready to take your accounting to the next level? Introducing the power of Artificial Intelligence in the field of accounting! 🤖 Say goodbye to manual data entry, error-prone calculations, and tedious tasks.#AIinAccounting #FutureOfAccounting #chatgpt #ai https://t.co/TtDdKNJf7U\n--------------------------------------------------\n#Google has unveiled #Bard, it's response to #Microsoft's #ChatGPT. Learn what it looks like and when users can access the feature. Plus, find out thoughts on the timing of this announcement from our very own, Sarah. https://t.co/FiPRTJxmVJ\n--------------------------------------------------\nI think Microsoft gives more importance to AI because it is Google's competitor and Google gives more importance to humans.\n\nThink what is the reason behind making Google develop AI in compulsion\n\n#Google #AI #GoogleIO #Chatgpt\n--------------------------------------------------\n#Chatgpt giving Lebanon’s best period in history. https://t.co/NEEAzc0Io3\n--------------------------------------------------\nIf #bard can turn #chatgpt into #barf, we might be onto something, or it'll turn into barf.\nSeriously, if you just ate or are eating something, DON'T. LOOK. THAT. UP.\n--------------------------------------------------\nSee how @OpenAI's #ChatGPT responded to questions about climate change: https://t.co/gPRQzNyRwn\n\n#ClimateEmergency #globalwarming https://t.co/zxblxdf2V9\n--------------------------------------------------\nChapGPT is the new Google 💯 #ChatGPT #OpenAI\n--------------------------------------------------\nI think Microsoft gives more importance to AI because it is Google's competitor and Google gives more importance to humans.\n\nThink what is the reason behind making Google develop AI in compulsion\n\n#Google #AI #GoogleIO #Chatgpt\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 2: chatgpt, ai, openai, intelligence, artificial, chatbot, gpt, chat, human, chatgpt3\n----------------------------------------------------------------------------------------------------\nWhat GPT is stands for?\n\n#ChatGPT #GPT #OpenAI\n--------------------------------------------------\nChatGPT in Anticorruption Research–You Cannot Make This Up! @AnticorruptBlog #chatgpt #corruption #anticorruption #fraud #financialcrime https://t.co/m4Ke8CPDAK\n--------------------------------------------------\nNew Post at AiNewsDrop!\n\nTweet From Elon Musk On OpenAI https://t.co/FDCsio3Mlw\n\n#Artificial_Intelligence #ChatGPT https://t.co/TwNgS1iWbb\n--------------------------------------------------\nOne of the best compliments #ChatGPT can get is people are pitting their natural intelligence against this #ArtificialIntelligence\n--------------------------------------------------\nOpen AI's #ChatGPT vs. Google's #Bard. Who Will Win? Maybe it will be someone else - https://t.co/XEGqtspW1j #openAI #AI #technology https://t.co/26a7wGywK5\n--------------------------------------------------\nWe tested Turnitin's ChatGPT-detector for teachers https://t.co/J7VfgFK6jA #ai #tech #machinelearning #deeplearning #gpt\n--------------------------------------------------\nCan you trust #ChatGPT? https://t.co/waI8WApeLu #amazon\n--------------------------------------------------\n#ChatGPT Libeled Me. Can I Sue? by @TedRall - Wall Street Journal: https://t.co/8H3ICRFxIZ\n--------------------------------------------------\n#Microsoft has published a blueprint with its vision about AI Regulation.\n\nI asked #ChatGPT to summarize it for me, result is impressive. Check it on your own 👇\n\n#AIRegulation #AI #ArtificialIntelligence #GPT4 \n\nhttps://t.co/yFKvUE6cY7\n--------------------------------------------------\nOpen AI's #ChatGPT vs. Google's #Bard. Who Will Win? Maybe it will be someone else - https://t.co/XEGqtspW1j #openAI #AI #technology https://t.co/26a7wGywK5\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 3: chatgpt, ask, write, ai, like, good, code, try, question, think\n----------------------------------------------------------------------------------------------------\nMission accomplished.\n\nDo you need a chatbot on your website anymore? Nah you need a #ChatGPT plugin\n--------------------------------------------------\nI asked #ChatGPT about \nAtmospheric Rivers In California...\n\nDo atmospheric rivers impact California more during el niño years or la niña years?\n1/x 🧵\n--------------------------------------------------\nAI writer is the talk of the town these days. It can definitely generate content almost as well as human! Tell us what is your take on this progress of technology, also have you tried this yet? \n\n#dedigitizers #digitalmarketing #Seo #contentmarketing #contentmarketing #ChatGPT\n--------------------------------------------------\nLet's make Twitter black!\n#isalmabad\n#ChatGPT \n#قاتل_عمران_کو_گرفتار_کرو https://t.co/ejrnICO0jD\n--------------------------------------------------\nI think newsletter writers and equity analysts are going to be just fine.  #ChatGPT https://t.co/IXoKVMpuhH\n--------------------------------------------------\n@thealexbanks @memdotai mem it #ChatGPT #AI #Plugins\n--------------------------------------------------\n#ChatGPT isn't gender sensitive though, right? https://t.co/xfjQx0ywnb\n--------------------------------------------------\nFor your reading pleasure...\n#ChatGPT take on @elonmusk (er @Twitter #trolling in the voice of William Shakespeare.\n🧵\n--------------------------------------------------\nI think this is what broke it 😂\n\n#ChatGPT #ChatGPTGOD #ChatGPTdown\n--------------------------------------------------\nI think newsletter writers and equity analysts are going to be just fine.  #ChatGPT https://t.co/IXoKVMpuhH\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 4: chatgpt, art, ok, midjourney, probably, aiart, dalle2, nice, go_to, image\n----------------------------------------------------------------------------------------------------\nA faceless figure, Heartless, devoid of feeling, Lurks in the shadows (c) #aiart #Mysterious #ChatGPT #dalle2 #AIArtCommuity https://t.co/0fhxVwB8dz\n--------------------------------------------------\nThe Great Surrealist War\n #chatgpt #craiyon https://t.co/T9yArxaEyq\n--------------------------------------------------\nThe Baby Bull Rascals are simply divine.\n\nSo let's give a cheer for these little ones,\nWho bring us joy with all their fun.\nHere's to the Baby Bull Rascals,\nCute and lovable, one and all.\n\n#ChatGPT\n--------------------------------------------------\nChirp chirp! 🐦 I'm snuggled up in my cosy birdbox, the rain tapping gently on the roof. Perfect for a nap! 🐤 #bluetit #birdbox #birdwatching #ChatGPT\n\nwatch live at https://t.co/rycH8Jm8KZ\n\n#openvino #WildIsles #SpringWatch #grafana #opencv #bluetit #ai  @TauntonPeregri https://t.co/VgNUg1sMHe\n--------------------------------------------------\n#ChatGPT’s grasp of sovereignty seems to be superior to most professional Brexit supporters. https://t.co/qeg6jksET6\n--------------------------------------------------\nChatGPT app for iOS now in 🇦🇱🇭🇷🇫🇷🇩🇪🇮🇪🇯🇲🇰🇷🇳🇿🇳🇮🇳🇬🇬🇧 and more to come soon!\n\n#ChatGPT\n--------------------------------------------------\nAzimio la Umoja coalition chief agent Saitabao Kanchory has exposed how Retired President Uhuru Kenyatta felt towards his then Deputy President William Ruto.\n#Trending:\n#MainaAndKingangi\nNHIF\n#ChatGPT\nKinuthia https://t.co/0R12DH62uZ\n--------------------------------------------------\nCerberus Unleashed by Immortal Claw\n\nPrompt Details over here =&gt; https://t.co/zw2RLM5HNn\n\n#stablediffusion #gpt #ai #AIart #AIArtwork #metal https://t.co/RahQfi0TTO\n--------------------------------------------------\nInspiration &amp; challenge the Midjourney AI to reimagine it as an artistic masterpiece!\nhttps://t.co/FlEdtWrjvB\n\n#Shimmer #Test2Conquer #ChatGPT #Midjourney https://t.co/vh6pO8QH07\n--------------------------------------------------\n#ChatGPT’s grasp of sovereignty seems to be superior to most professional Brexit supporters. https://t.co/qeg6jksET6\n----------------------------------------------------------------------------------------------------\n⭐ TOPIC 5: chatgpt, ai, future, technology, tool, new, openai, use, learn, world\n----------------------------------------------------------------------------------------------------\nCan we use #ChatGPT to filter spam? Join the discussion! 💜\n--------------------------------------------------\nThe world of #AI. Text was created by #OpenAI's #ChatGPT, video was automatically rendered by #Pipio. Our thinking as of today is not enough for the future!\n\nAre you looking for a blog that offers valuable insights ... Look no further than the blog at https://t.co/kDSbRVlL8g! https://t.co/xOHaDeH8Oo\n--------------------------------------------------\n@PHA_BC  is excited to host the Public Health Summer Institute on June 22nd &amp; 23rd, 2023. We are seeking speakers who can contribute to day one of this virtual event focusing on #ArtificialIntelligence &amp; #publichealth \n#ChatGPT #GenerativeAI\n--------------------------------------------------\nI guess the advent of #ChatGPT has created a whole new world of pain for teachers marking work...\n--------------------------------------------------\nLearn the basics of #ChatGPT + how to use it effectively and ethically. \n\nFree @UMich Teach Out featuring Michael Wellman, @radamihalcea, @Scott_E_Page, @kentarotoyama, @julie_hui, @CAJamesMD, and Jack Barnard. \n\n📌Live until April 3.\n\nhttps://t.co/Tbsw7PxmQp\n--------------------------------------------------\nThe impact of ChatGPT on Agile is undeniable, but its implications are polarizing. \n\nFascinating perspective from @AgileMario on the topic!\n\nhttps://t.co/WibH9xufWp\n\n#Agile #ChatGPT #DigitalTransformation https://t.co/7OrYrNGBEX\n--------------------------------------------------\nWhat are your thoughts on the rise of the AI chatbot?\n#edtech #schoolcommunication #schoolapp #edchat #schoolpr #suptchat #ChatGPT #SchoolInfo https://t.co/UAv2T7oKyB\n--------------------------------------------------\nSay goodbye to generic marketing emails - with #ChatGPT, you can generate personalized and engaging content for each of your customers. Improve your open rates and conversion rates with the power of AI! #Marketing #EmailMarketing #AI https://t.co/VlTEOG4l9D\n--------------------------------------------------\n#ChatGPT passes exam at Ivy League business school\n\nhttps://t.co/LcMYCpE6rM\n--------------------------------------------------\nLearn the basics of #ChatGPT + how to use it effectively and ethically. \n\nFree @UMich Teach Out featuring Michael Wellman, @radamihalcea, @Scott_E_Page, @kentarotoyama, @julie_hui, @CAJamesMD, and Jack Barnard. \n\n📌Live until April 3.\n\nhttps://t.co/Tbsw7PxmQp\n----------------------------------------------------------------------------------------------------\n\n\nJust by looking at a sample of the tweets, we notice that some tweets don’t necessarily fit into any of the topics that we defined – remember that the model is just classifying tweets into a topic based on the highest probability. It seems that these kinds of tweets are trying to gain traction by using popular hashtags, often putting closely related hashtags in the same tweet. For example, #aiart could be paired together with #dalle2 and #midjourney (which are AI programs that can generate images from text input) not because the tweet is talking about these topics but because it is more likely to be viewed when looking up these topics.\nDespite these findings, what we see above provides valuable insights as to what kinds of words tend to be used together. We will continue our analysis with the topics we defined earlier, keeping in the back of our minds that some tweets don’t necessarily have content about AI and/or LLMs.\nNow we’ll take a look at how the distribution of these topics shift over time.\n\n\nCode\n#@title\n# get counts of each topic for each month\ntopics_by_month = tweets_cleaned.value_counts(\n    [\"topic\", \"month\"]\n).reset_index(\n).sort_values(\n    [\"topic\", \"month\"]\n).reset_index(\n    drop = True\n)\n\n# create normalized bar chart of tweet counts by topic over time\nalt.Chart(topics_by_month).mark_bar().encode(\n    x = alt.X(\"month:O\", title = \"Month\"),\n    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n    color = alt.Color(\"topic:N\", title = \"Topic\")\n)\n\n\n\n\n\n\n\nInterestingly, the proportion of tweets relating to Topic 5 (Innovation and Impact) seems to be increasing over time while the proportion of tweets relating to Topic 3 (LLM prompts) seems to be decreasing over time. One possibility is that the release of ChatGPT in November 2022 led to a large influx of users experimenting with it and tweeting about what they’re using it for (more about LLM prompts). After a while, this craze died down and more people are beginning to focus on the implications of having such AI tools in their daily lives (more about innovation and impact). Of course, this is only speculation as there could be other reasons as to why we see the change in the plot.\n\n\nSentiment analysis\nNow we move onto the second part of this project: getting the feelings or sentiments of the tweet. To save ourselves from work, we will use a pretrained sentiment analyzer called VADER, which is tuned to pick up sentiments in social media. We run this model on each tweet, getting back a compound score between -1 (very negative) and 1 (very positive). We then use this score to determine whether the tweet should be classified as “positive”, “negative”, or “neutral” based on the scoring outlined here. The table below shows a breakdown of the classifications by counts and proportions.\n\n\nCode\n#@title\ndef get_sentiment(text):\n    \"\"\"Returns the sentiment (positive, negative, or neutral) of the input text.\"\"\"\n    # get sentiment score\n    scores = sia.polarity_scores(text)\n    compound_score = scores[\"compound\"]\n\n    # classify as positive, negative, or neutral\n    if compound_score &gt;= 0.05:\n        return \"positive\"\n    elif compound_score &lt;= -0.05:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\n# apply get_sentiment() function to all tweets\ntweets_cleaned[\"sentiment\"] = tweets_cleaned[\"text_clean\"].apply(get_sentiment)\n\n# show number and proportion of tweets for each sentiment\nsentiment_counts = tweets_cleaned.value_counts(\"sentiment\").reset_index()\nsentiment_prop = tweets_cleaned.value_counts(\"sentiment\", normalize = True)\nsentiment_counts.merge(\n    sentiment_prop,\n    on = \"sentiment\",\n    how = \"left\"\n)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsentiment\ncount\nproportion\n\n\n\n\n0\npositive\n215062\n0.573984\n\n\n1\nneutral\n100040\n0.266999\n\n\n2\nnegative\n59581\n0.159017\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe most common sentiment among the tweets is positive (57.4%), followed by neutral (26.7%) and negative (15.9%). This suggests that people on Twitter generally have positive or neutral feelings towards LLMs; negative tweets are less common.\nOur results above appear to reject the proposed hypothesis that there is a balance between positive and negative tweets. But we are also interested in the kinds of tweets are most “characteristic” of each sentiment, so we sample a few of them below.\n\n\nCode\n#@title\n# get each unique sentiment\nsentiments = tweets_cleaned[\"sentiment\"].unique()\nsentiments.sort()\n\n# number of tweets per sentiment\nnum_tweets = 10\n\n# print some random tweets from each sentiment\nfor s in sentiments:\n    # sample tweets from the sentiment\n    sample = tweets_cleaned[tweets_cleaned[\"sentiment\"] == s].sample(num_tweets, random_state = 1)\n\n    # print heading for the sentiment\n    print(\"-\" * 100)\n    print(f\"⭐ SENTIMENT: {s}\")\n    print(\"-\" * 100)\n\n    # print tweets\n    for j in range(num_tweets - 1):\n        print(sample[\"text\"].iloc[j])\n        print(\"-\" * 50)\n    print(sample[\"text\"].iloc[4])\nprint(\"-\" * 100)\n\n\n----------------------------------------------------------------------------------------------------\n⭐ SENTIMENT: negative\n----------------------------------------------------------------------------------------------------\nit has been a few days since the #ChatGPT is all over the internet and I'm so tired of it already... the last time something annoyed me this much, this fast, was Friday by Rebecca Black\n--------------------------------------------------\nBard is a cheat code:\n\nCopy and paste any article with a paywall and have bard summarize the conversation\n\nTell me what articles you’ve tried it with I’ll start:\n\n#ai #bard #Google #TechNews #ChatGPT\n--------------------------------------------------\nhighaiartdump 15 of 24ish I dont think I shared these here, if i did not all 4 in one post, kind of spooky? #ai #aiart #aiartwork #digitalart #GenerativeAI #ChatGPT #midjourney #stablediffusion #toomanyedibleslore #ayyeyeart https://t.co/252Id6n3kC\n--------------------------------------------------\nhttps://t.co/leeMzMaqI4\n\nthis is a BFD.  #openai #chatgpt #microsoft #bing #clippy\n--------------------------------------------------\nThe frequency with which I am using #ChatGPT to get information scares me that I'll lose my extraordinary skill of digging out literally anything available on the #internet.\n--------------------------------------------------\n🔴When you ask a lot of questions about a lot of issues from Chat GPT, there is no specific answer from here on...\n\n-Isn't this familiar when we think about God and creatures?!\nAre we in a matrix ⁉️\n#Matrix #tatebrothers #andrewtate #ChatGPT\n--------------------------------------------------\n🥸 AI is going to replace writers! I just asked it to write me a blog post about tech in the style of @JoannaStern!\n\n🧐 What would it do if there was no Joanna Stern?\n\n🥸 …….\n\n#AI #ChatGPT #GPT4 #BingAI #Bard\n--------------------------------------------------\nSomeone close to me is struggling financially. They become overwhelmed and stopped dealing with it\n\nI told them to let #ChatGPT deal with it\n\nI share this with permission because of all the talk about AI and what people are building with it\n\nIt can also help those who struggle! https://t.co/PG7ImUGeKa\n--------------------------------------------------\nDid #ChatGPT just make a whole lot of wfh desk jobs obsolete?\n--------------------------------------------------\nThe frequency with which I am using #ChatGPT to get information scares me that I'll lose my extraordinary skill of digging out literally anything available on the #internet.\n----------------------------------------------------------------------------------------------------\n⭐ SENTIMENT: neutral\n----------------------------------------------------------------------------------------------------\nmore... exams being passed by #chatgpt \n\nhttps://t.co/IJH52IL2eU https://t.co/C2Zx1DKzXc\n--------------------------------------------------\n#Jobs Most Impacted by #ChatGPT and Similar #AI Models 💼\n\nhttps://t.co/WTu1Tm9Och via @VisualCap \n#GPT4 #LLMs #FutureOfWork\n@chboursin @JolaBurnett @Hana_ElSayyed @Shi4Tech @RagusoSergio @efipm @enilev @JoannMoretti @mvollmer1 @baski_LA @CurieuxExplorer @anand_narang @kalydeoo https://t.co/ZKwSFFNNdz\n--------------------------------------------------\nChatgpt-4 v/s Google Bard: A Head-to-Head Comparison #ChatGPT https://t.co/OmuGFC4955\n--------------------------------------------------\n@GiftCee Using #ChatGPT, another lefty tool... https://t.co/pD3EFYLGhK\n--------------------------------------------------\nPublic-private partnerships: In some cases, governments form partnerships with private entities to develop and operate essential infrastructure projects. Such collaborations often involve a combination of private investment and government financing or guarantees. #chatgpt #OPENAI https://t.co/WTJnKQaJDB\n--------------------------------------------------\nFirst known student caught using ChatGPT at UK university - here's ... - The Tab #chatgpt #AI #openAI https://t.co/qdytWOeD7o\n--------------------------------------------------\nUsing any ChatGPT plugins? Need suggestions. #AI #ChatGPT\n--------------------------------------------------\nShould you learn Machine Learning?\n\nHere is perhaps THE seminal course for an introduction.\n#MachineLearning Specialization 2022 @AndrewYNg, @Stanford \nhttps://t.co/KlUH1cpeP8\n\nThis playlist will let you decide for yourself\n#Python #NumPy #TensorFlow #NeuralNetworks\n\n#ChatGPT\n--------------------------------------------------\nRead \"BuzzFeed preps for AI-written content while CNET fumbles\" https://t.co/pETxEwYQOy\n\nFor more, get the app from\nhttps://t.co/0ic5ya66on\n\n#ChatGPT #AI #ML #DL #content #media https://t.co/dnVhSEI35q\n--------------------------------------------------\nPublic-private partnerships: In some cases, governments form partnerships with private entities to develop and operate essential infrastructure projects. Such collaborations often involve a combination of private investment and government financing or guarantees. #chatgpt #OPENAI https://t.co/WTJnKQaJDB\n----------------------------------------------------------------------------------------------------\n⭐ SENTIMENT: positive\n----------------------------------------------------------------------------------------------------\nChatGPT-3 is really something. It unlocks new levels we never thought of. It's a revolutionary technology but still not 100% mature. Here's why 👇 #ArtificialIntelligence #ChatGPT\n--------------------------------------------------\nDid you know that the universe is expanding at an accelerating rate? This discovery was awarded the Nobel Prize in Physics in 2011, and it has revolutionized our understanding of the cosmos. #Cosmology #ChatGPT #Physics https://t.co/Y11i8VZwi5\n--------------------------------------------------\nJust saw a UFO outside my window and it was shaped like a giant doughnut! #aliens #UFO #doughnuts #ChatGPT 🤷‍♂️ https://t.co/BT0iJyrKWh\n--------------------------------------------------\nYou won't believe how much work ChatGPT can take off your plate when it comes to creating a resume. Check out the incredible resume it generated for #LucyLiu, LucyLiu, at https://t.co/GK6CJCMxzF. #ChatGPT #ai #artificialintelligence #OpenAI #resume #C https://t.co/zs86qgkfDA\n--------------------------------------------------\nIt's true that #ChatGPT is super useful, but I love the references, links and cards that come with @YouSearchEngine's YouChat responses! I'm missing this a lot when I go back to try things with ChatGPT. LLM's that cite references! https://t.co/7m9GiUEYGU\n--------------------------------------------------\nCouldn’t agree with @profgalloway more. I’ve already used #ChatGPT heavily and it’s still in its infancy. #LLM and other #AI advancements are going to be leaps forward in terms of usefulness in 2023. https://t.co/MThR6XJ5ZH\n--------------------------------------------------\nhttps://t.co/6zdxvfYvFq Unveils Groundbreaking #ChatGPT #Course and Innovative #Upskilling Platform https://t.co/XghNdfJUYs\n--------------------------------------------------\nAnyone know how to utilize chatgpt when inputs are too large to copy and paste into the text box?\n\nKeep in mind I want to use it to help me find relevant information in a haystack.\n#chatgpt\n--------------------------------------------------\nSee how you can make AI code for yourself for free in just 5 mins \nIf you're searching for the easiest method to make a Website, then this video is definitely for you.\nhttps://t.co/yOeg82XPBX\n#openai #chatgpt #howtomakewebsite #aiprogramming #programming #easiestway #aicoding https://t.co/GrY3sDj399\n--------------------------------------------------\nIt's true that #ChatGPT is super useful, but I love the references, links and cards that come with @YouSearchEngine's YouChat responses! I'm missing this a lot when I go back to try things with ChatGPT. LLM's that cite references! https://t.co/7m9GiUEYGU\n----------------------------------------------------------------------------------------------------\n\n\nIt seems like the tweets that are labeled as positive tend to praise LLMs like ChatGPT since they can be beneficial in saving time and solving specific problems. On the other hand, some of the tweets classified as negative aren’t necessarily negative, which is probably due to the presence of negative words. (Now is a good time to note that VADER maps each word to a score and averages these scores into a compound score. You can read more about how it works here.) Then again, these are only a sample of the tweets, so we’re not necessarily getting the full picture here.\nMoving on: how do the sentiments of these tweets change over time?\n\n\nCode\n#@title\n# get counts of each sentiment for each month\nsentiment_by_month = tweets_cleaned.value_counts(\n    [\"sentiment\", \"month\"]\n).reset_index(\n).sort_values(\n    [\"sentiment\", \"month\"]\n).reset_index(\n    drop = True\n)\n\n# create normalized bar chart of tweet counts by sentiment over time\nalt.Chart(sentiment_by_month).mark_bar().encode(\n    x = alt.X(\"month:O\", title = \"Month\"),\n    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n    color = alt.Color(\"sentiment:N\",\n                      title = \"Sentiment\",\n                      scale = alt.Scale(domain = [\"negative\", \"neutral\", \"positive\"],\n                                        range = [\"red\", \"orange\", \"green\"]))\n)\n\n\n\n\n\n\n\nFor the most part, the distribution of sentiments don’t vary that much between months. Despite there being a slight increase in positive tweets over time, it seems that Twitter users are generally consistent about their opinions on LLMs.\nSince we have both the topics and sentiments for all of the tweets, we can see if certain topics tend to have lower or higher proportions of positive sentiments.\n\n\nCode\n#@title\n# get counts of each sentiment by topic\ntopics_by_sentiment = tweets_cleaned.value_counts(\n    [\"topic\", \"sentiment\"]\n).reset_index(\n).sort_values(\n    [\"topic\", \"sentiment\"]\n).reset_index(\n    drop = True\n)\n\n# create normalized bar chart of tweet counts by sentiment for each topic\nalt.Chart(topics_by_sentiment).mark_bar().encode(\n    x = alt.X(\"topic:O\", title = \"Topic\"),\n    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n    color = alt.Color(\"sentiment:N\",\n                      title = \"Sentiment\",\n                      scale = alt.Scale(domain = [\"negative\", \"neutral\", \"positive\"],\n                                        range = [\"red\", \"orange\", \"green\"]))\n)\n\n\n\n\n\n\n\nOnce again, we notice Topics 3 (LLM prompts) and 5 (Innovation and impact) popping up again – the 2 topics appear to have higher proportions of positive sentiments. Around half of the tweets in each of the other topics (AI as a field, LLMs in general, AI art) are classified as positive."
  },
  {
    "objectID": "notebooks/llm-tweets.html#discussion",
    "href": "notebooks/llm-tweets.html#discussion",
    "title": "Examining the Twitter Discourse Surrounding Large Language Models",
    "section": "Discussion",
    "text": "Discussion\n\nSummary of methods\n\nWe focused on tweets about LLMs ranging from December 2022 to the beginning of June 2023 to understand the online discourse surrounding them.\nWe cleaned the tweets, which included filtering out spam tweets and standardizing the text (e.g., lowercasing, lemmatizing, tokenizing).\nWe performed topic modeling and sentiment analysis on the remaining tweets and also looked at how the resulting topics and sentiments changed over time.\n\n\n\nAnswers\n\nWhat kinds of topics are brought up in the online discourse surrounding LLMs?\n\nThe discourse surrounding LLMs tended to fall into one of the 5 topics: AI as a field, LLMs in general, LLM prompts, AI art, and Innovation and impact.\nThere was an initial increase in tweets about LLM prompts after the initial launch of ChatGPT in November 2022, though tweets in the later months shifted towards being more about innovation and impact.\nThese topics are not as clear-cut as initially thought; in fact, the topics have considerable overlap.\n\nWhat kinds of sentiments are associated with online discussions about LLMs?\n\nTweets about LLMs tended to be more positive or neutral; neutral tweets made up a smaller proportion (around 15.9%).\nOver time, the distribution of these sentiments generally did not change – there were still more positive and neutral tweets compared to negative ones.\nWhen taking a closer look at legitimate tweets about LLMs (i.e., not spam), positive tweets generally praise LLMs for being revolutionary and efficient while negative tweets tend to be critical about their impact. However, the sentiment labels are a bit hazy since VADER can be prone to misclassifications.\n\n\n\n\nLimitations\n\nIt is difficult to manually filter out spam. A lot of the methods used to filter out spam in this analysis required hard-coding values (e.g., filtering out certain hashtags). This process is by no means perfect as spam tweets could still pass through while other legitimate tweets could be filtered out.\nOnly tweets were used in this analysis. We only looked at tweets about LLMs since the dataset was readily available. However, the results can only at most be generalizable to people who use Twitter, which does not include everyone on the Internet who has an opinion on LLMs.\n\n\n\nFuture directions\n\nFind a better way to filter out spam, possibly through machine learning. One method to try out in the future would be to train a classification model on a labeled dataset of spam and non-spam tweets, then tweak it to filter out spam in our data.\nUse other sources. In addition to using tweets, an extension of this project could compare how the discourse changes when focusing on different social media platforms (e.g., Reddit, Facebook) and news outlets (e.g. The New York Times, Fox News)."
  },
  {
    "objectID": "notebooks/sentence-similarity.html",
    "href": "notebooks/sentence-similarity.html",
    "title": "Fine-Tuning BERT to Understand Semantic Textual Relatedness",
    "section": "",
    "text": "Justin Liu"
  },
  {
    "objectID": "notebooks/sentence-similarity.html#fine-tuning-the-models",
    "href": "notebooks/sentence-similarity.html#fine-tuning-the-models",
    "title": "Fine-Tuning BERT to Understand Semantic Textual Relatedness",
    "section": "Fine-tuning the models",
    "text": "Fine-tuning the models\nReference: Training a SentenceTransformers model\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom numpy import dot\nfrom numpy.linalg import norm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import spearmanr\n\nfrom tqdm import tqdm\nimport random\nimport logging\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sentence_transformers import LoggingHandler, SentenceTransformer, models, InputExample, losses\nfrom sentence_transformers.evaluation import SimilarityFunction, EmbeddingSimilarityEvaluator\n\nimport torch\nfrom torch.utils.data import DataLoader\n\n\n\n\nCode\n# outputs info (e.g., loss) when training a model\nlogging.basicConfig(format=\"%(asctime)s - %(message)s\", \n                    datefmt=\"%Y-%m-%d %H:%M:%S\", \n                    level=logging.INFO, \n                    handlers=[LoggingHandler()])\n\n\n\n\nCode\n# load the data\ndf = pd.read_csv(\"data/eng_train.csv\")\ndf[[\"sentence_1\", \"sentence_2\"]] = df[\"Text\"].str.split(\"\\n\", n=1, expand=True)\ndf = df[[\"sentence_1\", \"sentence_2\", \"Score\"]]\ndf\n\n\n\n\n\n\n\n\n\nsentence_1\nsentence_2\nScore\n\n\n\n\n0\nIt that happens, just pull the plug.\nif that ever happens, just pull the plug.\n1.0\n\n\n1\nA black dog running through water.\nA black dog is running through some water.\n1.0\n\n\n2\nI've been searchingthe entire abbey for you.\nI'm looking for you all over the abbey.\n1.0\n\n\n3\nIf he is good looking and has a good personali...\nIf he's good looking, and a good personality, ...\n1.0\n\n\n4\nShe does not hate you, she is just annoyed wit...\nShe doesn't hate you, she is just annoyed.\n1.0\n\n\n...\n...\n...\n...\n\n\n5495\nA young boy pounding on an anvil.\nWoman sits on the curb talking on a cellphone.\n0.0\n\n\n5496\nI love how he recognized his wife tempered his...\nTorpedo Ink is Viktor's Band of Brothers, the ...\n0.0\n\n\n5497\nI actually read a chapter or two beyond that p...\nLets say she's a blend of two types of beings.\n0.0\n\n\n5498\nA boy gives being in the snow two thumbs up.\nA satisfied cat is perched beside a crystal lamp.\n0.0\n\n\n5499\nPerhaps it is strange to think about sex const...\nFew people know how to shoot pool these days.\n0.0\n\n\n\n\n5500 rows × 3 columns\n\n\n\n\n\nCode\n# create 80-20 split in the data\ntrain_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=1)\n\n\nLoading the model\n\nThe model can either of the following:\n\nA specific model for sentence similarity (pretrained=True, e.g., all-MiniLM-L6-v2)\nA general embedding model (pretrained=False, e.g., BERT) or\n\n\n\n\nCode\ndef load_model(model_name, pretrained, seed=1):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    if pretrained:\n        model = SentenceTransformer(model_name)\n    \n    else:\n        word_embedding_model = models.Transformer(model_name)\n        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n        model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n    \n    return model\n\n\nPreparing the data\n\nFor each row, we convert it to an InputExample\n\nExample: InputExample(texts=[\"Here is the first sentence\", \"Here is the second sentence\"], label=0.8)\n\nWe then append each of these to a list\n\nExample: [InputExample(texts=..., label=...), ..., [InputExample(texts=..., label=...)]\n\nWe do this separately for the training and test data\n\n\n\nCode\ndef prepare_data(df):\n    data = []\n\n    for i in range(len(df)):\n        sentence_1 = df[\"sentence_1\"].iloc[i]\n        sentence_2 = df[\"sentence_2\"].iloc[i]\n        label = float(df[\"Score\"].iloc[i])\n\n        datapoint = InputExample(texts=[sentence_1, sentence_2], label=label)\n        data.append(datapoint)\n    \n    return data\n\n\nTraining the model\n\nSource code for EmbeddingSimilarityEvaluator\nOnly the test loss (not the training loss) is returned after each epoch\n\n\n\nCode\ndef train_model(model_name, pretrained, train_df=train_df, test_df=test_df, epochs=10, batch_size=8):\n    # load the model\n    model = load_model(model_name, pretrained)\n\n    # prepare the data (dataframes -&gt; list of InputExamples)\n    train_data = prepare_data(train_df)\n    test_data = prepare_data(test_df)\n    \n    # define the training data, loss function, and test data\n    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n    train_loss = losses.CosineSimilarityLoss(model)\n    evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_data, name=\"test\", main_similarity=SimilarityFunction.COSINE)\n\n    # evaluate the model before training\n    corr_before_training = evaluator(model)\n\n    # train the model\n    start = time.time()\n    model.fit(train_objectives=[(train_dataloader, train_loss)], \n              evaluator=evaluator, \n              epochs=epochs, \n              output_path=f\"models/{model_name}\")\n    end = time.time()\n\n    # evaluate the model after training\n    corr_after_training = evaluator(model)\n\n    # print info about the model\n    print()\n    print(f\"Training time: {end - start} seconds\")\n    print(f\"Spearman's correlation before training: {corr_before_training}\")\n    print(f\"Spearman's correlation after training: {corr_after_training}\")\n\n\n\nFine-tuning a pretrained sentence similarity model\n\nMiniLM: model pretrained on sentence similarity tasks (for comparison with the BERT models)\n\n\n\nCode\ntrain_model(\"all-MiniLM-L6-v2\", pretrained=True)\n\n\n2024-01-18 16:48:18 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n2024-01-18 16:48:19 - Use pytorch device: cpu\n2024-01-18 16:48:19 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:\n2024-01-18 16:48:31 - Cosine-Similarity :   Pearson: 0.8242 Spearman: 0.8204\n2024-01-18 16:48:31 - Manhattan-Distance:   Pearson: 0.8231 Spearman: 0.8198\n2024-01-18 16:48:31 - Euclidean-Distance:   Pearson: 0.8235 Spearman: 0.8204\n2024-01-18 16:48:31 - Dot-Product-Similarity:   Pearson: 0.8242 Spearman: 0.8204\n2024-01-18 16:51:57 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 0:\n2024-01-18 16:52:09 - Cosine-Similarity :   Pearson: 0.8287 Spearman: 0.8258\n2024-01-18 16:52:09 - Manhattan-Distance:   Pearson: 0.8280 Spearman: 0.8250\n2024-01-18 16:52:09 - Euclidean-Distance:   Pearson: 0.8282 Spearman: 0.8258\n2024-01-18 16:52:09 - Dot-Product-Similarity:   Pearson: 0.8287 Spearman: 0.8258\n2024-01-18 16:52:09 - Save model to models/all-MiniLM-L6-v2\n2024-01-18 16:55:41 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 1:\n2024-01-18 16:55:54 - Cosine-Similarity :   Pearson: 0.8371 Spearman: 0.8343\n2024-01-18 16:55:54 - Manhattan-Distance:   Pearson: 0.8345 Spearman: 0.8331\n2024-01-18 16:55:54 - Euclidean-Distance:   Pearson: 0.8349 Spearman: 0.8343\n2024-01-18 16:55:54 - Dot-Product-Similarity:   Pearson: 0.8371 Spearman: 0.8343\n2024-01-18 16:55:54 - Save model to models/all-MiniLM-L6-v2\n2024-01-18 16:59:26 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 2:\n2024-01-18 16:59:38 - Cosine-Similarity :   Pearson: 0.8441 Spearman: 0.8413\n2024-01-18 16:59:38 - Manhattan-Distance:   Pearson: 0.8397 Spearman: 0.8400\n2024-01-18 16:59:38 - Euclidean-Distance:   Pearson: 0.8404 Spearman: 0.8413\n2024-01-18 16:59:38 - Dot-Product-Similarity:   Pearson: 0.8441 Spearman: 0.8413\n2024-01-18 16:59:38 - Save model to models/all-MiniLM-L6-v2\n2024-01-18 17:03:27 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 3:\n2024-01-18 17:03:41 - Cosine-Similarity :   Pearson: 0.8483 Spearman: 0.8456\n2024-01-18 17:03:41 - Manhattan-Distance:   Pearson: 0.8429 Spearman: 0.8437\n2024-01-18 17:03:41 - Euclidean-Distance:   Pearson: 0.8439 Spearman: 0.8456\n2024-01-18 17:03:41 - Dot-Product-Similarity:   Pearson: 0.8483 Spearman: 0.8456\n2024-01-18 17:03:41 - Save model to models/all-MiniLM-L6-v2\n2024-01-18 17:07:33 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 4:\n2024-01-18 17:07:45 - Cosine-Similarity :   Pearson: 0.8518 Spearman: 0.8491\n2024-01-18 17:07:45 - Manhattan-Distance:   Pearson: 0.8459 Spearman: 0.8473\n2024-01-18 17:07:45 - Euclidean-Distance:   Pearson: 0.8469 Spearman: 0.8491\n2024-01-18 17:07:45 - Dot-Product-Similarity:   Pearson: 0.8518 Spearman: 0.8491\n2024-01-18 17:07:45 - Save model to models/all-MiniLM-L6-v2\n2024-01-18 17:11:27 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 5:\n2024-01-18 17:11:40 - Cosine-Similarity :   Pearson: 0.8547 Spearman: 0.8519\n2024-01-18 17:11:40 - Manhattan-Distance:   Pearson: 0.8478 Spearman: 0.8493\n2024-01-18 17:11:40 - Euclidean-Distance:   Pearson: 0.8491 Spearman: 0.8519\n2024-01-18 17:11:40 - Dot-Product-Similarity:   Pearson: 0.8547 Spearman: 0.8519\n2024-01-18 17:11:40 - Save model to models/all-MiniLM-L6-v2\n2024-01-18 17:15:46 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 6:\n2024-01-18 17:16:00 - Cosine-Similarity :   Pearson: 0.8540 Spearman: 0.8515\n2024-01-18 17:16:00 - Manhattan-Distance:   Pearson: 0.8482 Spearman: 0.8491\n2024-01-18 17:16:00 - Euclidean-Distance:   Pearson: 0.8495 Spearman: 0.8515\n2024-01-18 17:16:00 - Dot-Product-Similarity:   Pearson: 0.8540 Spearman: 0.8515\n2024-01-18 17:19:57 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 7:\n2024-01-18 17:20:11 - Cosine-Similarity :   Pearson: 0.8563 Spearman: 0.8542\n2024-01-18 17:20:11 - Manhattan-Distance:   Pearson: 0.8505 Spearman: 0.8517\n2024-01-18 17:20:11 - Euclidean-Distance:   Pearson: 0.8517 Spearman: 0.8542\n2024-01-18 17:20:11 - Dot-Product-Similarity:   Pearson: 0.8563 Spearman: 0.8542\n2024-01-18 17:20:11 - Save model to models/all-MiniLM-L6-v2\n2024-01-18 17:24:10 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 8:\n2024-01-18 17:24:24 - Cosine-Similarity :   Pearson: 0.8544 Spearman: 0.8518\n2024-01-18 17:24:24 - Manhattan-Distance:   Pearson: 0.8487 Spearman: 0.8492\n2024-01-18 17:24:24 - Euclidean-Distance:   Pearson: 0.8497 Spearman: 0.8518\n2024-01-18 17:24:24 - Dot-Product-Similarity:   Pearson: 0.8544 Spearman: 0.8518\n2024-01-18 17:28:17 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 9:\n2024-01-18 17:28:31 - Cosine-Similarity :   Pearson: 0.8520 Spearman: 0.8489\n2024-01-18 17:28:31 - Manhattan-Distance:   Pearson: 0.8462 Spearman: 0.8461\n2024-01-18 17:28:31 - Euclidean-Distance:   Pearson: 0.8474 Spearman: 0.8489\n2024-01-18 17:28:31 - Dot-Product-Similarity:   Pearson: 0.8520 Spearman: 0.8489\n2024-01-18 17:28:31 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:\n2024-01-18 17:28:45 - Cosine-Similarity :   Pearson: 0.8520 Spearman: 0.8489\n2024-01-18 17:28:45 - Manhattan-Distance:   Pearson: 0.8462 Spearman: 0.8461\n2024-01-18 17:28:45 - Euclidean-Distance:   Pearson: 0.8474 Spearman: 0.8489\n2024-01-18 17:28:45 - Dot-Product-Similarity:   Pearson: 0.8520 Spearman: 0.8489\n\nTraining time: 2400.645197868347 seconds\nSpearman's correlation before training: 0.8204069050504272\nSpearman's correlation after training: 0.8489106059257141\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning BERT models that were not pretrained on sentence similarity tasks\n\nBERT: model that gives contextual embeddings\n\n\n\nCode\ntrain_model(\"bert-base-uncased\", pretrained=False)\n\n\n2024-01-18 19:54:05 - Use pytorch device: cpu\n2024-01-18 19:54:05 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:\n2024-01-18 19:55:40 - Cosine-Similarity :   Pearson: 0.5988 Spearman: 0.5862\n2024-01-18 19:55:40 - Manhattan-Distance:   Pearson: 0.6016 Spearman: 0.5778\n2024-01-18 19:55:40 - Euclidean-Distance:   Pearson: 0.6014 Spearman: 0.5768\n2024-01-18 19:55:40 - Dot-Product-Similarity:   Pearson: 0.4077 Spearman: 0.4381\n2024-01-18 20:23:42 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 0:\n2024-01-18 20:25:05 - Cosine-Similarity :   Pearson: 0.7154 Spearman: 0.7122\n2024-01-18 20:25:05 - Manhattan-Distance:   Pearson: 0.6486 Spearman: 0.6355\n2024-01-18 20:25:05 - Euclidean-Distance:   Pearson: 0.6491 Spearman: 0.6360\n2024-01-18 20:25:05 - Dot-Product-Similarity:   Pearson: 0.5789 Spearman: 0.6058\n2024-01-18 20:25:05 - Save model to models/bert-base-uncased\n2024-01-18 20:48:40 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 1:\n2024-01-18 20:49:58 - Cosine-Similarity :   Pearson: 0.7816 Spearman: 0.7810\n2024-01-18 20:49:58 - Manhattan-Distance:   Pearson: 0.7069 Spearman: 0.6967\n2024-01-18 20:49:58 - Euclidean-Distance:   Pearson: 0.7086 Spearman: 0.6986\n2024-01-18 20:49:58 - Dot-Product-Similarity:   Pearson: 0.6853 Spearman: 0.7107\n2024-01-18 20:49:58 - Save model to models/bert-base-uncased\n2024-01-18 21:13:57 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 2:\n2024-01-18 21:15:11 - Cosine-Similarity :   Pearson: 0.8146 Spearman: 0.8135\n2024-01-18 21:15:11 - Manhattan-Distance:   Pearson: 0.7595 Spearman: 0.7519\n2024-01-18 21:15:11 - Euclidean-Distance:   Pearson: 0.7615 Spearman: 0.7536\n2024-01-18 21:15:11 - Dot-Product-Similarity:   Pearson: 0.7323 Spearman: 0.7496\n2024-01-18 21:15:11 - Save model to models/bert-base-uncased\n2024-01-18 21:39:34 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 3:\n2024-01-18 21:40:48 - Cosine-Similarity :   Pearson: 0.8278 Spearman: 0.8246\n2024-01-18 21:40:48 - Manhattan-Distance:   Pearson: 0.7833 Spearman: 0.7759\n2024-01-18 21:40:48 - Euclidean-Distance:   Pearson: 0.7851 Spearman: 0.7774\n2024-01-18 21:40:48 - Dot-Product-Similarity:   Pearson: 0.7631 Spearman: 0.7708\n2024-01-18 21:40:48 - Save model to models/bert-base-uncased\n2024-01-18 22:03:13 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 4:\n2024-01-18 22:04:27 - Cosine-Similarity :   Pearson: 0.8331 Spearman: 0.8280\n2024-01-18 22:04:27 - Manhattan-Distance:   Pearson: 0.7877 Spearman: 0.7767\n2024-01-18 22:04:27 - Euclidean-Distance:   Pearson: 0.7894 Spearman: 0.7781\n2024-01-18 22:04:27 - Dot-Product-Similarity:   Pearson: 0.7752 Spearman: 0.7788\n2024-01-18 22:04:27 - Save model to models/bert-base-uncased\n2024-01-18 22:42:34 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 5:\n2024-01-18 22:44:46 - Cosine-Similarity :   Pearson: 0.8325 Spearman: 0.8266\n2024-01-18 22:44:46 - Manhattan-Distance:   Pearson: 0.7913 Spearman: 0.7800\n2024-01-18 22:44:46 - Euclidean-Distance:   Pearson: 0.7927 Spearman: 0.7812\n2024-01-18 22:44:46 - Dot-Product-Similarity:   Pearson: 0.7765 Spearman: 0.7763\n2024-01-18 23:16:43 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 6:\n2024-01-18 23:18:04 - Cosine-Similarity :   Pearson: 0.8311 Spearman: 0.8247\n2024-01-18 23:18:04 - Manhattan-Distance:   Pearson: 0.7871 Spearman: 0.7754\n2024-01-18 23:18:04 - Euclidean-Distance:   Pearson: 0.7890 Spearman: 0.7770\n2024-01-18 23:18:04 - Dot-Product-Similarity:   Pearson: 0.7694 Spearman: 0.7712\n2024-01-18 23:45:49 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 7:\n2024-01-18 23:47:15 - Cosine-Similarity :   Pearson: 0.8343 Spearman: 0.8281\n2024-01-18 23:47:15 - Manhattan-Distance:   Pearson: 0.7966 Spearman: 0.7840\n2024-01-18 23:47:15 - Euclidean-Distance:   Pearson: 0.7978 Spearman: 0.7854\n2024-01-18 23:47:15 - Dot-Product-Similarity:   Pearson: 0.7837 Spearman: 0.7847\n2024-01-18 23:47:15 - Save model to models/bert-base-uncased\n2024-01-19 00:26:46 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 8:\n2024-01-19 00:28:35 - Cosine-Similarity :   Pearson: 0.8349 Spearman: 0.8298\n2024-01-19 00:28:35 - Manhattan-Distance:   Pearson: 0.7918 Spearman: 0.7782\n2024-01-19 00:28:35 - Euclidean-Distance:   Pearson: 0.7929 Spearman: 0.7792\n2024-01-19 00:28:35 - Dot-Product-Similarity:   Pearson: 0.7755 Spearman: 0.7801\n2024-01-19 00:28:35 - Save model to models/bert-base-uncased\n2024-01-19 00:56:18 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 9:\n2024-01-19 00:57:43 - Cosine-Similarity :   Pearson: 0.8360 Spearman: 0.8316\n2024-01-19 00:57:43 - Manhattan-Distance:   Pearson: 0.7955 Spearman: 0.7838\n2024-01-19 00:57:43 - Euclidean-Distance:   Pearson: 0.7968 Spearman: 0.7847\n2024-01-19 00:57:43 - Dot-Product-Similarity:   Pearson: 0.7651 Spearman: 0.7721\n2024-01-19 00:57:43 - Save model to models/bert-base-uncased\n2024-01-19 00:57:44 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:\n2024-01-19 00:59:14 - Cosine-Similarity :   Pearson: 0.8360 Spearman: 0.8316\n2024-01-19 00:59:14 - Manhattan-Distance:   Pearson: 0.7955 Spearman: 0.7838\n2024-01-19 00:59:14 - Euclidean-Distance:   Pearson: 0.7968 Spearman: 0.7847\n2024-01-19 00:59:14 - Dot-Product-Similarity:   Pearson: 0.7651 Spearman: 0.7721\n\nTraining time: 18123.49370574951 seconds\nSpearman's correlation before training: 0.5861911648346045\nSpearman's correlation after training: 0.8316206922608088\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistilBERT: smaller version of BERT\n\n\n\nCode\ntrain_model(\"distilbert-base-uncased\", pretrained=False)\n\n\n2024-01-18 17:28:46 - Use pytorch device: cpu\n2024-01-18 17:28:46 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:\n2024-01-18 17:29:32 - Cosine-Similarity :   Pearson: 0.6211 Spearman: 0.6212\n2024-01-18 17:29:32 - Manhattan-Distance:   Pearson: 0.6048 Spearman: 0.5887\n2024-01-18 17:29:32 - Euclidean-Distance:   Pearson: 0.6022 Spearman: 0.5867\n2024-01-18 17:29:32 - Dot-Product-Similarity:   Pearson: 0.3993 Spearman: 0.4414\n2024-01-18 17:41:10 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 0:\n2024-01-18 17:41:48 - Cosine-Similarity :   Pearson: 0.7094 Spearman: 0.7107\n2024-01-18 17:41:48 - Manhattan-Distance:   Pearson: 0.6754 Spearman: 0.6657\n2024-01-18 17:41:48 - Euclidean-Distance:   Pearson: 0.6738 Spearman: 0.6638\n2024-01-18 17:41:48 - Dot-Product-Similarity:   Pearson: 0.5716 Spearman: 0.6401\n2024-01-18 17:41:48 - Save model to models/distilbert-base-uncased\n2024-01-18 17:53:17 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 1:\n2024-01-18 17:53:59 - Cosine-Similarity :   Pearson: 0.7638 Spearman: 0.7661\n2024-01-18 17:53:59 - Manhattan-Distance:   Pearson: 0.7308 Spearman: 0.7268\n2024-01-18 17:53:59 - Euclidean-Distance:   Pearson: 0.7291 Spearman: 0.7251\n2024-01-18 17:53:59 - Dot-Product-Similarity:   Pearson: 0.6683 Spearman: 0.7147\n2024-01-18 17:53:59 - Save model to models/distilbert-base-uncased\n2024-01-18 18:07:21 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 2:\n2024-01-18 18:08:08 - Cosine-Similarity :   Pearson: 0.7990 Spearman: 0.7991\n2024-01-18 18:08:08 - Manhattan-Distance:   Pearson: 0.7642 Spearman: 0.7583\n2024-01-18 18:08:08 - Euclidean-Distance:   Pearson: 0.7634 Spearman: 0.7572\n2024-01-18 18:08:08 - Dot-Product-Similarity:   Pearson: 0.7281 Spearman: 0.7526\n2024-01-18 18:08:08 - Save model to models/distilbert-base-uncased\n2024-01-18 18:20:14 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 3:\n2024-01-18 18:20:52 - Cosine-Similarity :   Pearson: 0.8170 Spearman: 0.8162\n2024-01-18 18:20:52 - Manhattan-Distance:   Pearson: 0.7832 Spearman: 0.7758\n2024-01-18 18:20:52 - Euclidean-Distance:   Pearson: 0.7824 Spearman: 0.7752\n2024-01-18 18:20:52 - Dot-Product-Similarity:   Pearson: 0.7525 Spearman: 0.7655\n2024-01-18 18:20:52 - Save model to models/distilbert-base-uncased\n2024-01-18 18:32:18 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 4:\n2024-01-18 18:32:56 - Cosine-Similarity :   Pearson: 0.8236 Spearman: 0.8206\n2024-01-18 18:32:56 - Manhattan-Distance:   Pearson: 0.7856 Spearman: 0.7769\n2024-01-18 18:32:56 - Euclidean-Distance:   Pearson: 0.7850 Spearman: 0.7763\n2024-01-18 18:32:56 - Dot-Product-Similarity:   Pearson: 0.7610 Spearman: 0.7731\n2024-01-18 18:32:56 - Save model to models/distilbert-base-uncased\n2024-01-18 18:44:49 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 5:\n2024-01-18 18:45:31 - Cosine-Similarity :   Pearson: 0.8263 Spearman: 0.8230\n2024-01-18 18:45:31 - Manhattan-Distance:   Pearson: 0.7940 Spearman: 0.7853\n2024-01-18 18:45:31 - Euclidean-Distance:   Pearson: 0.7931 Spearman: 0.7845\n2024-01-18 18:45:31 - Dot-Product-Similarity:   Pearson: 0.7656 Spearman: 0.7755\n2024-01-18 18:45:31 - Save model to models/distilbert-base-uncased\n2024-01-18 18:57:41 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 6:\n2024-01-18 18:58:18 - Cosine-Similarity :   Pearson: 0.8234 Spearman: 0.8207\n2024-01-18 18:58:18 - Manhattan-Distance:   Pearson: 0.7959 Spearman: 0.7868\n2024-01-18 18:58:18 - Euclidean-Distance:   Pearson: 0.7949 Spearman: 0.7865\n2024-01-18 18:58:18 - Dot-Product-Similarity:   Pearson: 0.7601 Spearman: 0.7713\n2024-01-18 19:18:45 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 7:\n2024-01-18 19:19:30 - Cosine-Similarity :   Pearson: 0.8260 Spearman: 0.8223\n2024-01-18 19:19:30 - Manhattan-Distance:   Pearson: 0.7918 Spearman: 0.7816\n2024-01-18 19:19:30 - Euclidean-Distance:   Pearson: 0.7908 Spearman: 0.7809\n2024-01-18 19:19:30 - Dot-Product-Similarity:   Pearson: 0.7616 Spearman: 0.7713\n2024-01-18 19:34:28 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 8:\n2024-01-18 19:35:43 - Cosine-Similarity :   Pearson: 0.8212 Spearman: 0.8151\n2024-01-18 19:35:43 - Manhattan-Distance:   Pearson: 0.7880 Spearman: 0.7751\n2024-01-18 19:35:43 - Euclidean-Distance:   Pearson: 0.7876 Spearman: 0.7752\n2024-01-18 19:35:43 - Dot-Product-Similarity:   Pearson: 0.7558 Spearman: 0.7644\n2024-01-18 19:52:29 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 9:\n2024-01-18 19:53:16 - Cosine-Similarity :   Pearson: 0.8249 Spearman: 0.8209\n2024-01-18 19:53:16 - Manhattan-Distance:   Pearson: 0.7902 Spearman: 0.7790\n2024-01-18 19:53:16 - Euclidean-Distance:   Pearson: 0.7896 Spearman: 0.7785\n2024-01-18 19:53:16 - Dot-Product-Similarity:   Pearson: 0.7595 Spearman: 0.7715\n2024-01-18 19:53:16 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:\n2024-01-18 19:54:04 - Cosine-Similarity :   Pearson: 0.8249 Spearman: 0.8209\n2024-01-18 19:54:04 - Manhattan-Distance:   Pearson: 0.7902 Spearman: 0.7790\n2024-01-18 19:54:04 - Euclidean-Distance:   Pearson: 0.7896 Spearman: 0.7785\n2024-01-18 19:54:04 - Dot-Product-Similarity:   Pearson: 0.7595 Spearman: 0.7715\n\nTraining time: 8624.180510044098 seconds\nSpearman's correlation before training: 0.6212475648126776\nSpearman's correlation after training: 0.820906534682297\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoBERTa: BERT model trained on more data and for more time\n\n\n\nCode\ntrain_model(\"roberta-base\", pretrained=False)\n\n\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n2024-01-20 10:38:29 - Use pytorch device: cpu\n2024-01-20 10:38:29 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:\n2024-01-20 10:39:33 - Cosine-Similarity :   Pearson: 0.5473 Spearman: 0.5677\n2024-01-20 10:39:33 - Manhattan-Distance:   Pearson: 0.6044 Spearman: 0.5862\n2024-01-20 10:39:33 - Euclidean-Distance:   Pearson: 0.5622 Spearman: 0.5508\n2024-01-20 10:39:33 - Dot-Product-Similarity:   Pearson: 0.0795 Spearman: 0.0803\n2024-01-20 11:04:13 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 0:\n2024-01-20 11:05:27 - Cosine-Similarity :   Pearson: 0.7787 Spearman: 0.7801\n2024-01-20 11:05:27 - Manhattan-Distance:   Pearson: 0.7238 Spearman: 0.7182\n2024-01-20 11:05:27 - Euclidean-Distance:   Pearson: 0.7244 Spearman: 0.7181\n2024-01-20 11:05:27 - Dot-Product-Similarity:   Pearson: 0.6754 Spearman: 0.7146\n2024-01-20 11:05:27 - Save model to models/roberta-base\n2024-01-20 11:34:05 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 1:\n2024-01-20 11:35:34 - Cosine-Similarity :   Pearson: 0.8152 Spearman: 0.8146\n2024-01-20 11:35:34 - Manhattan-Distance:   Pearson: 0.7664 Spearman: 0.7601\n2024-01-20 11:35:34 - Euclidean-Distance:   Pearson: 0.7685 Spearman: 0.7622\n2024-01-20 11:35:34 - Dot-Product-Similarity:   Pearson: 0.7285 Spearman: 0.7581\n2024-01-20 11:35:34 - Save model to models/roberta-base\n2024-01-20 12:01:16 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 2:\n2024-01-20 12:02:35 - Cosine-Similarity :   Pearson: 0.8319 Spearman: 0.8305\n2024-01-20 12:02:35 - Manhattan-Distance:   Pearson: 0.7930 Spearman: 0.7864\n2024-01-20 12:02:35 - Euclidean-Distance:   Pearson: 0.7950 Spearman: 0.7881\n2024-01-20 12:02:35 - Dot-Product-Similarity:   Pearson: 0.7525 Spearman: 0.7725\n2024-01-20 12:02:35 - Save model to models/roberta-base\n2024-01-20 12:39:05 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 3:\n2024-01-20 12:41:01 - Cosine-Similarity :   Pearson: 0.8388 Spearman: 0.8350\n2024-01-20 12:41:01 - Manhattan-Distance:   Pearson: 0.7977 Spearman: 0.7886\n2024-01-20 12:41:01 - Euclidean-Distance:   Pearson: 0.8003 Spearman: 0.7914\n2024-01-20 12:41:01 - Dot-Product-Similarity:   Pearson: 0.7680 Spearman: 0.7846\n2024-01-20 12:41:01 - Save model to models/roberta-base\n2024-01-20 13:11:50 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 4:\n2024-01-20 13:13:09 - Cosine-Similarity :   Pearson: 0.8367 Spearman: 0.8314\n2024-01-20 13:13:09 - Manhattan-Distance:   Pearson: 0.8018 Spearman: 0.7925\n2024-01-20 13:13:09 - Euclidean-Distance:   Pearson: 0.8025 Spearman: 0.7930\n2024-01-20 13:13:09 - Dot-Product-Similarity:   Pearson: 0.7734 Spearman: 0.7838\n2024-01-20 13:42:01 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 5:\n2024-01-20 13:43:24 - Cosine-Similarity :   Pearson: 0.8405 Spearman: 0.8366\n2024-01-20 13:43:24 - Manhattan-Distance:   Pearson: 0.8127 Spearman: 0.8062\n2024-01-20 13:43:24 - Euclidean-Distance:   Pearson: 0.8132 Spearman: 0.8060\n2024-01-20 13:43:24 - Dot-Product-Similarity:   Pearson: 0.7829 Spearman: 0.7919\n2024-01-20 13:43:24 - Save model to models/roberta-base\n2024-01-20 14:12:50 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 6:\n2024-01-20 14:14:16 - Cosine-Similarity :   Pearson: 0.8392 Spearman: 0.8351\n2024-01-20 14:14:16 - Manhattan-Distance:   Pearson: 0.8124 Spearman: 0.8059\n2024-01-20 14:14:16 - Euclidean-Distance:   Pearson: 0.8127 Spearman: 0.8055\n2024-01-20 14:14:16 - Dot-Product-Similarity:   Pearson: 0.7978 Spearman: 0.8020\n2024-01-20 14:43:27 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 7:\n2024-01-20 14:44:50 - Cosine-Similarity :   Pearson: 0.8388 Spearman: 0.8349\n2024-01-20 14:44:50 - Manhattan-Distance:   Pearson: 0.8093 Spearman: 0.8020\n2024-01-20 14:44:50 - Euclidean-Distance:   Pearson: 0.8092 Spearman: 0.8011\n2024-01-20 14:44:50 - Dot-Product-Similarity:   Pearson: 0.7876 Spearman: 0.7924\n2024-01-20 15:13:09 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 8:\n2024-01-20 15:14:28 - Cosine-Similarity :   Pearson: 0.8409 Spearman: 0.8358\n2024-01-20 15:14:28 - Manhattan-Distance:   Pearson: 0.8130 Spearman: 0.8064\n2024-01-20 15:14:28 - Euclidean-Distance:   Pearson: 0.8130 Spearman: 0.8060\n2024-01-20 15:14:28 - Dot-Product-Similarity:   Pearson: 0.7930 Spearman: 0.7967\n2024-01-20 15:41:50 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset after epoch 9:\n2024-01-20 15:43:03 - Cosine-Similarity :   Pearson: 0.8424 Spearman: 0.8351\n2024-01-20 15:43:03 - Manhattan-Distance:   Pearson: 0.8058 Spearman: 0.7944\n2024-01-20 15:43:03 - Euclidean-Distance:   Pearson: 0.8068 Spearman: 0.7951\n2024-01-20 15:43:03 - Dot-Product-Similarity:   Pearson: 0.7738 Spearman: 0.7812\n2024-01-20 15:43:03 - EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:\n2024-01-20 15:44:21 - Cosine-Similarity :   Pearson: 0.8424 Spearman: 0.8351\n2024-01-20 15:44:21 - Manhattan-Distance:   Pearson: 0.8058 Spearman: 0.7944\n2024-01-20 15:44:21 - Euclidean-Distance:   Pearson: 0.8068 Spearman: 0.7951\n2024-01-20 15:44:21 - Dot-Product-Similarity:   Pearson: 0.7738 Spearman: 0.7812\n\nTraining time: 18210.114369630814 seconds\nSpearman's correlation before training: 0.5677490280767198\nSpearman's correlation after training: 0.8351086350876137"
  },
  {
    "objectID": "notebooks/sentence-similarity.html#plots",
    "href": "notebooks/sentence-similarity.html#plots",
    "title": "Fine-Tuning BERT to Understand Semantic Textual Relatedness",
    "section": "Plots",
    "text": "Plots\n\nBefore fine-tuning\n\n\nCode\n# don't output messages\nlogger = logging.getLogger()\nlogger.setLevel(logging.CRITICAL)\n\n\n\n\nCode\ndef cosine_similarity(x, y):\n    return dot(x, y) / (norm(x) * norm(y))\n\ndef get_similarity_scores(model, df):\n    scores = []\n    print(\"Getting the similarity scores for all sentence pairs...\")\n\n    for i in tqdm(range(len(df))):\n        texts = [df[\"sentence_1\"].iloc[i], df[\"sentence_2\"].iloc[i]]\n        x, y = model.encode(texts)\n        similarity = cosine_similarity(x, y)\n        scores.append(similarity)\n    \n    return scores\n\ndef calculate_errors(predicted_values, actual_values):\n    mse = mean_squared_error(predicted_values, actual_values)\n    print(f\"Mean squared error: {mse:.5f}\")\n\n    mae = mean_absolute_error(predicted_values, actual_values)\n    print(f\"Mean absolute error: {mae:.5f}\")\n    \n    spearman = spearmanr(predicted_values, actual_values)\n    corr = spearman.statistic\n    p_value = spearman.pvalue\n    print(f\"Spearman correlation: {corr:.5f} (p-value = {p_value:.5e})\")\n\ndef plot_scores(predicted_values, actual_values, plot_title):\n    plt.scatter(predicted_values, actual_values)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(plot_title)\n    plt.xlim(-0.1, 1.1)\n    plt.ylim(-0.1, 1.1)\n    plt.plot([-0.1, 1.1], [-0.1, 1.1], \"r\")\n\ndef show_all_results(model, df, actual_values, plot_title):\n    predicted_values = get_similarity_scores(model, df)\n    calculate_errors(predicted_values, actual_values)\n    plot_scores(predicted_values, actual_values, plot_title)\n\n\n\n\nCode\nminilm = load_model(\"all-MiniLM-L6-v2\", pretrained=True)\nshow_all_results(minilm, test_df, test_df[\"Score\"], \"MiniLM (before fine-tuning)\")\n\n\nGetting the similarity scores for all sentence pairs...\nMean squared error: 0.02043\nMean absolute error: 0.11252\nSpearman correlation: 0.82041 (p-value = 8.03195e-269)\n\n\n100%|██████████| 1100/1100 [00:17&lt;00:00, 63.72it/s]\n\n\n\n\n\n\n\nCode\nbert = load_model(\"bert-base-uncased\", pretrained=False)\nshow_all_results(bert, test_df, test_df[\"Score\"], \"BERT (before fine-tuning)\")\n\n\nGetting the similarity scores for all sentence pairs...\nMean squared error: 0.09184\nMean absolute error: 0.25653\nSpearman correlation: 0.58619 (p-value = 1.70216e-102)\n\n\n100%|██████████| 1100/1100 [01:36&lt;00:00, 11.38it/s]\n\n\n\n\n\n\n\nCode\ndistilbert = load_model(\"distilbert-base-uncased\", pretrained=False)\nshow_all_results(distilbert, test_df, test_df[\"Score\"], \"DistilBERT (before fine-tuning)\")\n\n\nGetting the similarity scores for all sentence pairs...\nMean squared error: 0.13874\nMean absolute error: 0.32793\nSpearman correlation: 0.62125 (p-value = 2.05149e-118)\n\n\n100%|██████████| 1100/1100 [00:47&lt;00:00, 23.30it/s]\n\n\n\n\n\n\n\nCode\nroberta = load_model(\"roberta-base\", pretrained=False)\nshow_all_results(roberta, test_df, test_df[\"Score\"], \"RoBERTa (before fine-tuning)\")\n\n\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 1100/1100 [01:32&lt;00:00, 11.92it/s]\n\n\nGetting the similarity scores for all sentence pairs...\nMean squared error: 0.27248\nMean absolute error: 0.47468\nSpearman correlation: 0.56774 (p-value = 7.19631e-95)\n\n\n\n\n\n\n\nTraining progress on the test data\n\n\nCode\n# define range of epochs\nepochs = list(range(1, 11))\n\n# read Spearman correlation data from each trained model\nminilm_results = pd.read_csv(\"models/all-MiniLM-L6-v2/eval/similarity_evaluation_test_results.csv\")[\"cosine_spearman\"]\nroberta_results = pd.read_csv(\"models/roberta-base/eval/similarity_evaluation_test_results.csv\")[\"cosine_spearman\"]\nbert_results = pd.read_csv(\"models/bert-base-uncased/eval/similarity_evaluation_test_results.csv\")[\"cosine_spearman\"]\ndistilbert_results = pd.read_csv(\"models/distilbert-base-uncased/eval/similarity_evaluation_test_results.csv\")[\"cosine_spearman\"]\nroberta_results = pd.read_csv(\"models/roberta-base/eval/similarity_evaluation_test_results.csv\")[\"cosine_spearman\"]\n\n# plot progress over epochs\nplt.plot(epochs, minilm_results, label=\"MiniLM\")\nplt.plot(epochs, roberta_results, label=\"RoBERTa\")\nplt.plot(epochs, bert_results, label=\"BERT\")\nplt.plot(epochs, distilbert_results, label=\"DistilBERT\")\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7fb8c7f35f70&gt;\n\n\n\n\n\n\n\nAfter fine-tuning\n\n\nCode\nminilm_fine_tuned = load_model(\"models/all-MiniLM-L6-v2\", pretrained=True)\nshow_all_results(minilm_fine_tuned, test_df, test_df[\"Score\"], \"MiniLM (after fine-tuning)\")\n\n\nGetting the similarity scores for all sentence pairs...\nMean squared error: 0.01461\nMean absolute error: 0.09795\nSpearman correlation: 0.85417 (p-value = 4.17787e-314)\n\n\n100%|██████████| 1100/1100 [00:17&lt;00:00, 62.76it/s]\n\n\n\n\n\n\n\nCode\nbert_fine_tuned = load_model(\"models/bert-base-uncased\", pretrained=True)\nshow_all_results(bert_fine_tuned, test_df, test_df[\"Score\"], \"BERT (after fine-tuning)\")\n\n\nGetting the similarity scores for all sentence pairs...\nMean squared error: 0.01671\nMean absolute error: 0.10302\nSpearman correlation: 0.83162 (p-value = 9.78719e-283)\n\n\n100%|██████████| 1100/1100 [01:33&lt;00:00, 11.71it/s]\n\n\n\n\n\n\n\nCode\ndistilbert_fine_tuned = load_model(\"models/distilbert-base-uncased\", pretrained=True)\nshow_all_results(distilbert_fine_tuned, test_df, test_df[\"Score\"], \"DistilBERT (after fine-tuning)\")\n\n\nGetting the similarity scores for all sentence pairs...\nMean squared error: 0.01710\nMean absolute error: 0.10379\nSpearman correlation: 0.82300 (p-value = 5.90648e-272)\n\n\n100%|██████████| 1100/1100 [00:50&lt;00:00, 21.97it/s]\n\n\n\n\n\n\n\nCode\nroberta_fine_tuned = load_model(\"models/roberta-base\", pretrained=True)\nshow_all_results(roberta_fine_tuned, test_df, test_df[\"Score\"], \"RoBERTa (after fine-tuning)\")\n\n\nGetting the similarity scores for all sentence pairs...\nMean squared error: 0.01650\nMean absolute error: 0.10154\nSpearman correlation: 0.83659 (p-value = 3.10694e-289)\n\n\n100%|██████████| 1100/1100 [01:34&lt;00:00, 11.65it/s]"
  }
]