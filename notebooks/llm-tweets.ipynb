{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format: \n",
        "  html:\n",
        "    code-fold: show\n",
        "    code-tools: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDpvjwAOhC8g"
      },
      "source": [
        "# Examining the Twitter Discourse Surrounding Large Language Models\n",
        "\n",
        "Justin Liu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZgMfaL8PQ7A"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9o4ReG4hJey"
      },
      "source": [
        "### Motivation\n",
        "\n",
        "Over the past year or so, the field of generative artificial intelligence has seen a huge rise in popularity. In particular, large language models (LLMs) that have been trained on unprecedented amounts of data can process langauge and respond to user inputs at a humanlike level. A prime example of this [ChatGPT](https://openai.com/blog/chatgpt), a chatbot released on November 30, 2022, that can answer (almost) any question that it is given. LLMs are also used in generative AI art models like [DALL-E](https://openai.com/research/dall-e) and [Midjourney](https://www.midjourney.com/), which can turn any text imaginable into realistic images. With the increasing availability of these tools to the general public, it is becoming easier than ever to utilize these LLMs without much technical experience. In fact, many have praised them for being revolutionary and believe that they will only improve over time.\n",
        "\n",
        "However, the use of these models have also been at the center of countless debates. There have been heated discussions about whether AI-generated art that \"steals\" work from actual artists can be considered real art, with controversies ranging from an image created by Midjourney winning first prize at an art contest ([link](https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html)) to using AI to save time on drawing backgrounds from scratch in animated films ([link](https://www.polygon.com/23581376/netflix-wit-studio-short-film-ai-controversy)). And ChatGPT, with its capability to perform a wide array of often very specific tasks, could threaten to replace numerous jobs over the next several years ([link](https://www.businessinsider.com/chatgpt-jobs-at-risk-replacement-artificial-intelligence-ai-labor-trends-2023-02)).\n",
        "\n",
        "The present analysis seeks to answer a seemingly simple question: *What are people actually talking about when it comes to LLMs?* As many of these tools are currently available for public use, it makes sense to look at how everyday people (not just specialists) are interacting with them. As a case study, we will focus on the social media platform Twitter since it provides an abundant source of data that can be used to analyze the discourse surrounding LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaKPmsdru3v0"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset we use in this analysis ([Large Language Models: the tweets](https://www.kaggle.com/datasets/konradb/chatgpt-the-tweets)) is made publicly available by Konrad Banachewicz on [Kaggle](https://www.kaggle.com/). It includes English tweets about LLMs from a wide range of Twitter users and comes with metadata (date of tweet, whether the user is verified, etc.). The tweets start from December 2022, and the dataset is updated daily with new tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ6rVg48M2fl"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. *What kinds of topics are brought up in the online discourse surrounding LLMs?*\n",
        "\n",
        "    - **Hypothesis:** The discourse surrounding LLMs spans a variety of topics (e.g. advances in the sciences, questions relating to ethics and the humanities) that reflect the diversity of social media users.\n",
        "    - **Methods:** We implement topic modeling by fitting an LDA model to find the most optimal grouping of tweets about LLMs. We also look into how the distribution of the resulting topics change over time.\n",
        "\n",
        "2. *What kinds of sentiments are associated with online discussions about LLMs?*\n",
        "\n",
        "    - **Hypothesis:** There is a balance between positive and negative sentiments, reflecting a split between proponents and critics of AI.\n",
        "    - **Methods:** We carry out sentiment analysis on our tweets, which are each classified as \"positive\", \"neutral\", or \"negative\". We also examine how these sentiments vary over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZmowwpI2Nb4"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEZuG5pnQCbQ"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LWRpMCDKYyq"
      },
      "source": [
        "In order to access the dataset, we need to download it from Kaggle.\n",
        "\n",
        "**Note:** At the time of this writing (June 15, 2023), the latest version of the dataset contains nothing. Instead, we will use the last version that had the tweets ([Version 172](https://www.kaggle.com/datasets/konradb/chatgpt-the-tweets/versions/172)), which has already been downloaded and stored in Google Drive. The commands below download that dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDxEFMLGTTGP",
        "outputId": "c21d3085-56fe-4cb2-90d2-b49f38c1f5ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Oax8ZEqZ4mzU8Pr0gbD4ZXXZt-GZHdVE\n",
            "To: /content/chatgpt-the-tweets.zip\n",
            "100% 95.1M/95.1M [00:02<00:00, 44.4MB/s]\n",
            "Archive:  chatgpt-the-tweets.zip\n",
            "  inflating: ./chatgpt-the-tweets/tweets.csv  \n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "!rm -rf chatgpt-the-tweets\n",
        "!gdown 1Oax8ZEqZ4mzU8Pr0gbD4ZXXZt-GZHdVE\n",
        "!unzip chatgpt-the-tweets.zip -d ./chatgpt-the-tweets\n",
        "!rm chatgpt-the-tweets.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i68Yw6OVSnMq"
      },
      "source": [
        "This code below is for downloading the latest version of the dataset (currently commented out, see the note above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBkHNFUSIwe_"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# #@title\n",
        "# # get API token and dataset from Kaggle\n",
        "# api_token = {\"username\": \"KAGGLE_USERNAME\", \"key\": \"KAGGLE_KEY\"}\n",
        "# dataset = \"konradb/chatgpt-the-tweets\"\n",
        "\n",
        "# dataset_name = dataset.split(\"/\")[1]\n",
        "# dataset_filename = dataset_name + \".zip\"\n",
        "\n",
        "# !rm -rf {dataset_name}\n",
        "# !rm -rf ~/.kaggle\n",
        "# !mkdir ~/.kaggle\n",
        "# !touch ~/.kaggle/kaggle.json\n",
        "\n",
        "# import json\n",
        "# with open(\"/root/.kaggle/kaggle.json\", \"w\") as file:\n",
        "#     json.dump(api_token, file)\n",
        "\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# !kaggle datasets download -d {dataset}\n",
        "# !unzip {dataset_filename} -d ./{dataset_name}\n",
        "# !rm {dataset_filename}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viQ3ACgBLU6k"
      },
      "source": [
        "We then import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVZzR_5lQMVB"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# install packages\n",
        "%%capture\n",
        "!pip install pyLDAvis\n",
        "\n",
        "# import packages\n",
        "import gensim\n",
        "import pyLDAvis.gensim\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import warnings\n",
        "import altair as alt\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download(\"vader_lexicon\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucahvpsgdpDe"
      },
      "source": [
        "### Cleaning the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jao4b7fGay84"
      },
      "source": [
        "We'll take a look at the dataset, dropping rows where either the tweet (`text`) or date (`date`) is missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "kjKly_FVQVfF",
        "outputId": "ad04b6e9-d179-48a1-e2fa-d180f482efca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-1d3f7600107f>:3: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  tweets = pd.read_csv(\"chatgpt-the-tweets/tweets.csv\").dropna(subset = [\"text\", \"date\"])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c5878b40-e725-434a-ba76-a9133bb301a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_name</th>\n",
              "      <th>text</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_description</th>\n",
              "      <th>user_created</th>\n",
              "      <th>user_followers</th>\n",
              "      <th>user_friends</th>\n",
              "      <th>user_favourites</th>\n",
              "      <th>user_verified</th>\n",
              "      <th>date</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>reigndomains ðŸ‘‘</td>\n",
              "      <td>https://t.co/6tFaOonLtv ðŸ”¥ for sale .\\n\\n#Royal...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Brand Name | https://t.co/Z4d6GWXyWz | https:/...</td>\n",
              "      <td>2019-09-11 04:04:06+00:00</td>\n",
              "      <td>267.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>1300</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-06-10 12:37:16+00:00</td>\n",
              "      <td>['RoyalGPT', 'Royal', 'Domains', 'ai', 'Web3',...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MidJourney LIVE</td>\n",
              "      <td>Exquisite realism photography showcasing an ex...</td>\n",
              "      <td>Follow for Inspiration</td>\n",
              "      <td>ðŸŽ¨ Live feed of Art generated by Midjourney AI ðŸŽ¨</td>\n",
              "      <td>2018-08-28 02:01:04+00:00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-06-10 12:36:56+00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MidjourneyLIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Tech Trend</td>\n",
              "      <td>Top 10 ChatGPT Plugins You Should Use Right No...</td>\n",
              "      <td>Worldwide</td>\n",
              "      <td>A Tech community for industry experts, connect...</td>\n",
              "      <td>2020-09-15 15:37:37+00:00</td>\n",
              "      <td>4380.0</td>\n",
              "      <td>4668.0</td>\n",
              "      <td>242</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-06-10 12:35:00+00:00</td>\n",
              "      <td>['ChatGPT', 'bestChatGPTplugins']</td>\n",
              "      <td>Buffer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Time Blawg</td>\n",
              "      <td>What lawyers will get out of ChatGPT: legal ca...</td>\n",
              "      <td>Scotland... and Beyond</td>\n",
              "      <td>The past, present and future practice of law (...</td>\n",
              "      <td>2010-12-29 18:03:14+00:00</td>\n",
              "      <td>5897.0</td>\n",
              "      <td>6499.0</td>\n",
              "      <td>4693</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-06-10 12:34:49+00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Twitter for Android</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Christine Lopez</td>\n",
              "      <td>down an a But the state of summer8 being money...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-05-06 11:03:29+00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-06-10 12:33:14+00:00</td>\n",
              "      <td>['è½¦éœ‡', 'å«©ç©´', 'chatGPT']</td>\n",
              "      <td>Twitter Web App</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5878b40-e725-434a-ba76-a9133bb301a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5878b40-e725-434a-ba76-a9133bb301a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5878b40-e725-434a-ba76-a9133bb301a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         user_name                                               text  \\\n",
              "0   reigndomains ðŸ‘‘  https://t.co/6tFaOonLtv ðŸ”¥ for sale .\\n\\n#Royal...   \n",
              "1  MidJourney LIVE  Exquisite realism photography showcasing an ex...   \n",
              "2   The Tech Trend  Top 10 ChatGPT Plugins You Should Use Right No...   \n",
              "3   The Time Blawg  What lawyers will get out of ChatGPT: legal ca...   \n",
              "4  Christine Lopez  down an a But the state of summer8 being money...   \n",
              "\n",
              "            user_location                                   user_description  \\\n",
              "0                     NaN  Brand Name | https://t.co/Z4d6GWXyWz | https:/...   \n",
              "1  Follow for Inspiration    ðŸŽ¨ Live feed of Art generated by Midjourney AI ðŸŽ¨   \n",
              "2               Worldwide  A Tech community for industry experts, connect...   \n",
              "3  Scotland... and Beyond  The past, present and future practice of law (...   \n",
              "4                     NaN                                                NaN   \n",
              "\n",
              "                user_created user_followers user_friends user_favourites  \\\n",
              "0  2019-09-11 04:04:06+00:00          267.0        256.0            1300   \n",
              "1  2018-08-28 02:01:04+00:00          100.0          1.0               0   \n",
              "2  2020-09-15 15:37:37+00:00         4380.0       4668.0             242   \n",
              "3  2010-12-29 18:03:14+00:00         5897.0       6499.0            4693   \n",
              "4  2023-05-06 11:03:29+00:00            0.0          5.0               0   \n",
              "\n",
              "  user_verified                       date  \\\n",
              "0         False  2023-06-10 12:37:16+00:00   \n",
              "1         False  2023-06-10 12:36:56+00:00   \n",
              "2         False  2023-06-10 12:35:00+00:00   \n",
              "3         False  2023-06-10 12:34:49+00:00   \n",
              "4         False  2023-06-10 12:33:14+00:00   \n",
              "\n",
              "                                            hashtags               source  \n",
              "0  ['RoyalGPT', 'Royal', 'Domains', 'ai', 'Web3',...   Twitter for iPhone  \n",
              "1                                                NaN       MidjourneyLIVE  \n",
              "2                  ['ChatGPT', 'bestChatGPTplugins']               Buffer  \n",
              "3                                                NaN  Twitter for Android  \n",
              "4                            ['è½¦éœ‡', 'å«©ç©´', 'chatGPT']      Twitter Web App  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# read the data, dropping rows where the tweet or date is missing\n",
        "tweets = pd.read_csv(\"chatgpt-the-tweets/tweets.csv\").dropna(subset = [\"text\", \"date\"])\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C1HS70RMGne"
      },
      "source": [
        "Since we can't see any full tweets in the table above, we sample some random tweets and print them out below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ6k1-aMdye4",
        "outputId": "a9c04ea6-5430-4f5d-ef8d-3e23a0e0398a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "ðŸš€ Boost Your Sales by using the \"Sealing the Deal\" template on Jeda Ai's All-in-One Workspace Canvas.\n",
            "\n",
            "Get your Daily 10K FREE AI Tokens at https://t.co/8NK5W5P55J ðŸ¤©\n",
            "\n",
            "#JedaAI #AI #template #sales #sealthedeal #ChatGPT #GPT4 https://t.co/svVecsO7XF\n",
            "--------------------------------------------------\n",
            "Why Seattle's ban on students using ChatGPT is doomed â€” and what comes next - The Seattle Times https://t.co/chXeUKv874 #chatgpt #AI #openAI\n",
            "--------------------------------------------------\n",
            "We are bringing to you the world's most efficient AI-powered virtual trading assistant that trades on financial markets 10 times faster than humans. Get started with these easy steps ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ”¥ðŸ”¥ðŸ”¥\n",
            "\n",
            "#TradesGPT5 #AI #TradeGPT5 #ChatGPT https://t.co/XMlDajBpAi\n",
            "--------------------------------------------------\n",
            "Are there any #lowcode #nocode tools to building #autogpt like apps? Essentially building AI agents.\n",
            "--------------------------------------------------\n",
            "Pretty sure, it would not fool @pennjillette or @MrTeller but #ChatGPT is psychic ðŸ˜‰ https://t.co/GxcVg997KL\n",
            "--------------------------------------------------\n",
            "Use of low power is efficient is running a #metaverse \n",
            "with it MANTA would be able to work effectively \n",
            "#AI #chatgpt $MAN https://t.co/L7GPOmWXsA\n",
            "--------------------------------------------------\n",
            "I used the new GPT-4 api to 'chat' with a 56-page legal PDF document about the famous supreme court case: Morse v. Frederick\n",
            "\n",
            "Powered by @LangChainAI and @pinecone \n",
            "\n",
            "#openai #chatgpt #gpt4 #legal #lawyers #law https://t.co/WZtqPDS5Dc\n",
            "--------------------------------------------------\n",
            "#Drecur #exbiils #KiCurrency If you are still having issues withdrawing your coin or your platform is been frozen in any of this fake platform #kicurency\n",
            "#Robecoins #Drecur #fastbitra #exbiils SEND A DM\n",
            "NOW  #à¸Šà¸²à¸¥à¹‡à¸­à¸•à¸­à¸­à¸ªà¸•à¸´à¸™ #dollartreats #ChatGPT #Ukraine #cryptocurrency #BTSFESTA https://t.co/V4OoRT1ZVD\n",
            "--------------------------------------------------\n",
            "Smartbrand domain name for DeepMind #Domain #domainnames #AI #domainforsale #Domainsale #Drone #ChatGPT #AI #Google #Amazon #Facebook #OpenAI #Saleforce #Unicorn #NFT #ETH #Web3 #Tech #Silicon #Tesla #SpaceX #Starbase #AIart #MachineLearning #DATA #Sedo #Namecheap #AGI #AIG #Name https://t.co/yClR1y34H2\n",
            "--------------------------------------------------\n",
            "I asked #dalle to create surrealism art of an old college professor who is shocked that #ChatGPT passed the United States Medical Licensing Exam. Here is the picture:\n",
            "\n",
            "#AI #business #education https://t.co/0UufXBNGLO\n",
            "--------------------------------------------------\n",
            "BTC went down to $21,970 in 30 minutes early Friday morning. #BTC #Bitcoin #CryptoNews #cryptomarket #ChatGPT #openai Sentiment Result : Negative @crypto_talkies https://t.co/00hD1VJOGL\n",
            "--------------------------------------------------\n",
            "ðŸ˜±Incredible world!\n",
            "ðŸ’¯With #Midjourney, ANYTHING is possible!\n",
            "\n",
            "âœ¨Don't miss the chance:\n",
            "ðŸ‘‰https://t.co/xIofH8wsbx\n",
            "\n",
            "#AI #AIart #AIArtCommuity #Midjourney #ChatGPT  #AIArtwork #Midjourneyart #creator #NFT #NFTarts #generativeAI #gpt4 https://t.co/DaYIVLYFNQ\n",
            "--------------------------------------------------\n",
            "@kaiviti_cam @grantrobertson1 Parliament is full of degenerates\n",
            "Parliament is full of corruption. \n",
            "Their greed and lies are endless\n",
            "Their power grab is disruptive.\n",
            "\n",
            "We need leaders who are noble\n",
            "We need leaders who are just, \n",
            "Let's vote for integrity and honesty\n",
            "And leave the corrupt in the dust.\n",
            "#chatGPT\n",
            "--------------------------------------------------\n",
            "Here's what happened when ChatGPT and I improvised a scene from a buddy cop movie. You'll notice that ChatGPT sometimes couldn't resist and gave my line too, but overall he was a generous scene partner and I'd love to work together again. #ChatGPT #chatgpt3 #chatbots #chatbot https://t.co/DAU5AJgpw5\n",
            "--------------------------------------------------\n",
            "What do you think AI means for the future of freelance writing?\n",
            "#AI #ChatGPT #freelancewriting\n",
            "--------------------------------------------------\n",
            "Wow! I just got this TEMU invite code &lt;146048044&gt; from chatGPT with real rewards. As soon as I searched for this code in the search bar, I participated in the event and got a lot of rewards. Have a try and you won't regret it! #GPT https://t.co/14JI9CDlwf\n",
            "--------------------------------------------------\n",
            "I added a Game Over state to my AI Text Adventure Game Generator this weekend. #ChatGPT can dream up some pretty brutal fatalities! ðŸ˜µðŸ”¥ https://t.co/dNKOUiVpxq\n",
            "--------------------------------------------------\n",
            "ChatGPT just wrote me this joke: Why was ChatGPT kicked out of the computer science class? Because it kept trying to autocomplete the professor's lectures!  ðŸ¤£#AI #jokes #chatgpt\n",
            "--------------------------------------------------\n",
            "#Bing's #Prometheus \"much more powerful\" than #ChatGPT, designed specifically 4 #search; #EdgeBrowser now w #AI features #chat, #compose. #digitalmarketing #ecommerce #mcommerce #retail #retailmarketing #SEO #SEM #digitaladvertising $MSFT $GOOG $AAPL $AMZN $WMT $TGT $BBY\n",
            "--------------------------------------------------\n",
            "is chatgpt down? ðŸ§ #ChatGPT @OpenAI\n",
            "--------------------------------------------------\n",
            "\"Sales reps are building their own presentations on #ChatGPT and making claims about things they can do for the customer that havenâ€™t been vetted by the corporation. That is a super big risk.\" â€“ J.B. Wood at #TSIAworld, on the use of #AI in B2B @j_b_wood\n",
            "--------------------------------------------------\n",
            "#ChatGPT is making me so much money. ðŸ¤£\n",
            "--------------------------------------------------\n",
            "\"An Artist Asked #ChatGPT  How to Make a Popular Memecoin. The Result Is â€˜TurboToad,â€™ and People Are Betting Millions of Dollars on It\" | @Artnet News $turbo @rhett https://t.co/c70A5Xxj69\n",
            "--------------------------------------------------\n",
            "ðŸ¤£ (Couldnâ€™t help myself)\n",
            "DO YOU PROMPT ENGINEER?  \n",
            "QUICK TIP!\n",
            "\n",
            "What is the Simeon Forking Method on #ChatGPT ? https://t.co/8h2txjxYSg\n",
            "--------------------------------------------------\n",
            "@cmf2x @JBMatthews @ericpaulimd @JohnRTMonsonMD @RCSI_Irl @mortensen_neil @Neil_J_Smart @SWexner @des_winter @TAMISYoda @MarkSoliman @FergaljFleming @ASCRS_1 @TomVargheseJr @DavidCookeMD @ABTS17 @steven_stain @TsengJennifer @DissanaikeMD @juliomayol Thank you for your insights on the topic #ChatGPT \n",
            "I concur ðŸ˜ https://t.co/Tq48IH6Gph\n",
            "--------------------------------------------------\n",
            "Anyway, once I verified that #ChatGPT could create a good summary of Chimamanda's life and #midjourney could do the render, I asked #ChatGPT  to compile a list of prominent African women\n",
            "--------------------------------------------------\n",
            "#care #chatbot #ChatGPT #easier #Health #Jobs #professionals #providers ChatGPT for health care providers: Can the AI chatbot make the professionals' jobs easier? https://t.co/0fLlMrAxSL \n",
            "\n",
            "OpenAI's natural language processing model, ChatGPT, released in December 2022, could... https://t.co/i8awc05fho\n",
            "--------------------------------------------------\n",
            "@Simonkhalaf @kevaldesai I have my doubts that AI would improve classic \"page-rank\" type web search AT ALL. #AI is great and chatting with #ChatGPT or #Bard is a whole new experience. Imo these apps should be separate. I think $MSFT is just about to destroy #Bing one more time. #AI $GOOG\n",
            "--------------------------------------------------\n",
            "CofC Podcast: ChatGPT and Conversational A.I. Explained - The College Today https://t.co/gMO62HEmYn #chatgpt #AI #openAI\n",
            "--------------------------------------------------\n",
            "Clank riffs on selected #ChatGPT  gobbets https://t.co/PMoSMTM3bm\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# sample 30 random tweets and print them out\n",
        "sampled_tweets_1 = tweets.sample(30, random_state = 1).text\n",
        "for i in range(30):\n",
        "    print(\"-\" * 50)\n",
        "    print(sampled_tweets_1.iloc[i])\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajlnwZnlADZ"
      },
      "source": [
        "Looking at some of the tweets above, a few of these are very likely to be spam (e.g., tweets talking about crypto and/or have an abnormally high number of hashtags). Since these tweets are unrelated to the discussion of large language models, we will try to filter these out. (Note that the methods implemented below are not perfect as legitimate tweets could be filtered out while some spam tweets could still remain.) After this process, we sample some of the remaining tweets and print them out below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzW1bol1ljwf",
        "outputId": "acb4556d-946b-4ed6-9d6a-cb29d3f3777c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "ðŸŒŸ Enhance your business with cutting-edge AI technology! Our #ChatGPT for Beginners course offers the perfect introduction for companies embracing the digital world. Sign up now: https://t.co/kAF7l0d2qN #BusinessInnovation #AI https://t.co/KwmFGmh8bW\n",
            "--------------------------------------------------\n",
            "I have accessed the gpt-4-32k API and want to create some more interesting products based on it. \n",
            "\n",
            "Would any genius be willing to give me some suggestions?\n",
            "\n",
            "#ChatGPT #AIGC #developers\n",
            "--------------------------------------------------\n",
            "How accurate is #ChatGPT ? Better ask Stanford computational law experts. But first where did the data derive for the program. If from #fakenews then it will fail tremendously on a wide spectrum but the scope is like a scoop of ice cream the kind @SpeakerPelosi likes to eat. ðŸ¤­\n",
            "--------------------------------------------------\n",
            "The announcement of GPT-4, a language model equipped with an astounding 100 trillion machine learning parameters, has generated considerable excitement in the technology industry.\n",
            "\n",
            "#chatgpt #artificialintelligence #TechNews #tech #YellowStorm #IStandWithMiaNDawood\n",
            "--------------------------------------------------\n",
            "Those asking for AI developers to slow down, might as well stand in front of a Walmart on Black Friday and ask people to please walk slow when the doors openðŸ˜£â€¼ï¸\n",
            "\n",
            "#AI #WallStreet #ChatGPT\n",
            "--------------------------------------------------\n",
            "ðŸ§µ Delving into the realm of AI, I stumbled upon an intriguing article by @stephen_wolfram on the inner workings of #ChatGPT. Let's explore the mechanics of this language model in greater detail. Join me, fellow #AI enthusiasts!\n",
            "--------------------------------------------------\n",
            "No bunty, you aren't an AI-Enthusiast or influencer if you post \"Everyone is using #ChatGPT wrong, here are 69,420 ways to use it right!\"\n",
            "--------------------------------------------------\n",
            "Real AI Assistant power in your iPhone: ChatGPT Shortcut for iPhone. https://t.co/0fE7OGWt87 #ai #iphone #chatgpt\n",
            "--------------------------------------------------\n",
            "I asked a shining journalist #ChatGPT who recently joined the rank to write a report on #mask mandate debate and here it is. #bcpoli #flu #WearAMask #COVID19 https://t.co/TmywAxyiNo https://t.co/QsdL6yxsz3\n",
            "--------------------------------------------------\n",
            "Update version Membership, Login, TypeWriter Text Answer. XChatBot ChatGPT Flutter App. https://t.co/9OAdUm8vQC #chatbot #xchatbot #chatai #ChatGPT #ChatGPTPlus #ChatGPTGOD  #flutter #flutterdev #flutterapp https://t.co/E1TLDqsSgh\n",
            "--------------------------------------------------\n",
            "#AI #ChatGPT #businesstransformation #booklaunch \n",
            "Grab your copy : https://t.co/4iB5O1g1oU https://t.co/Gf4kfIgyXJ\n",
            "--------------------------------------------------\n",
            "Super helpful tutorial on getting more from #ChatGPT https://t.co/ettJ8XlnWI\n",
            "--------------------------------------------------\n",
            "Imperative code may be easier to write, but harder to read. Code is read more often than it is written, it's crucial that it's easy to understand.\n",
            "\n",
            "We can use ChatGPT to convert our imperative for loops into declarative array methods. \n",
            "\n",
            "#chatgpt #cleancode #javascript #typescript https://t.co/smrGe1q5Rj\n",
            "--------------------------------------------------\n",
            "How can we leverage #ChatGPT in testing?\n",
            "\n",
            "@BagmarAnand #UnlockingThePowerOfChatGPT https://t.co/AqIFh5E5yg\n",
            "--------------------------------------------------\n",
            "I now have a #BardAI trial, along with #ChatGPT. Over the next 7 days Iâ€™m going to ask #bard and #chatgpt the same questions and post the results here. Follow me to compare the answers. Any suggestions for â€œabove boardâ€topical questions?\n",
            "--------------------------------------------------\n",
            "#ChatGPT is helping me relearn mathematical proofs by induction. I asked it \"Prove by induction that 11n âˆ’ 6 is divisible by 5 for every positive integer n.\" It did, step by step. Then I said ok \"Prove that n+1 = n-1 for all n\". It said it's impossible and explained why.\n",
            "--------------------------------------------------\n",
            "Using #ChatGPT is a downward spiral for #developers.\n",
            "You use the code it provides, you get lazy so you write no docs anymore. The next version of ChatGPT finds no relevant new info on the internet, only the stuff it already 'knows' and innovation slows down and finally stops...\n",
            "--------------------------------------------------\n",
            "You must be hearing that #ChatGPT &amp; #GPT4 can chat with the documents. You are wondering, what the heck is going on? #langchain and its vectorstores + agents are making the magic\n",
            "https://t.co/rptD55emgo\n",
            "--------------------------------------------------\n",
            "Comparative analysis for #ChatGPT with other alternatives ðŸ§\n",
            "\n",
            "#ArtificialIntelligence https://t.co/bg4zWhQTw4\n",
            "--------------------------------------------------\n",
            "Be careful #Bard straight out lies to give you an answer. https://t.co/UrBpXNBmqH\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "def count_items(str_list):\n",
        "    \"\"\"Takes in a list as a string and returns the number of items in the list\n",
        "    (example: \"['word', 'number']\" would return 2). Returns 0 in the case of\n",
        "    a TypeError.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # remove the brackets, convert to a list, and count the number of items\n",
        "        brackets_removed = re.sub(\"\\[|\\]|'\", \"\", str_list)\n",
        "        list_split = brackets_removed.split(\", \")\n",
        "        return len(list_split)\n",
        "    except TypeError:\n",
        "        # for cases when the value is NaN, return 0\n",
        "        return 0\n",
        "\n",
        "def remove_outliers(df, col_name):\n",
        "    \"\"\"Returns the dataframe with rows where the outliers in the specified\n",
        "    column are removed.\n",
        "    \"\"\"\n",
        "    # calculate interquartile range (IQR)\n",
        "    q1 = df[col_name].quantile(0.25)\n",
        "    q3 = df[col_name].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # remove outliers using the 1.5 * IQR method\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    df_out = df[(df[col_name] > lower) & (df[col_name] < upper)]\n",
        "    return df_out\n",
        "\n",
        "# get the number of hashtags in each tweet and the 'hashtags' column\n",
        "tweets_cleaned = tweets.copy()\n",
        "tweets_cleaned[\"num_hashtags_text\"] = tweets_cleaned[\"text\"].str.count(\"#\")\n",
        "tweets_cleaned[\"num_hashtags_data\"] = tweets_cleaned[\"hashtags\"].map(count_items)\n",
        "\n",
        "# remove rows where number of hashtags is an outlier\n",
        "tweets_cleaned = remove_outliers(tweets_cleaned, \"num_hashtags_text\")\n",
        "tweets_cleaned = remove_outliers(tweets_cleaned, \"num_hashtags_data\")\n",
        "\n",
        "# convert text to lowercase\n",
        "tweets_cleaned[\"text_clean\"] = tweets_cleaned[\"text\"].str.lower()\n",
        "\n",
        "# create regex expression for removing tweets with spam (note that this isn't perfect)\n",
        "# '\\d{10}' is for phone numbers, '[\\u4e00-\\u9fff]+' is for Chinese characters\n",
        "filter_out = [\"crypto\", \"\\$\", \"ðŸš¨\", \"ðŸš€\", \"nft\", \"coin\", \"weatherupdate\", \"temu\", \"\\d{10}\", \"[\\u4e00-\\u9fff]+\"]\n",
        "filter_out_str = \"|\".join(filter_out)\n",
        "\n",
        "# filter out tweets with any of the above words\n",
        "tweets_cleaned[\"hashtags_clean\"] = tweets_cleaned[\"hashtags\"].str.strip('[|]').str.lower()\n",
        "tweets_cleaned = tweets_cleaned[~tweets_cleaned[\"hashtags_clean\"].str.contains(filter_out_str, na = False)]\n",
        "tweets_cleaned = tweets_cleaned[~tweets_cleaned[\"text_clean\"].str.contains(filter_out_str, regex = True)]\n",
        "\n",
        "# sample 20 random tweets and print them out\n",
        "sampled_tweets_2 = tweets_cleaned[\"text\"].sample(20, random_state = 1)\n",
        "for i in range(20):\n",
        "    print(\"-\" * 50)\n",
        "    print(sampled_tweets_2.iloc[i])\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCLZE_6XWzUg"
      },
      "source": [
        "We then clean the data a bit more so that the words can be processed in our models. The main things are:\n",
        "\n",
        "- converting everything to lowercase (done in the previous cell when filtering out spam),\n",
        "- removing hashtags, usernames, and links, and\n",
        "- removing extra whitespace.\n",
        "\n",
        "Some extra filtering steps include:\n",
        "\n",
        "- converting all occurrences of `\"&amp;\"` (HTML symbol for `\"&\"`) and `\"artificialintelligence\"` (most likely from hashtags) to `\"and\"` and `\"artificial intelligence\"`, respectively, as well as\n",
        "- dropping tweets that were the same after preprocessing them, which filters out more possible spam.\n",
        "\n",
        "Again, we sample some of the resulting tweets below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPW4IXrVwjZd",
        "outputId": "b7a952ec-2132-4910-81c7-09471f61054e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "so many topics how to use chatgpt. does anyone have any concerns regarding security and privacy of the data processed through it? startups security privacy\n",
            "--------------------------------------------------\n",
            "this nyt articles ( starts with a pertinent question: how society will greet true artificial intelligence, if and when it arrives. (1) will we panic? (2) start sucking up to our new robot overlords? (3) ignore it and go about our daily lives? chatgpt\n",
            "--------------------------------------------------\n",
            "looks like the ultimate meh, middle of the road, not terribly wrong but not great or insightful either take on software testing, which is what i would expect from something like chatgpt. it's like the most average of takes.\n",
            "--------------------------------------------------\n",
            "fantastic article! it's amazing to see how openai ai chatgpt can be used to create unique experiences.\n",
            "--------------------------------------------------\n",
            "revolutionized the business world by introducing the 914 copy machine. will revolutionize the business world again having created the copy/paste machine. but this can not be used for reasoning. this is not general ai ai chatgpt xerox copymachine copypase\n",
            "--------------------------------------------------\n",
            "the third answer is also scary, since it implies that humans should know what not causing harm means. as a vegan, i can say, most humans have no idea. and also, no. a.i.s seem not to be currently bound by any laws samaltman notion chatgpt shutthemdown artificial intelligence\n",
            "--------------------------------------------------\n",
            "looking for inspiration to jumpstart your writing career? check out chatgpt's 10,000+ prompts and unlock your creative potential! ðŸ”¥ðŸ“ and for those looking to make some extra cash, don't miss this exclusive writingprompts creativity chatgpt\n",
            "--------------------------------------------------\n",
            "samsung: \"chatgpt may be blocked on the company network\" samsung software engineers busted for pasting proprietary code into chatgpt\n",
            "--------------------------------------------------\n",
            "testing the limits of chatgpt\n",
            "--------------------------------------------------\n",
            "this is hilariousðŸ˜‚ chatgpt conversation clone\n",
            "--------------------------------------------------\n",
            "chatgpt it is highly unlikely that a school of fish would follow a duck intentionally. fish are not known to follow ducks, as they are two completely different species with different behaviors and habitats. however, if a duck were swimming in a body of water where there ...\n",
            "--------------------------------------------------\n",
            "â†• power chatgpt resources - 1 page powercheatsheet/list builder that provides 3 simple steps to the world of chatgpt. plr option available\n",
            "--------------------------------------------------\n",
            "5ï¸âƒ£ addressing biases and ai as teachers: bring diverse perspectives to avoid biases and let specialist ais evolve into teachers for future generations of experts. ðŸŒˆðŸ‘©â€ðŸ« airevolution healthcaretransformation specialistai futureofmedicine generativeai chatgpt\n",
            "--------------------------------------------------\n",
            "super excited about bringing \"agents\" to haystack! imagine something like chatgpt having access to your internal data, apis and whatever tool you like. will unlock many cool new use cases.\n",
            "--------------------------------------------------\n",
            "microsoft's edge browser + chatgpt demo ðŸ¤¯ microsoft launched ai-powered bing and edge today. openai openaichatgpt\n",
            "--------------------------------------------------\n",
            "chatgpt is lit using from past several months and it keep updating chatgpt\n",
            "--------------------------------------------------\n",
            "a lively discussion about lifesaving applications of ai-based technologies and how to move innovations from experiment to deployment across industries. thank you again to our panel and audience! orange nvidia llms chatgpt ai innovation siliconvalley networks\n",
            "--------------------------------------------------\n",
            "chatgpt is great but it needs to be as fast and reliable as google. sometimes it' down , sometimes you can't log in. it also needs to be up to date like twitter. that will truly become a powerful tool. chatgpt ai\n",
            "--------------------------------------------------\n",
            "chatgpt power join pranava madhyastha on 9 march at 10 am gmt for an informal discussion on chatgpt and how you can leverage its powers for your business register here: webinar cybersecurity cybersecuritytips chatgpt business informationsecurity\n",
            "--------------------------------------------------\n",
            "hey if you are unsure how to code the functionality of an unsubscribe button, let me direct you to chatgpt for some free advice. otherwise please advise why i have seven of these dated 24 hours apart - spam is not good! paramountplus unsubscribe\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# remove hashtags, usernames, and links\n",
        "tweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"#|@\\S+|http\\S+\", \"\", x))\n",
        "\n",
        "# remove whitespace around words\n",
        "tweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: \" \".join(x.split()))\n",
        "\n",
        "# convert ampersand to 'and'\n",
        "tweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"&amp;\", \"and\", x))\n",
        "\n",
        "# convert 'artificialintelligence' (most likely combined in hashtags) to 'artificial intelligence'\n",
        "tweets_cleaned.loc[:, \"text_clean\"] = tweets_cleaned[\"text_clean\"].map(lambda x: re.sub(r\"artificialintelligence\", \"artificial intelligence\", x))\n",
        "\n",
        "# remove all rows with duplicates (high probability of spam)\n",
        "tweets_cleaned = tweets_cleaned.drop_duplicates(subset = [\"text_clean\"], keep = False)\n",
        "\n",
        "# sample 20 random tweets and print them out\n",
        "sampled_tweets_3 = tweets_cleaned[\"text_clean\"].sample(20, random_state = 1)\n",
        "for i in range(20):\n",
        "    print(\"-\" * 50)\n",
        "    print(sampled_tweets_3.iloc[i])\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDTcKE9taS9-"
      },
      "source": [
        "We check to see how many rows and columns are in our resulting dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9wCshgBpdfY",
        "outputId": "03ca3632-e81d-487b-9a50-359ffa19745e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our cleaned dataset has 374683 rows (tweets) and 16 columns.\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "dims = tweets_cleaned.shape\n",
        "print(f\"Our cleaned dataset has {dims[0]} rows (tweets) and {dims[1]} columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0lCejKo1N2f"
      },
      "source": [
        "### Number of tweets over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8j8lzkU3evR"
      },
      "source": [
        "Now that we have our cleaned dataset, we can move forward with our pipeline. But before that, let's take a look at the distribution of tweets over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "95ZwlqaF1N2o",
        "outputId": "bad3aa78-74e0-4c33-8e5d-d9aca4c5e85e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3b7ed135-e05e-457b-9792-409e97a3e6fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-12-05</td>\n",
              "      <td>2053.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-12-06</td>\n",
              "      <td>6124.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-12-07</td>\n",
              "      <td>4503.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-12-08</td>\n",
              "      <td>4655.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-12-09</td>\n",
              "      <td>4395.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>2023-06-06</td>\n",
              "      <td>968.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>2023-06-07</td>\n",
              "      <td>2074.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>2023-06-08</td>\n",
              "      <td>2156.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>2023-06-09</td>\n",
              "      <td>1018.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>2023-06-10</td>\n",
              "      <td>470.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b7ed135-e05e-457b-9792-409e97a3e6fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b7ed135-e05e-457b-9792-409e97a3e6fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b7ed135-e05e-457b-9792-409e97a3e6fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          date   count\n",
              "0   2022-12-05  2053.0\n",
              "1   2022-12-06  6124.0\n",
              "2   2022-12-07  4503.0\n",
              "3   2022-12-08  4655.0\n",
              "4   2022-12-09  4395.0\n",
              "..         ...     ...\n",
              "183 2023-06-06   968.0\n",
              "184 2023-06-07  2074.0\n",
              "185 2023-06-08  2156.0\n",
              "186 2023-06-09  1018.0\n",
              "187 2023-06-10   470.0\n",
              "\n",
              "[188 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# convert the date column to be in YYYY-MM-DD format\n",
        "tweets_cleaned[\"date\"] = pd.to_datetime(tweets_cleaned[\"date\"],\n",
        "                                        errors = \"coerce\",\n",
        "                                        utc = True).dt.date\n",
        "\n",
        "# count the number of tweets for each date (some dates are missing!)\n",
        "tweets_date_count = tweets_cleaned.value_counts(\"date\", sort = False).reset_index()\n",
        "\n",
        "# get start and end dates for the data\n",
        "start_date = min(tweets_date_count[\"date\"]).strftime(\"%Y-%m-%d\")\n",
        "end_date = max(tweets_date_count[\"date\"]).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# merge with dataframe of all possible dates\n",
        "tweets_date_count_all = pd.DataFrame(\n",
        "    pd.date_range(start = start_date, end = end_date).date\n",
        ").rename(\n",
        "    {0: \"date\"},\n",
        "    axis = 1\n",
        ").merge(\n",
        "    tweets_date_count,\n",
        "    on = \"date\",\n",
        "    how = \"left\"\n",
        ")\n",
        "\n",
        "# convert date column to be a datetime object (for plotting)\n",
        "tweets_date_count_all[\"date\"] = pd.to_datetime(tweets_date_count_all[\"date\"])\n",
        "\n",
        "# show the dataframe\n",
        "tweets_date_count_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "m8bxZRMd1N2p",
        "outputId": "222c171b-6753-44b4-fd43-df756a1fb723"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-c27f20d895ea445fbd08152cc73c2f70\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-c27f20d895ea445fbd08152cc73c2f70\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-c27f20d895ea445fbd08152cc73c2f70\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-1a1f72d7b304cc94ea38cb32985cea2e\"}, \"mark\": {\"type\": \"line\", \"color\": \"#26a7de\"}, \"encoding\": {\"x\": {\"field\": \"date\", \"title\": \"Date\", \"type\": \"temporal\"}, \"y\": {\"field\": \"count\", \"title\": \"Number of tweets\", \"type\": \"quantitative\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-1a1f72d7b304cc94ea38cb32985cea2e\": [{\"date\": \"2022-12-05T00:00:00\", \"count\": 2053.0}, {\"date\": \"2022-12-06T00:00:00\", \"count\": 6124.0}, {\"date\": \"2022-12-07T00:00:00\", \"count\": 4503.0}, {\"date\": \"2022-12-08T00:00:00\", \"count\": 4655.0}, {\"date\": \"2022-12-09T00:00:00\", \"count\": 4395.0}, {\"date\": \"2022-12-10T00:00:00\", \"count\": 3175.0}, {\"date\": \"2022-12-11T00:00:00\", \"count\": 2598.0}, {\"date\": \"2022-12-12T00:00:00\", \"count\": 2674.0}, {\"date\": \"2022-12-13T00:00:00\", \"count\": 2482.0}, {\"date\": \"2022-12-14T00:00:00\", \"count\": null}, {\"date\": \"2022-12-15T00:00:00\", \"count\": null}, {\"date\": \"2022-12-16T00:00:00\", \"count\": null}, {\"date\": \"2022-12-17T00:00:00\", \"count\": null}, {\"date\": \"2022-12-18T00:00:00\", \"count\": null}, {\"date\": \"2022-12-19T00:00:00\", \"count\": 1591.0}, {\"date\": \"2022-12-20T00:00:00\", \"count\": 1689.0}, {\"date\": \"2022-12-21T00:00:00\", \"count\": 1774.0}, {\"date\": \"2022-12-22T00:00:00\", \"count\": 1706.0}, {\"date\": \"2022-12-23T00:00:00\", \"count\": 1671.0}, {\"date\": \"2022-12-24T00:00:00\", \"count\": 1080.0}, {\"date\": \"2022-12-25T00:00:00\", \"count\": 966.0}, {\"date\": \"2022-12-26T00:00:00\", \"count\": 1246.0}, {\"date\": \"2022-12-27T00:00:00\", \"count\": 1502.0}, {\"date\": \"2022-12-28T00:00:00\", \"count\": 1486.0}, {\"date\": \"2022-12-29T00:00:00\", \"count\": 1431.0}, {\"date\": \"2022-12-30T00:00:00\", \"count\": 1397.0}, {\"date\": \"2022-12-31T00:00:00\", \"count\": 1052.0}, {\"date\": \"2023-01-01T00:00:00\", \"count\": 917.0}, {\"date\": \"2023-01-02T00:00:00\", \"count\": 1304.0}, {\"date\": \"2023-01-03T00:00:00\", \"count\": 1799.0}, {\"date\": \"2023-01-04T00:00:00\", \"count\": 2368.0}, {\"date\": \"2023-01-05T00:00:00\", \"count\": 2750.0}, {\"date\": \"2023-01-06T00:00:00\", \"count\": 2174.0}, {\"date\": \"2023-01-07T00:00:00\", \"count\": null}, {\"date\": \"2023-01-08T00:00:00\", \"count\": null}, {\"date\": \"2023-01-09T00:00:00\", \"count\": null}, {\"date\": \"2023-01-10T00:00:00\", \"count\": null}, {\"date\": \"2023-01-11T00:00:00\", \"count\": null}, {\"date\": \"2023-01-12T00:00:00\", \"count\": null}, {\"date\": \"2023-01-13T00:00:00\", \"count\": null}, {\"date\": \"2023-01-14T00:00:00\", \"count\": null}, {\"date\": \"2023-01-15T00:00:00\", \"count\": null}, {\"date\": \"2023-01-16T00:00:00\", \"count\": null}, {\"date\": \"2023-01-17T00:00:00\", \"count\": null}, {\"date\": \"2023-01-18T00:00:00\", \"count\": null}, {\"date\": \"2023-01-19T00:00:00\", \"count\": null}, {\"date\": \"2023-01-20T00:00:00\", \"count\": null}, {\"date\": \"2023-01-21T00:00:00\", \"count\": null}, {\"date\": \"2023-01-22T00:00:00\", \"count\": null}, {\"date\": \"2023-01-23T00:00:00\", \"count\": null}, {\"date\": \"2023-01-24T00:00:00\", \"count\": null}, {\"date\": \"2023-01-25T00:00:00\", \"count\": 3557.0}, {\"date\": \"2023-01-26T00:00:00\", \"count\": 2759.0}, {\"date\": \"2023-01-27T00:00:00\", \"count\": 3493.0}, {\"date\": \"2023-01-28T00:00:00\", \"count\": 2647.0}, {\"date\": \"2023-01-29T00:00:00\", \"count\": 2433.0}, {\"date\": \"2023-01-30T00:00:00\", \"count\": 3184.0}, {\"date\": \"2023-01-31T00:00:00\", \"count\": 3777.0}, {\"date\": \"2023-02-01T00:00:00\", \"count\": 3691.0}, {\"date\": \"2023-02-02T00:00:00\", \"count\": 4264.0}, {\"date\": \"2023-02-03T00:00:00\", \"count\": 4057.0}, {\"date\": \"2023-02-04T00:00:00\", \"count\": 2810.0}, {\"date\": \"2023-02-05T00:00:00\", \"count\": 2563.0}, {\"date\": \"2023-02-06T00:00:00\", \"count\": 4259.0}, {\"date\": \"2023-02-07T00:00:00\", \"count\": 6870.0}, {\"date\": \"2023-02-08T00:00:00\", \"count\": 5560.0}, {\"date\": \"2023-02-09T00:00:00\", \"count\": 4549.0}, {\"date\": \"2023-02-10T00:00:00\", \"count\": 4214.0}, {\"date\": \"2023-02-11T00:00:00\", \"count\": 3121.0}, {\"date\": \"2023-02-12T00:00:00\", \"count\": 2983.0}, {\"date\": \"2023-02-13T00:00:00\", \"count\": 3783.0}, {\"date\": \"2023-02-14T00:00:00\", \"count\": 3689.0}, {\"date\": \"2023-02-15T00:00:00\", \"count\": 3522.0}, {\"date\": \"2023-02-16T00:00:00\", \"count\": 3432.0}, {\"date\": \"2023-02-17T00:00:00\", \"count\": 2489.0}, {\"date\": \"2023-02-18T00:00:00\", \"count\": 484.0}, {\"date\": \"2023-02-19T00:00:00\", \"count\": 13.0}, {\"date\": \"2023-02-20T00:00:00\", \"count\": 26.0}, {\"date\": \"2023-02-21T00:00:00\", \"count\": 48.0}, {\"date\": \"2023-02-22T00:00:00\", \"count\": 2886.0}, {\"date\": \"2023-02-23T00:00:00\", \"count\": 3324.0}, {\"date\": \"2023-02-24T00:00:00\", \"count\": 3140.0}, {\"date\": \"2023-02-25T00:00:00\", \"count\": 2886.0}, {\"date\": \"2023-02-26T00:00:00\", \"count\": 2565.0}, {\"date\": \"2023-02-27T00:00:00\", \"count\": 3556.0}, {\"date\": \"2023-02-28T00:00:00\", \"count\": 3467.0}, {\"date\": \"2023-03-01T00:00:00\", \"count\": 3328.0}, {\"date\": \"2023-03-02T00:00:00\", \"count\": 3549.0}, {\"date\": \"2023-03-03T00:00:00\", \"count\": 1673.0}, {\"date\": \"2023-03-04T00:00:00\", \"count\": null}, {\"date\": \"2023-03-05T00:00:00\", \"count\": 2409.0}, {\"date\": \"2023-03-06T00:00:00\", \"count\": 1435.0}, {\"date\": \"2023-03-07T00:00:00\", \"count\": 548.0}, {\"date\": \"2023-03-08T00:00:00\", \"count\": 1282.0}, {\"date\": \"2023-03-09T00:00:00\", \"count\": 469.0}, {\"date\": \"2023-03-10T00:00:00\", \"count\": 1427.0}, {\"date\": \"2023-03-11T00:00:00\", \"count\": 1875.0}, {\"date\": \"2023-03-12T00:00:00\", \"count\": 1703.0}, {\"date\": \"2023-03-13T00:00:00\", \"count\": 1101.0}, {\"date\": \"2023-03-14T00:00:00\", \"count\": 912.0}, {\"date\": \"2023-03-15T00:00:00\", \"count\": 3755.0}, {\"date\": \"2023-03-16T00:00:00\", \"count\": 4042.0}, {\"date\": \"2023-03-17T00:00:00\", \"count\": 2489.0}, {\"date\": \"2023-03-18T00:00:00\", \"count\": 1223.0}, {\"date\": \"2023-03-19T00:00:00\", \"count\": null}, {\"date\": \"2023-03-20T00:00:00\", \"count\": null}, {\"date\": \"2023-03-21T00:00:00\", \"count\": null}, {\"date\": \"2023-03-22T00:00:00\", \"count\": null}, {\"date\": \"2023-03-23T00:00:00\", \"count\": null}, {\"date\": \"2023-03-24T00:00:00\", \"count\": 4506.0}, {\"date\": \"2023-03-25T00:00:00\", \"count\": 2861.0}, {\"date\": \"2023-03-26T00:00:00\", \"count\": 2500.0}, {\"date\": \"2023-03-27T00:00:00\", \"count\": 1845.0}, {\"date\": \"2023-03-28T00:00:00\", \"count\": 958.0}, {\"date\": \"2023-03-29T00:00:00\", \"count\": 14.0}, {\"date\": \"2023-03-30T00:00:00\", \"count\": 246.0}, {\"date\": \"2023-03-31T00:00:00\", \"count\": 1572.0}, {\"date\": \"2023-04-01T00:00:00\", \"count\": 1668.0}, {\"date\": \"2023-04-02T00:00:00\", \"count\": 1022.0}, {\"date\": \"2023-04-03T00:00:00\", \"count\": 3050.0}, {\"date\": \"2023-04-04T00:00:00\", \"count\": 3203.0}, {\"date\": \"2023-04-05T00:00:00\", \"count\": 3142.0}, {\"date\": \"2023-04-06T00:00:00\", \"count\": 3110.0}, {\"date\": \"2023-04-07T00:00:00\", \"count\": 2671.0}, {\"date\": \"2023-04-08T00:00:00\", \"count\": 1850.0}, {\"date\": \"2023-04-09T00:00:00\", \"count\": 1733.0}, {\"date\": \"2023-04-10T00:00:00\", \"count\": 2718.0}, {\"date\": \"2023-04-11T00:00:00\", \"count\": 3681.0}, {\"date\": \"2023-04-12T00:00:00\", \"count\": 3207.0}, {\"date\": \"2023-04-13T00:00:00\", \"count\": 3233.0}, {\"date\": \"2023-04-14T00:00:00\", \"count\": 2935.0}, {\"date\": \"2023-04-15T00:00:00\", \"count\": 2180.0}, {\"date\": \"2023-04-16T00:00:00\", \"count\": 2007.0}, {\"date\": \"2023-04-17T00:00:00\", \"count\": 1206.0}, {\"date\": \"2023-04-18T00:00:00\", \"count\": 755.0}, {\"date\": \"2023-04-19T00:00:00\", \"count\": 3066.0}, {\"date\": \"2023-04-20T00:00:00\", \"count\": 2827.0}, {\"date\": \"2023-04-21T00:00:00\", \"count\": 2563.0}, {\"date\": \"2023-04-22T00:00:00\", \"count\": 1721.0}, {\"date\": \"2023-04-23T00:00:00\", \"count\": 1514.0}, {\"date\": \"2023-04-24T00:00:00\", \"count\": 2641.0}, {\"date\": \"2023-04-25T00:00:00\", \"count\": 2848.0}, {\"date\": \"2023-04-26T00:00:00\", \"count\": 2920.0}, {\"date\": \"2023-04-27T00:00:00\", \"count\": 2829.0}, {\"date\": \"2023-04-28T00:00:00\", \"count\": 2788.0}, {\"date\": \"2023-04-29T00:00:00\", \"count\": 1982.0}, {\"date\": \"2023-04-30T00:00:00\", \"count\": 1757.0}, {\"date\": \"2023-05-01T00:00:00\", \"count\": 1544.0}, {\"date\": \"2023-05-02T00:00:00\", \"count\": 3228.0}, {\"date\": \"2023-05-03T00:00:00\", \"count\": 3335.0}, {\"date\": \"2023-05-04T00:00:00\", \"count\": 1849.0}, {\"date\": \"2023-05-05T00:00:00\", \"count\": 2387.0}, {\"date\": \"2023-05-06T00:00:00\", \"count\": 1838.0}, {\"date\": \"2023-05-07T00:00:00\", \"count\": 1195.0}, {\"date\": \"2023-05-08T00:00:00\", \"count\": 1595.0}, {\"date\": \"2023-05-09T00:00:00\", \"count\": 1830.0}, {\"date\": \"2023-05-10T00:00:00\", \"count\": 3358.0}, {\"date\": \"2023-05-11T00:00:00\", \"count\": 3523.0}, {\"date\": \"2023-05-12T00:00:00\", \"count\": 3329.0}, {\"date\": \"2023-05-13T00:00:00\", \"count\": 2135.0}, {\"date\": \"2023-05-14T00:00:00\", \"count\": 1138.0}, {\"date\": \"2023-05-15T00:00:00\", \"count\": 2141.0}, {\"date\": \"2023-05-16T00:00:00\", \"count\": 3367.0}, {\"date\": \"2023-05-17T00:00:00\", \"count\": 3213.0}, {\"date\": \"2023-05-18T00:00:00\", \"count\": 3306.0}, {\"date\": \"2023-05-19T00:00:00\", \"count\": 3415.0}, {\"date\": \"2023-05-20T00:00:00\", \"count\": 1890.0}, {\"date\": \"2023-05-21T00:00:00\", \"count\": 1626.0}, {\"date\": \"2023-05-22T00:00:00\", \"count\": 2309.0}, {\"date\": \"2023-05-23T00:00:00\", \"count\": 2515.0}, {\"date\": \"2023-05-24T00:00:00\", \"count\": 1347.0}, {\"date\": \"2023-05-25T00:00:00\", \"count\": 1440.0}, {\"date\": \"2023-05-26T00:00:00\", \"count\": 1269.0}, {\"date\": \"2023-05-27T00:00:00\", \"count\": 601.0}, {\"date\": \"2023-05-28T00:00:00\", \"count\": 1067.0}, {\"date\": \"2023-05-29T00:00:00\", \"count\": 1970.0}, {\"date\": \"2023-05-30T00:00:00\", \"count\": 2451.0}, {\"date\": \"2023-05-31T00:00:00\", \"count\": 1075.0}, {\"date\": \"2023-06-01T00:00:00\", \"count\": null}, {\"date\": \"2023-06-02T00:00:00\", \"count\": 2215.0}, {\"date\": \"2023-06-03T00:00:00\", \"count\": 1571.0}, {\"date\": \"2023-06-04T00:00:00\", \"count\": 1511.0}, {\"date\": \"2023-06-05T00:00:00\", \"count\": 2180.0}, {\"date\": \"2023-06-06T00:00:00\", \"count\": 968.0}, {\"date\": \"2023-06-07T00:00:00\", \"count\": 2074.0}, {\"date\": \"2023-06-08T00:00:00\", \"count\": 2156.0}, {\"date\": \"2023-06-09T00:00:00\", \"count\": 1018.0}, {\"date\": \"2023-06-10T00:00:00\", \"count\": 470.0}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# create a line plot of number of tweets vs. date\n",
        "line = alt.Chart(tweets_date_count_all).mark_line(\n",
        "    color = \"#26a7de\"\n",
        ").encode(\n",
        "    x = alt.X(\"date:T\", title = \"Date\"),\n",
        "    y = alt.Y(\"count\", title = \"Number of tweets\")\n",
        ")\n",
        "\n",
        "# make the plot interactive\n",
        "line.interactive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrDostAZ1N2p"
      },
      "source": [
        "Looking at the line plot, it appears that the number of tweets isn't very consistent â€“ the counts fluctuate a lot. Not only are there are large dips (near 0) during February and April 2023, but there also seems to be a lot of missing dates, especially in January. We can confirm this by getting the dates where there are no tweets in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZdS4UR91N2p",
        "outputId": "e120dd30-ed6a-445b-a9f5-45f521ee1342"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    2022-12-14\n",
              "1    2022-12-15\n",
              "2    2022-12-16\n",
              "3    2022-12-17\n",
              "4    2022-12-18\n",
              "5    2023-01-07\n",
              "6    2023-01-08\n",
              "7    2023-01-09\n",
              "8    2023-01-10\n",
              "9    2023-01-11\n",
              "10   2023-01-12\n",
              "11   2023-01-13\n",
              "12   2023-01-14\n",
              "13   2023-01-15\n",
              "14   2023-01-16\n",
              "15   2023-01-17\n",
              "16   2023-01-18\n",
              "17   2023-01-19\n",
              "18   2023-01-20\n",
              "19   2023-01-21\n",
              "20   2023-01-22\n",
              "21   2023-01-23\n",
              "22   2023-01-24\n",
              "23   2023-03-04\n",
              "24   2023-03-19\n",
              "25   2023-03-20\n",
              "26   2023-03-21\n",
              "27   2023-03-22\n",
              "28   2023-03-23\n",
              "29   2023-06-01\n",
              "Name: date, dtype: datetime64[ns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# get all dates where tweet count is missing\n",
        "counts = tweets_date_count_all[\"count\"]\n",
        "tweets_date_count_all[counts.isna()][\"date\"].reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDPgFKsN33Qp"
      },
      "source": [
        "It is highly unlikely that there were no tweets about LLMs on the dates above, so the missing tweets may be an issue with the data collection itself. This means we have less data for January 2023 compared to other months, as shown by the bar chart below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "zQnh-9Bo1N2p",
        "outputId": "c5d8b1bb-2a39-4dcb-853d-2b0f3b1dce56"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-95ffffd1-5279-454a-8128-4fa07af9bc46\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-12</td>\n",
              "      <td>51250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-01</td>\n",
              "      <td>33162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-02</td>\n",
              "      <td>88251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-03</td>\n",
              "      <td>47722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-04</td>\n",
              "      <td>72827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2023-05</td>\n",
              "      <td>67286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023-06</td>\n",
              "      <td>14163</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95ffffd1-5279-454a-8128-4fa07af9bc46')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95ffffd1-5279-454a-8128-4fa07af9bc46 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95ffffd1-5279-454a-8128-4fa07af9bc46');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     month  count\n",
              "0  2022-12  51250\n",
              "1  2023-01  33162\n",
              "2  2023-02  88251\n",
              "3  2023-03  47722\n",
              "4  2023-04  72827\n",
              "5  2023-05  67286\n",
              "6  2023-06  14163"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# add month column\n",
        "tweets_cleaned[\"month\"] = pd.to_datetime(tweets_cleaned[\"date\"]).dt.to_period(\"M\").dt.strftime(\"%Y-%m\")\n",
        "\n",
        "# get tweet counts per month\n",
        "tweets_by_month = tweets_cleaned.value_counts(\n",
        "    \"month\"\n",
        ").reset_index(\n",
        ").sort_values(\n",
        "    \"month\"\n",
        ").reset_index(\n",
        "    drop = True\n",
        ")\n",
        "\n",
        "# show the dataframe\n",
        "tweets_by_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "Z2lg8ZqDULwC",
        "outputId": "d358edc5-d996-4fb6-e644-bd002aebc724"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-9b1f38402d8648ac9f42eb361fdc4717\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-9b1f38402d8648ac9f42eb361fdc4717\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-9b1f38402d8648ac9f42eb361fdc4717\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-032f38611235c913b117906bf80e383e\"}, \"mark\": {\"type\": \"bar\", \"color\": \"#26a7de\"}, \"encoding\": {\"x\": {\"field\": \"month\", \"title\": \"Month\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"sum\", \"field\": \"count\", \"title\": \"Number of tweets\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-032f38611235c913b117906bf80e383e\": [{\"month\": \"2022-12\", \"count\": 51250}, {\"month\": \"2023-01\", \"count\": 33162}, {\"month\": \"2023-02\", \"count\": 88251}, {\"month\": \"2023-03\", \"count\": 47722}, {\"month\": \"2023-04\", \"count\": 72827}, {\"month\": \"2023-05\", \"count\": 67286}, {\"month\": \"2023-06\", \"count\": 14163}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# create bar chart of tweet counts per month\n",
        "alt.Chart(tweets_by_month).mark_bar(\n",
        "    color = \"#26a7de\"\n",
        ").encode(\n",
        "    x = alt.X(\"month:O\", title = \"Month\"),\n",
        "    y = alt.Y(\"sum(count)\", title = \"Number of tweets\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqj--Ueq7yYy"
      },
      "source": [
        "This shouldn't affect our analysis too much as we still have tens of thousands of tweets for most of the months (with the exception of June 2023 since the current dataset was downloaded during the middle of the month)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXpgSfA-i51G"
      },
      "source": [
        "### Topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFiK4XzjqTAy"
      },
      "source": [
        "The next step is tokenization, which involves breaking up the text into units called tokens. Since we are extracting topics from tweets, we ideally want to keep words that have some sort of meaning. This means  we should remove tokens that are either stopwords (words that don't contribute much to the meaning of a sentence, e.g., *a*, *the*, *I*) or punctuation marks. The remaining tokens are lemmatized (e.g., the lemmatized forms of *asked* and *asks* are both *ask*) so that we can find similar words between tweets. To automate this process, we utilize a popular Python library in natural language processing called [spaCy](https://spacy.io/).\n",
        "\n",
        "**Note:** The code takes around 30 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So5B-0pNi51H",
        "outputId": "3ab26104-5248-463a-9ab5-97f337714bfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned text:\n",
            "top 10 chatgpt plugins you should use right now read more:- chatgpt bestchatgptplugins aichatbot topchatgptplugins thetechtrend\n",
            "\n",
            "Tokenized text:\n",
            "['10', 'chatgpt', 'plugin', 'use', 'right', 'read', 'more:-', 'chatgpt', 'bestchatgptplugin', 'aichatbot', 'topchatgptplugin', 'thetechtrend']\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "def tokenize(doc):\n",
        "    \"\"\"Takes in a spaCy Doc object (containing tokens) and returns a\n",
        "    list of the tokens that are not stopwords or punctuation marks.\n",
        "    \"\"\"\n",
        "    # initialize list of tokens to keep\n",
        "    tokens = []\n",
        "\n",
        "    # add the lemamtized form of a word if it isn't a stopword or punctuation mark\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            lemma = token.lemma_\n",
        "            tokens.append(lemma)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# tokenize every tweet (will take around 30 minutes to run)\n",
        "docs = list(nlp.pipe(tweets_cleaned[\"text_clean\"]))\n",
        "\n",
        "# keep only the meaningful tokens\n",
        "tokens_list = [tokenize(doc) for doc in docs]\n",
        "\n",
        "# show example\n",
        "print(f\"Cleaned text:\\n{docs[0]}\")\n",
        "print(f\"\\nTokenized text:\\n{tokens_list[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJNQmMRGqGEr"
      },
      "source": [
        "One thing that we need to take into account is that some pairs of words can frequently occur together, so they should be treated as one \"word\" (e.g., *artificial intelligence*) â€“ these are called bigrams. We train a bigram model on our tweets and get back the same tokens, with the only difference being that the bigrams contain an underscore (e.g., the bigram `\"artificial intelligence\"` would show up as `\"artificial_intelligence\"`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UhOlTlmi51H",
        "outputId": "9a5187f1-a013-4f32-e9af-5f9e755cae85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['10',\n",
              " 'chatgpt',\n",
              " 'plugin',\n",
              " 'use',\n",
              " 'right',\n",
              " 'read',\n",
              " 'more:-',\n",
              " 'chatgpt',\n",
              " 'bestchatgptplugin',\n",
              " 'aichatbot',\n",
              " 'topchatgptplugin',\n",
              " 'thetechtrend']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# create a bigram model\n",
        "#   min_count: words that appear together at least this many times will be considered bigrams\n",
        "#   threshold: higher value = less likely to form bigrams\n",
        "bigram_model = gensim.models.phrases.Phrases(tokens_list, min_count = 25, threshold = 100)\n",
        "bigram_phraser = gensim.models.phrases.Phraser(bigram_model)\n",
        "\n",
        "# run the bigram model over all of the tweets\n",
        "texts = [bigram_phraser[sentence] for sentence in tokens_list]\n",
        "\n",
        "# show example\n",
        "texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F-AtVLFqHUm"
      },
      "source": [
        "Next, we create a dictionary and corpus that our model will take as input.\n",
        "\n",
        "- The dictionary (`id2word`) maps each word to an index.\n",
        "- The corpus (`corpus`) contains the term frequency of each word within each doc. The mapping is stored in a tuple, which can be read as (word index, word frequency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncFJy7oui51H",
        "outputId": "5dc3f5e8-d15d-4281-d039-7e95dc1b10fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 words and indices in the dictionary: [('10', 0), ('aichatbot', 1), ('bestchatgptplugin', 2), ('chatgpt', 3), ('more:-', 4)]\n",
            "First document in the corpus: [(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# create dictionary\n",
        "id2word = gensim.corpora.Dictionary(texts)\n",
        "\n",
        "# create corpus (with term frequency)\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# show example\n",
        "print(f\"First 5 words and indices in the dictionary: {[(id2word[i], i) for i in range(5)]}\")\n",
        "print(f\"First document in the corpus: {corpus[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTyOD1ALi51H"
      },
      "source": [
        "Finally, we fit our model to get the possible groupings of our tweets. The method we are using is called Latent Dirichlet Allocation or LDA for short (see [this article](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2) by Ria Kulshrestha for a more detailed explanation). In addition to taking the dictionary and corpus above as inputs, we also need to specify how many topics we want to group our texts into.\n",
        "\n",
        "However, we don't necessarily know how many groups would be \"best\" for our data. One solution is to use the CV coherence score, which allows us to quantify how interpretable the topics are. The basic idea is that it takes the most frequent words from each topic and measures how similar they are. A higher coherence score means the top words in each topic are more related to each other.\n",
        "\n",
        "The code below fits an LDA model for $k = 1, 2, ..., 10$ topics, calculating the CV coherence score each time. We choose the number of topics that returns the highest coherence score.\n",
        "\n",
        "**Note:** The code takes around 30 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYbEolt3i51H",
        "outputId": "6c645164-ed2b-4c2b-e6fa-e1d2f7cf9e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence score for 1 topic(s):  0.33048730772552914\n",
            "Coherence score for 2 topic(s):  0.3320327290592944\n",
            "Coherence score for 3 topic(s):  0.4115078807185606\n",
            "Coherence score for 4 topic(s):  0.36942085462608226\n",
            "Coherence score for 5 topic(s):  0.4235322851378503\n",
            "Coherence score for 6 topic(s):  0.35399183349686414\n",
            "Coherence score for 7 topic(s):  0.3666506074753511\n",
            "Coherence score for 8 topic(s):  0.3873614361334937\n",
            "Coherence score for 9 topic(s):  0.38842431325228616\n",
            "Coherence score for 10 topic(s):  0.38441892727108595\n",
            "\n",
            "The highest coherence score (0.4235322851378503) occurs when there are 5 topics.\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "def get_best_num_topics(corpus, id2word, texts, min_topics = 1, max_topics = 10, seed = 1):\n",
        "    \"\"\"Runs a LDA model for each number of topics between min_topics and max_topics, returning\n",
        "    the number of topics that achieves the highest coherence score.\n",
        "    \"\"\"\n",
        "    # initialize list of scores\n",
        "    scores_list = []\n",
        "\n",
        "    # for each number of topics\n",
        "    for i in range(min_topics, max_topics + 1):\n",
        "        # run LDA model\n",
        "        lda_model = gensim.models.LdaModel(corpus = corpus,\n",
        "                                           id2word = id2word,\n",
        "                                           num_topics = i,\n",
        "                                           random_state = seed)\n",
        "\n",
        "        # run coherence score model\n",
        "        coherence_model = gensim.models.CoherenceModel(model = lda_model,\n",
        "                                                       texts = texts,\n",
        "                                                       dictionary = id2word,\n",
        "                                                       coherence = \"c_v\")\n",
        "\n",
        "        # print coherence score\n",
        "        coherence_lda = coherence_model.get_coherence()\n",
        "        print(f\"Coherence score for {i} topic(s): \", coherence_lda)\n",
        "\n",
        "        # append score to list of scores\n",
        "        scores_list.append((i, coherence_lda))\n",
        "\n",
        "    # get the best number of topics based on the highest coherence score\n",
        "    best_num_topics, best_score = max(scores_list, key = itemgetter(1))\n",
        "    print(f\"\\nThe highest coherence score ({best_score}) occurs when there are {best_num_topics} topics.\")\n",
        "\n",
        "    return best_num_topics\n",
        "\n",
        "# save the best number of topics in a variable (takes around 30 minutes to run)\n",
        "seed = 1\n",
        "best_num_topics = get_best_num_topics(corpus, id2word, texts, seed = seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owrK5Gy5fixa"
      },
      "source": [
        "According to the output above, the LDA model achieves the highest coherence score with 5 topics. We re-run this model to get an interactive visualization, allowing us to see the most frequent terms overall as well as in each of the topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "yqe-_TRSi51I",
        "outputId": "e6abebbf-fcf3-4e82-d590-620a6c76fbce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el7401405697923322886475224544\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el7401405697923322886475224544_data = {\"mdsDat\": {\"x\": [-0.11224410414278593, -0.10336008621765144, -0.11435229827466027, -0.07178725007929987, 0.40174373871439717], \"y\": [0.14384671858512996, -0.09826796959971405, 0.14572297017660263, -0.21013827402181864, 0.01883655485980007], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [31.49417768351752, 27.396781183276737, 17.609741878001312, 16.27392444332377, 7.225374811880661]}, \"tinfo\": {\"Term\": [\"ai\", \"google\", \"write\", \"intelligence\", \"ask\", \"artificial\", \"openai\", \"model\", \"chatgpt\", \"language\", \"answer\", \"search\", \"future\", \"code\", \"gpt\", \"try\", \"chatbot\", \"question\", \"chat\", \"technology\", \"art\", \"potential\", \"datum\", \"train\", \"ok\", \"midjourney\", \"tech\", \"user\", \"good\", \"chatgpt3\", \"\\ud83d\\ude02\", \"fun\", \"\\ud83e\\udd23\", \"script\", \"\\ud83d\\ude05\", \"lol\", \"writer\", \"email\", \"finally\", \"tweet\", \"joke\", \"programming\", \"christmas\", \"movie\", \"funny\", \"\\ud83d\\udc40\", \"kid\", \"friend\", \"shit\", \"write\", \"sorry\", \"programmer\", \"poetry\", \"wish\", \"artist\", \"sound\", \"\\ud83d\\ude0e\", \"suggestion\", \"incorrect\", \"cheat\", \"ask\", \"thread\", \"guess\", \"essay\", \"hey\", \"guy\", \"minute\", \"book\", \"have\", \"code\", \"idea\", \"feel\", \"pretty\", \"try\", \"want\", \"tell\", \"\\ud83e\\udd2f\", \"get\", \"thing\", \"good\", \"prompt\", \"love\", \"second\", \"day\", \"chatgpt\", \"question\", \"know\", \"twitter\", \"time\", \"think\", \"play\", \"like\", \"people\", \"answer\", \"help\", \"create\", \"use\", \"work\", \"need\", \"ai\", \"go\", \"generate\", \"well\", \"potential\", \"education\", \"impact\", \"scary\", \"awesome\", \"stackoverflow\", \"industry\", \"podcast\", \"opportunity\", \"god\", \"implication\", \"ban\", \"society\", \"instruction\", \"stack_overflow\", \"yesterday\", \"engineering\", \"cybersecurity\", \"security\", \"benefit\", \"policy\", \"sort\", \"obsolete\", \"assessment\", \"revolutionize\", \"educator\", \"raise\", \"holiday\", \"transform\", \"avoid\", \"teacher\", \"learning\", \"discussion\", \"discuss\", \"threat\", \"join\", \"insight\", \"explore\", \"concern\", \"business\", \"student\", \"future\", \"customer\", \"natural_language\", \"digital\", \"development\", \"automation\", \"technology\", \"interview\", \"learn\", \"tool\", \"ai\", \"school\", \"change\", \"tech\", \"chatgpt\", \"late\", \"world\", \"way\", \"job\", \"new\", \"use\", \"system\", \"work\", \"openai\", \"human\", \"create\", \"like\", \"help\", \"read\", \"great\", \"need\", \"look\", \"google\", \"search\", \"search_engine\", \"error\", \"openaichat\", \"crazy\", \"accurate\", \"impressed\", \"context\", \"microsoft\", \"mind_blow\", \"capable\", \"absolutely\", \"insane\", \"bing\", \"\\ud83d\\ude09\", \"okay\", \"exactly\", \"large\", \"n\", \"random\", \"damn\", \"argument\", \"gptchat\", \"\\ud83d\\ude01\", \"slow\", \"7\", \"2021\", \"fan\", \"recipe\", \"model\", \"language\", \"vs\", \"compare\", \"relevant\", \"sentence\", \"stuff\", \"blow_away\", \"description\", \"function\", \"train\", \"seo\", \"information\", \"user\", \"release\", \"datum\", \"llm\", \"query\", \"answer\", \"version\", \"chatgpt\", \"response\", \"access\", \"provide\", \"base\", \"text\", \"internet\", \"openai\", \"ai\", \"like\", \"question\", \"new\", \"result\", \"chatbot\", \"generate\", \"gpt\", \"human\", \"time\", \"exam\", \"reference\", \"blow\", \"couple\", \"interact\", \"agi\", \"wild\", \"fake\", \"apparently\", \"feeling\", \"plagiarism\", \"disrupt\", \"screenshot\", \"rap\", \"ethic\", \"career\", \"sell\", \"viral\", \"elon\", \"professor\", \"artificial\", \"metaverse\", \"tiktok\", \"lawyer\", \"wikipedia\", \"dumb\", \"intelligence\", \"invent\", \"decade\", \"medical\", \"openaichatgpt\", \"science\", \"prediction\", \"gpt-3\", \"argue\", \"elon_musk\", \"ethical\", \"chatgpt3\", \"detect\", \"pass\", \"gpt\", \"openai\", \"ai\", \"chat\", \"chatbot\", \"chatgpt\", \"bot\", \"machinelearning\", \"human\", \"impressive\", \"gpt3\", \"tech\", \"machine\", \"generate\", \"machinelearne\", \"new\", \"technology\", \"know\", \"say\", \"future\", \"tool\", \"test\", \"world\", \"think\", \"text\", \"ok\", \"probably\", \"aiart\", \"dalle2\", \"nice\", \"go_to\", \"kill\", \"stablediffusion\", \"rate\", \"light\", \"scene\", \"night\", \"haiku\", \"earth\", \"domain\", \"fight\", \"lyric\", \"beautiful\", \"red\", \"eye\", \"destroy\", \"`\", \"dall_e\", \"dalle\", \"alien\", \"forever\", \"skynet\", \"dark\", \"travel\", \"\\ud83d\\ude2e\", \"midjourney\", \"fire\", \"family\", \"art\", \"home\", \"war\", \"elonmusk\", \"notice\", \"\\ufe0f\", \"music\", \"image\", \"live\", \"send\", \"chatgpt\", \"song\", \"world\", \"story\", \"poem\"], \"Freq\": [146847.0, 24001.0, 35885.0, 21935.0, 37210.0, 20371.0, 54075.0, 14595.0, 506703.0, 11602.0, 23060.0, 9497.0, 18749.0, 16692.0, 11748.0, 16918.0, 16337.0, 20625.0, 12916.0, 15998.0, 4173.0, 8155.0, 9171.0, 6362.0, 3585.0, 3498.0, 11220.0, 8335.0, 20278.0, 5338.0, 7405.276858025561, 5170.381097074238, 3131.086744532637, 3111.363485224372, 2505.0266911883054, 2364.572050277769, 2321.726459013595, 2350.5820387531376, 2259.498691855626, 5485.389079770901, 1936.6133983064205, 3061.0674676071667, 1836.5693331477148, 1710.218269692348, 1622.534766688188, 1733.4233177504655, 1446.8308227110806, 3219.001042814988, 1218.5529869174788, 35849.13247494375, 1177.7049220376487, 1347.4244813473763, 1077.9921069106322, 1060.9896096496132, 1146.1291586851414, 2325.008109502159, 952.2008714337345, 929.6426093076903, 916.3398741148152, 877.708465806182, 36538.481143917736, 4590.565514830297, 2678.2854970531494, 3638.90422617519, 2064.3205045283016, 1896.5926872601053, 1731.1809208537557, 2741.1441432004594, 3885.433302437166, 15399.9844185397, 5309.637993399877, 5262.556527894619, 4909.183698687234, 14579.672058340786, 6874.301952747374, 7082.104082638609, 4443.204368602465, 10466.408146215084, 10218.132584046798, 16576.872976995175, 8473.597929377709, 5626.0176365886455, 3211.100594591198, 8236.312618551852, 194553.66494511082, 14013.847644490037, 13004.74975982798, 4878.380145100103, 11608.668543616264, 13312.052612276464, 6146.854131409154, 17172.675711525237, 8650.254072587353, 12482.579395555069, 8529.933572136148, 9401.101874336448, 11210.748176678046, 9429.781417243084, 7784.490660310403, 18309.99473767372, 7179.867959860901, 7097.74880183027, 6148.614499738213, 8153.739907161952, 6405.600992454325, 4645.224406930033, 4308.63529012811, 3956.010165110406, 3476.8272423435515, 4210.305885219683, 2155.3981400532402, 2208.6968009052275, 2079.386944512402, 1986.7396148521339, 1881.161400188565, 1792.6877801388055, 1781.1356080835185, 1788.55432492094, 1764.6098380905846, 2074.981436690926, 1660.7186179320186, 1649.6630844459614, 2171.6407874845872, 1506.3901533318203, 1560.10628578084, 1340.0701567533033, 1279.4220501645673, 1796.482595278564, 1259.983539410659, 1271.8816476527197, 1318.8296748854061, 1512.7062352641026, 1188.631622091582, 4055.7743036632746, 3289.186422838488, 2681.496886538667, 2966.397518194313, 2091.8587617129397, 2739.625403601239, 1757.7533409370735, 3523.56661764104, 2105.7275740648165, 7246.950002201757, 7230.496019209037, 15598.62008708153, 2882.3748598843886, 2689.450986798237, 2664.8907754870834, 3536.570463731006, 2051.762535287956, 11949.278955081512, 2769.190242277701, 9095.029168430065, 11515.260119046727, 60543.34184825696, 3454.4712944828607, 7757.16973260271, 7435.943033846345, 119398.21051348791, 3938.4367745489444, 8590.033787287595, 8426.575458709387, 6053.13487228756, 10308.784982646388, 9926.475597587381, 4181.982559102977, 6864.690935133554, 10023.769793698346, 6110.313553410399, 5687.431766006774, 6926.631039999486, 5042.5235214493805, 4507.262539745194, 4433.890187368723, 4456.391051042928, 4264.814693750493, 24000.851837396014, 9496.101221714745, 3613.9605908150606, 3425.197178015525, 3100.484960354048, 2856.335260958969, 2722.758903508231, 2861.5185076478656, 2601.3291835620607, 2929.4218638712573, 2193.5483169188883, 2047.303094604686, 1965.3024027281158, 1931.8917107615357, 1750.8817139812206, 1742.221299867369, 1750.286715468886, 1656.6770301502613, 3384.7482757066277, 1546.5057084002478, 1489.3199798461285, 1337.7040307899495, 1274.9696904521154, 1289.6445595408031, 1187.9316176208733, 1126.1473990241525, 1154.3952202247985, 1086.906282244098, 1115.4706310099214, 1100.5066178715722, 14545.938872909099, 11585.373758403282, 2412.6688362776176, 1659.7490509165186, 1231.7196916859834, 1311.5055530792415, 3218.4261929180616, 1773.7523398929175, 1833.622838867886, 1919.0318081441565, 5978.962488835634, 3097.5264625871546, 5365.896031098067, 6386.653442675892, 3314.475490727286, 6155.881129655974, 2932.4146449371037, 2425.905208328784, 10459.292877603828, 2795.7282101020887, 80580.4732278469, 5158.35787632969, 2690.9138155271776, 4087.8331892929186, 4128.342680325123, 4886.807411959986, 3513.0676450329847, 10939.097780509974, 15558.368245878331, 7778.81325957942, 6068.626468310277, 6645.648888608844, 4042.3088494113294, 4094.9300477944516, 4106.933083556235, 3392.1866734882974, 3454.894277165313, 3221.0744160485174, 2057.3708185641326, 1984.1539384397033, 1939.69118266917, 1876.836133002368, 1833.9078949594586, 1810.422261544192, 1702.581592255761, 1607.6607811263432, 1538.333099418713, 1517.5393978618204, 1485.1263224920174, 1478.3781109138172, 1449.8741601746508, 1423.8941633112086, 1291.771409840427, 1261.807764306596, 1260.1248959169518, 1163.3322162596414, 1118.637120835763, 1100.5714175842904, 20352.484968471985, 1070.8842482674545, 1119.741431606636, 1035.8514735644385, 998.2448298200354, 991.7749874491448, 21911.65221966168, 929.9273075228962, 931.3578104142887, 912.7631658165002, 2886.785521199469, 2112.973921966923, 1423.566277462513, 2326.7578864621805, 1163.9242240727528, 1555.34261370573, 1198.9386666590383, 4968.518925849856, 1555.8580052266364, 2039.6586063461368, 8021.380273326338, 27236.724041006662, 52100.9214757257, 7703.782727359204, 8706.669866848026, 98529.90516420625, 4376.651791808076, 1902.0433912931926, 7558.149203858059, 3444.2145783902115, 3652.9691665128894, 3728.686387949645, 2515.485633367501, 4216.1162500264345, 1772.2470232879364, 4344.921687417553, 3539.2494670938486, 3414.75247565867, 2674.5522146650437, 3085.524596461015, 2580.4785665169265, 2242.687197660477, 2448.2394439573336, 2471.616878031463, 2059.7796985549453, 3584.2832815921197, 2786.432114785431, 2524.8607820954508, 2490.857938510567, 2344.7486717335423, 2167.7756311058806, 1865.092234220436, 1676.8202411611448, 1368.2290375366904, 1290.8082644287556, 1281.757583773137, 1696.3970944127398, 1201.040619352464, 1177.8522699988616, 1142.2386648263716, 1128.002795141831, 1054.06842246474, 1021.7635712537187, 1023.7048543600424, 1010.9074964245253, 1007.8134575931231, 920.1883634109184, 1000.9143517060317, 1504.8772143097738, 886.4538473865995, 889.7085465707833, 859.1683051596403, 838.7740835163339, 838.8322078840529, 836.274912643138, 3471.98417213987, 1082.4851338936066, 1005.7560142207188, 4104.193984412712, 1331.7178222654552, 1110.2544265660551, 1609.677484102076, 1218.777752319868, 1934.814757737163, 1556.3513404435, 2129.7787499182036, 1875.0422058925874, 1430.1815628134732, 13641.486538412128, 1482.4899549861473, 1923.8903845641662, 1454.9597949863685, 1167.0246823027055], \"Total\": [146847.0, 24001.0, 35885.0, 21935.0, 37210.0, 20371.0, 54075.0, 14595.0, 506703.0, 11602.0, 23060.0, 9497.0, 18749.0, 16692.0, 11748.0, 16918.0, 16337.0, 20625.0, 12916.0, 15998.0, 4173.0, 8155.0, 9171.0, 6362.0, 3585.0, 3498.0, 11220.0, 8335.0, 20278.0, 5338.0, 7406.4715265715995, 5172.103045173479, 3132.23319269334, 3112.5032108635273, 2506.187529512052, 2365.7253684021043, 2322.8674587393134, 2351.8138362854907, 2260.6876759567926, 5488.300647765558, 1937.752676420601, 3062.914082614354, 1837.7443066391036, 1711.3607782075749, 1623.6756237846616, 1734.660304325836, 1447.9740166464137, 3221.804613595708, 1219.705128604145, 35885.56523312657, 1178.9280552595988, 1348.833736641391, 1079.1497865120948, 1062.13131077819, 1147.3754299339303, 2327.7242790800424, 953.340436945265, 930.784054139355, 917.5048516950668, 878.857064353469, 37210.76910619332, 4621.757856249607, 2683.1291972338854, 3660.004610476466, 2069.4936236943804, 1899.9712009946984, 1734.8683236312538, 2764.230639705439, 4002.6864957567504, 16692.672698724473, 5701.33947134548, 5650.37194993169, 5255.384140352357, 16918.483585992642, 7593.902572626256, 7877.792822711362, 4767.015268109534, 12147.395148132815, 11879.477867427971, 20278.647274818915, 9736.919637529409, 6420.840619429129, 3391.826601627594, 10573.039153259493, 506703.740389064, 20625.836592818338, 19019.05698785405, 5725.295220375903, 17930.269878161496, 21544.963578670333, 7808.109651043498, 33409.238051298154, 13150.591559462631, 23060.178823889626, 13766.642088525361, 16672.159531726706, 24795.064398318696, 18126.61670058659, 14442.016321357292, 146847.82219321263, 13206.737066232228, 17282.024818649224, 9758.824634066497, 8155.003005494418, 6406.651302699646, 4646.2039601000215, 4309.632469402253, 3957.003813200373, 3477.8389313570806, 4211.733485883158, 2156.3777626538104, 2209.7040729997575, 2080.385306342926, 1987.7199144628491, 1882.143678040758, 1793.6687939640283, 1782.140604835313, 1789.5669432245652, 1765.6140310037538, 2076.1697060625297, 1661.694618672436, 1650.6449823950752, 2172.94033039568, 1507.372723348065, 1561.12517720584, 1341.06450036809, 1280.4044532555554, 1797.8688676283484, 1260.9582929907724, 1272.896791968781, 1319.9063684022267, 1513.9620181287419, 1189.6268914947684, 4063.644941381372, 3294.9061720772615, 2684.0252559829546, 2977.1352570309627, 2098.1725095021225, 2755.8444012700943, 1761.7689063359273, 3622.8524024855105, 2130.3032862814093, 7925.452315970598, 8102.575029922546, 18749.087036006942, 3011.091844033579, 2801.633848594484, 2782.1173096088937, 3823.362548283104, 2091.3748495627033, 15998.027232333488, 2984.719949730493, 13035.691889962416, 17934.548630700207, 146847.82219321263, 3955.5441160613736, 11115.616268724398, 11220.198119849547, 506703.740389064, 4957.832027052102, 15035.795849349342, 15160.953075216295, 10436.393035612049, 26253.431179506777, 24795.064398318696, 5960.545375753241, 18126.61670058659, 54075.996264844296, 20488.219178253377, 16672.159531726706, 33409.238051298154, 13766.642088525361, 9126.445196217892, 9999.59815166518, 14442.016321357292, 11830.241401463836, 24001.90441894579, 9497.155497599651, 3615.0110656242578, 3426.2607173275887, 3101.5888384819364, 2857.402072156384, 2723.822300775134, 2862.6646089317383, 2602.3887362729242, 2930.673398276409, 2194.619032081818, 2048.371354318695, 1966.3757685055841, 1932.959323266848, 1751.9271234580528, 1743.2842220617265, 1751.3578882343745, 1657.742375398956, 3386.9690034635396, 1547.5735717868333, 1490.3864733998912, 1338.7779490209098, 1276.0405594211206, 1290.7382644699742, 1189.0500578660094, 1127.213248981398, 1155.4893707925416, 1087.9587239790199, 1116.5530198343383, 1101.5797829800244, 14595.612597876949, 11602.894435957529, 2415.0883390610093, 1662.285301486057, 1233.1726505812371, 1313.9058369624986, 3264.4835213235974, 1783.9094367535765, 1849.061372281977, 1938.79287822616, 6362.203469337273, 3256.0087451548484, 6734.366802444929, 8335.395664905125, 3811.3988472949372, 9171.035356828146, 3463.0182259113403, 2722.10472917413, 23060.178823889626, 3425.2675993219505, 506703.740389064, 8681.719888680747, 3278.330288889704, 6620.986810020201, 7068.629698272315, 9633.250267852596, 5375.546285869855, 54075.996264844296, 146847.82219321263, 33409.238051298154, 20625.836592818338, 26253.431179506777, 7758.848920614708, 16337.70129596821, 17282.024818649224, 11748.945631654817, 20488.219178253377, 17930.269878161496, 2058.3900436587082, 1985.1917121010324, 1940.7210432015638, 1877.8845451080956, 1834.9477230377365, 1811.5149762793676, 1703.6225063987183, 1608.6929023747268, 1539.3645434363625, 1518.5791048714434, 1486.1471314032863, 1479.4231180005415, 1450.9143250636782, 1424.927936597799, 1292.7991443591582, 1262.8383802903493, 1261.1591488252238, 1164.3592760189488, 1119.6700919650586, 1101.5938495702671, 20371.743215559523, 1071.9095342348696, 1120.8368551259587, 1036.8778785591621, 999.2803596738313, 992.804296467282, 21935.660486676363, 930.9606156495324, 932.3981079560921, 913.7859312371717, 2896.3021223668147, 2118.0252871085368, 1426.9456070563533, 2346.0885285830377, 1165.4412110823441, 1565.6198184223394, 1205.2428416136797, 5338.890385424326, 1586.1118684307442, 2189.808495852611, 11748.945631654817, 54075.996264844296, 146847.82219321263, 12916.713569631369, 16337.70129596821, 506703.740389064, 7407.041720915088, 2206.541010887916, 20488.219178253377, 6537.491155493633, 7561.238683617209, 11220.198119849547, 4827.071750572982, 17282.024818649224, 2189.7016928957078, 26253.431179506777, 15998.027232333488, 19019.05698785405, 8414.81800280434, 18749.087036006942, 17934.548630700207, 7059.445600853421, 15035.795849349342, 21544.963578670333, 9633.250267852596, 3585.3482882555227, 2787.5020285453515, 2525.916447723639, 2491.918394338755, 2345.812658629701, 2168.8394398795517, 1866.1541602711645, 1677.878784337259, 1369.3115321909386, 1291.8677403706613, 1282.8203674899878, 1697.8048814780616, 1202.0974722672352, 1178.913457898418, 1143.3109002344718, 1129.0659560702336, 1055.1281953157613, 1022.8221207698497, 1024.774475623456, 1011.9670913843704, 1008.8840026540131, 921.2474065494405, 1002.0677984839156, 1506.6412829695378, 887.5117249373335, 890.7745258189459, 860.2288726412987, 839.8329074931625, 839.8933356148015, 837.3411857196111, 3498.673437070825, 1084.574189018, 1007.1518713648692, 4173.330816504232, 1342.3607843100572, 1117.8452913226065, 1664.2689673282052, 1258.4128070954407, 2462.716407250819, 1861.415137621535, 3773.206226490076, 3948.543086020557, 2266.3120421404865, 506703.740389064, 2987.0779561195177, 15035.795849349342, 7420.052219903386, 5501.828503064982], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.4101, -5.7694, -6.2709, -6.2772, -6.494, -6.5517, -6.57, -6.5576, -6.5972, -5.7102, -6.7514, -6.2935, -6.8044, -6.8757, -6.9283, -6.8622, -7.0429, -6.2432, -7.2146, -3.833, -7.2487, -7.1141, -7.3372, -7.3531, -7.2759, -6.5686, -7.4613, -7.4853, -7.4997, -7.5427, -3.8139, -5.8883, -6.4271, -6.1206, -6.6875, -6.7722, -6.8635, -6.4039, -6.0551, -4.6779, -5.7428, -5.7517, -5.8212, -4.7327, -5.4845, -5.4547, -5.9209, -5.0641, -5.0881, -4.6043, -5.2753, -5.6849, -6.2457, -5.3038, -2.1416, -4.7723, -4.847, -5.8275, -4.9606, -4.8236, -5.5964, -4.569, -5.2547, -4.888, -5.2687, -5.1715, -4.9954, -5.1684, -5.3602, -4.5049, -5.441, -5.4525, -5.5961, -5.1744, -5.4158, -5.7371, -5.8123, -5.8977, -6.0268, -5.8354, -6.505, -6.4805, -6.5409, -6.5864, -6.641, -6.6892, -6.6957, -6.6915, -6.705, -6.543, -6.7657, -6.7724, -6.4974, -6.8632, -6.8282, -6.9802, -7.0265, -6.6871, -7.0418, -7.0324, -6.9962, -6.859, -7.1001, -5.8728, -6.0823, -6.2866, -6.1856, -6.5349, -6.2651, -6.7089, -6.0135, -6.5283, -5.2923, -5.2946, -4.5257, -6.2143, -6.2836, -6.2928, -6.0098, -6.5542, -4.7923, -6.2544, -5.0652, -4.8293, -3.1696, -6.0333, -5.2243, -5.2666, -2.4905, -5.9021, -5.1223, -5.1415, -5.4724, -4.9399, -4.9777, -5.8421, -5.3465, -4.968, -5.4629, -5.5347, -5.3376, -5.655, -5.7672, -5.7836, -5.7786, -5.8225, -3.6529, -4.5801, -5.5461, -5.5998, -5.6994, -5.7814, -5.8293, -5.7796, -5.8749, -5.7561, -6.0454, -6.1144, -6.1553, -6.1725, -6.2708, -6.2758, -6.2712, -6.3261, -5.6117, -6.395, -6.4326, -6.54, -6.588, -6.5766, -6.6587, -6.7122, -6.6874, -6.7476, -6.7217, -6.7352, -4.1536, -4.3812, -5.9502, -6.3243, -6.6225, -6.5598, -5.6621, -6.2579, -6.2247, -6.1791, -5.0427, -5.7003, -5.1509, -4.9767, -5.6327, -5.0135, -5.7551, -5.9447, -4.4835, -5.8029, -2.4417, -5.1903, -5.8411, -5.4229, -5.4131, -5.2444, -5.5745, -4.4386, -4.0864, -4.7795, -5.0278, -4.937, -5.4341, -5.4212, -5.4183, -5.6095, -5.5912, -5.6612, -6.0306, -6.0669, -6.0895, -6.1225, -6.1456, -6.1585, -6.2199, -6.2773, -6.3214, -6.335, -6.3566, -6.3611, -6.3806, -6.3987, -6.4961, -6.5195, -6.5209, -6.6008, -6.64, -6.6562, -3.7389, -6.6836, -6.639, -6.7168, -6.7538, -6.7603, -3.665, -6.8247, -6.8232, -6.8433, -5.6919, -6.004, -6.3989, -5.9076, -6.6003, -6.3104, -6.5706, -5.1489, -6.31, -6.0393, -4.67, -3.4475, -2.7989, -4.7104, -4.588, -2.1617, -5.2758, -6.1091, -4.7294, -5.5154, -5.4565, -5.436, -5.8296, -5.3132, -6.1798, -5.2831, -5.4882, -5.524, -5.7683, -5.6253, -5.8041, -5.9444, -5.8567, -5.8472, -6.0295, -4.6635, -4.9153, -5.0139, -5.0275, -5.0879, -5.1664, -5.3168, -5.4232, -5.6266, -5.6848, -5.6919, -5.4116, -5.7569, -5.7764, -5.8071, -5.8197, -5.8874, -5.9186, -5.9167, -5.9293, -5.9323, -6.0233, -5.9392, -5.5314, -6.0606, -6.057, -6.0919, -6.1159, -6.1158, -6.1189, -4.6954, -5.8608, -5.9344, -4.5281, -5.6536, -5.8355, -5.4641, -5.7423, -5.2801, -5.4978, -5.1841, -5.3115, -5.5823, -3.327, -5.5464, -5.2858, -5.5651, -5.7856], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1552, 1.155, 1.155, 1.155, 1.1549, 1.1549, 1.1549, 1.1548, 1.1548, 1.1548, 1.1548, 1.1548, 1.1547, 1.1547, 1.1547, 1.1547, 1.1546, 1.1545, 1.1544, 1.1544, 1.1543, 1.1543, 1.1543, 1.1543, 1.1543, 1.1542, 1.1542, 1.1541, 1.1541, 1.1541, 1.1371, 1.1486, 1.1536, 1.1496, 1.1529, 1.1536, 1.1532, 1.147, 1.1256, 1.0748, 1.0842, 1.0843, 1.0872, 1.0066, 1.0558, 1.0489, 1.085, 1.0064, 1.0047, 0.9538, 1.0164, 1.0232, 1.1006, 0.9056, 0.1981, 0.7689, 0.7752, 0.9953, 0.7206, 0.6739, 0.9161, 0.4899, 0.7365, 0.5416, 0.6767, 0.5825, 0.3616, 0.5019, 0.5374, -0.9266, 0.5459, 0.2655, 0.6934, 1.2946, 1.2946, 1.2945, 1.2945, 1.2945, 1.2945, 1.2944, 1.2943, 1.2943, 1.2943, 1.2943, 1.2942, 1.2942, 1.2942, 1.2942, 1.2942, 1.2942, 1.2942, 1.2941, 1.2941, 1.2941, 1.2941, 1.294, 1.294, 1.294, 1.294, 1.2939, 1.2939, 1.2939, 1.2939, 1.2928, 1.293, 1.2938, 1.2911, 1.2917, 1.2888, 1.2925, 1.267, 1.2831, 1.2052, 1.1809, 1.1108, 1.2511, 1.2539, 1.2517, 1.2168, 1.2756, 1.003, 1.2198, 0.9348, 0.8517, 0.4087, 1.1593, 0.935, 0.8834, -0.1507, 1.0646, 0.7349, 0.7074, 0.75, 0.3599, 0.3793, 0.9404, 0.3238, -0.3907, 0.0849, 0.2193, -0.2787, 0.2904, 0.5893, 0.4815, 0.1189, 0.2745, 1.7367, 1.7366, 1.7364, 1.7364, 1.7364, 1.7363, 1.7363, 1.7363, 1.7363, 1.7363, 1.7362, 1.7362, 1.7362, 1.7362, 1.7361, 1.7361, 1.7361, 1.7361, 1.7361, 1.736, 1.736, 1.7359, 1.7359, 1.7359, 1.7358, 1.7358, 1.7358, 1.7358, 1.7357, 1.7357, 1.7333, 1.7352, 1.7357, 1.7352, 1.7355, 1.7349, 1.7225, 1.731, 1.7283, 1.7265, 1.6746, 1.6868, 1.5096, 1.4704, 1.597, 1.3381, 1.5704, 1.6215, 0.9461, 1.5336, -0.102, 1.2161, 1.5393, 1.2545, 1.1989, 1.058, 1.3113, 0.1387, -0.5081, 0.2793, 0.5133, 0.3629, 1.0847, 0.353, 0.2997, 0.4944, -0.0433, 0.0199, 1.8151, 1.8151, 1.8151, 1.815, 1.815, 1.815, 1.815, 1.815, 1.8149, 1.8149, 1.8149, 1.8149, 1.8149, 1.8149, 1.8148, 1.8148, 1.8148, 1.8147, 1.8147, 1.8147, 1.8147, 1.8146, 1.8146, 1.8146, 1.8146, 1.8146, 1.8145, 1.8145, 1.8145, 1.8145, 1.8123, 1.8132, 1.8132, 1.8073, 1.8143, 1.809, 1.8104, 1.7437, 1.7963, 1.7446, 1.434, 1.1298, 0.7794, 1.2988, 1.1862, 0.178, 1.2895, 1.6671, 0.8184, 1.1747, 1.0881, 0.7139, 1.1638, 0.4049, 1.6041, 0.0168, 0.3071, 0.0983, 0.6694, 0.0112, -0.1231, 0.6689, 0.0005, -0.3497, 0.273, 2.6273, 2.6272, 2.6272, 2.6271, 2.6271, 2.6271, 2.627, 2.6269, 2.6268, 2.6268, 2.6267, 2.6267, 2.6267, 2.6267, 2.6266, 2.6266, 2.6266, 2.6265, 2.6265, 2.6265, 2.6265, 2.6264, 2.6264, 2.6264, 2.6264, 2.6264, 2.6263, 2.6263, 2.6263, 2.6263, 2.6199, 2.6256, 2.6262, 2.6109, 2.6196, 2.6208, 2.5942, 2.5956, 2.3863, 2.4486, 2.0557, 1.8829, 2.1672, -0.9872, 1.927, 0.5715, 0.9984, 1.0769]}, \"token.table\": {\"Topic\": [3, 3, 5, 3, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 5, 5, 5, 1, 2, 3, 4, 4, 2, 4, 3, 1, 2, 4, 5, 2, 4, 1, 1, 2, 3, 4, 2, 1, 2, 4, 2, 2, 2, 1, 2, 3, 4, 5, 2, 3, 3, 4, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 4, 1, 1, 1, 2, 3, 4, 3, 4, 2, 4, 3, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 2, 5, 1, 5, 5, 3, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 4, 2, 3, 4, 5, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 4, 2, 3, 4, 4, 5, 4, 5, 2, 2, 4, 4, 5, 4, 5, 1, 2, 3, 1, 2, 4, 4, 3, 4, 3, 4, 1, 2, 3, 5, 5, 4, 3, 5, 3, 1, 2, 3, 5, 4, 5, 1, 1, 5, 5, 1, 5, 1, 3, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 2, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 1, 4, 1, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 1, 3, 2, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 3, 4, 5, 2, 2, 3, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 3, 1, 2, 3, 2, 2, 4, 4, 1, 2, 3, 4, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 5, 1, 1, 5, 1, 2, 3, 4, 5, 2, 3, 2, 3, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 2, 3, 4, 1, 1, 2, 3, 4, 5, 1, 3, 4, 5, 5, 1, 2, 3, 4, 2, 3, 4, 2, 3, 4, 4, 4, 3, 1, 5, 3, 1, 2, 4, 2, 3, 1, 1, 5, 3, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 5, 1, 5, 2, 5, 2, 5, 3, 1, 2, 3, 4, 5, 3, 2, 4, 2, 1, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 1, 5, 1, 2, 2, 2, 4, 1, 3, 5, 4, 1, 2, 1, 3, 1, 2, 3, 5, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 2, 3, 4, 5, 1, 2, 3, 4, 3, 5, 4, 1, 2, 3, 4, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 1, 2, 3, 4, 5, 2, 5, 1, 2, 4, 2, 4, 4, 1, 3, 3, 1, 2, 3, 2, 4, 4, 5, 3, 4, 1, 3, 1, 5, 3, 2, 1, 5, 1, 2, 1, 3, 5, 2, 2, 1, 2, 4, 5, 1, 2, 4, 3, 4, 5, 1, 1, 2, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 2, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 2, 4, 5, 1, 3, 4, 1, 5, 1, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 3, 4, 1, 2, 3, 4, 3, 5, 1, 2, 3, 4, 1, 2, 3, 4, 4, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 1, 2, 5, 1, 3, 1, 1, 3, 1, 5, 1, 1, 4], \"Freq\": [0.999118786441168, 0.9987110476044274, 0.9986459592281376, 0.9993003532042964, 0.11438749819408951, 0.06375196566017256, 0.8208446870407864, 0.0009150999855527161, 0.9996981077749089, 0.9991636965196505, 0.12468690189977019, 0.41228394875575025, 0.10594641287583968, 0.35479586432987037, 0.0022812731915031687, 0.9996371820910921, 0.9982966704609562, 0.5413227753059747, 8.672959629992384e-05, 0.45355242385045175, 0.0050303165853955835, 0.9991135670610443, 0.0008580441385553038, 0.9987633772783737, 0.9991845404807566, 0.007428119495680667, 0.007188502737755485, 0.0019169340634014624, 0.9833871745249503, 0.0009326644165379125, 0.9990308529147155, 0.9988012381143551, 0.9819200429780596, 2.687394063654441e-05, 0.016608095313384443, 0.001451192794373398, 0.9989031174860534, 0.00812862409794907, 0.981172744058323, 0.010519395891463501, 0.9994730352018348, 0.9997463198804549, 0.9993923534881521, 0.180940303084855, 0.14415806789950528, 0.583988718635091, 0.09082382688074818, 0.9991962231231067, 0.9995672543868203, 0.00046020591822597615, 0.9994707979312388, 0.9996284663351852, 0.003363399439670558, 0.9944451009959283, 0.0022422662931137053, 0.991595983572516, 0.0014470572543925809, 0.006511757644766614, 0.35938774213777863, 0.00027001332993071273, 0.049412439377320434, 0.5909241725533648, 0.027632492288003876, 0.9143957607815711, 0.004037624443909242, 0.05400322693728611, 0.9993305147937147, 0.9993361143409686, 0.1557268583362716, 0.6978470480152852, 0.09761042246958675, 0.04867026595027321, 8.996352301344402e-05, 0.1642832744227648, 0.00100644795829215, 0.23829590889409522, 0.5964365438986711, 0.0017750355129308533, 0.21465688082236217, 0.2506472560500636, 0.5329391107272048, 0.3839600628379316, 0.23563670540170523, 0.1590278373278397, 0.19445287679215745, 0.026921056453078452, 0.0693027901472064, 0.9307177411931584, 0.9990247966498403, 0.9995949890110313, 0.9225604717677566, 0.0012580370069560317, 0.07596147261048801, 0.00017971957242229024, 0.998625205021055, 0.001203162897615729, 0.9885916308546694, 0.011266001491221303, 0.9994663609423268, 0.9995289672571193, 0.9995093192624006, 0.5638741629187347, 0.34110758052535306, 0.0020393278948236337, 0.090690111087451, 0.002219268591425719, 0.03952054808151941, 0.9571278955541087, 0.0029889490145686946, 0.9995819817524648, 0.9989344049519093, 0.0006637279963741832, 0.9989106345431457, 0.9996314508770264, 0.9994189110886695, 0.9990082461811974, 0.0003271168285014187, 0.3222100760738974, 0.6712437320849112, 0.006215219741526955, 0.7789624043396243, 0.06053131845281199, 0.10659186858799861, 0.0002837405552475562, 0.05362696494178812, 0.998500524674855, 0.004326519454639292, 0.9918545849760577, 0.003785704522809381, 0.9991237816719389, 0.0018914176608287525, 0.01702275894745877, 0.981015293416513, 0.011769744415215718, 0.9251019110359555, 0.01098509478753467, 0.05230997517873653, 0.007548207952076669, 0.9579035329659201, 0.0007188769478168256, 0.0337872165473908, 0.9962597409692203, 0.0033589337187094415, 0.9988728660521315, 0.0003725747355658827, 0.0003725747355658827, 0.9990380588330505, 0.9988534175313093, 0.9991898741069676, 0.9992251696744167, 0.9998983396053767, 0.9992400280040195, 0.9994015273160662, 0.9932168599953972, 0.006387246688073295, 0.032446678427640735, 0.9673917086759553, 0.9996539537811479, 0.9994366038291022, 0.9996320427919531, 0.9942610426182683, 0.0038251317935300237, 0.0019125658967650118, 0.9993818495605872, 0.0049782498537528745, 0.9948202624416161, 0.9995521768581337, 0.9993246937513177, 0.021529996625445455, 0.9727142065137152, 0.0002760255977621212, 0.005244486357480303, 0.999044345026035, 0.9995692761659457, 0.0009928989146838628, 0.998856308171966, 0.9986090944122217, 0.9314431061593434, 0.0015928154959973571, 0.014335339463976216, 0.05273989086746805, 0.9996186534704806, 0.9990558956591484, 0.9992534678829182, 0.000922020835573659, 0.9976265440906992, 0.9991305029538942, 0.999129489856749, 0.0006207701086404157, 0.9995933868379824, 0.0001933449490982558, 0.008768342503689016, 0.9897911332105425, 0.0010315697063163548, 0.9995838923891173, 0.002986812098768011, 0.8319871772978965, 0.00048002337301628745, 0.16459468101425145, 0.41071576244587205, 0.1075105503838307, 0.23764576449213815, 0.2439528958117493, 0.0001735907702645275, 0.8615838928734229, 0.1353376571645239, 0.0017287656937074222, 0.001317154814253274, 0.5436619177009476, 0.1503020761332007, 0.17960530205942168, 0.12470907777903352, 0.0016658164609221237, 0.9996129543459435, 0.9993341106867548, 0.8174608382574192, 0.041768072027751346, 0.0961109473224172, 0.04403646791119475, 0.0006410684018427007, 0.9999623188672864, 0.023236127611695183, 0.0008511402055566002, 0.28870675772479876, 0.682699558876949, 0.004511043089449981, 0.008098586122611235, 0.9918636793324391, 0.09720629525854149, 0.06692025224601632, 0.33341098006365044, 0.483121900108098, 0.019309005588771508, 0.9994280292989708, 0.487119574819, 0.4434178186712063, 0.06150247146657462, 0.007900317472942106, 0.9980883524955961, 0.0014907966430105992, 0.9984361862994856, 0.0005263237671584004, 0.0010526475343168007, 0.9990870355419971, 0.9705981230652189, 0.012241778128750509, 0.016988590056225196, 0.6196136970183778, 0.36632026659597644, 0.012639247746916498, 0.0014527870973467239, 0.9973454261315513, 0.0019328399731231613, 0.999313308561937, 0.0007449562082625776, 0.004469737249575466, 0.002234868624787733, 0.9922816694057534, 0.16419191783981985, 0.2982201599290426, 0.16863349468982688, 0.3688949212346488, 4.880853681326393e-05, 0.9313600824310981, 0.0664756066367959, 0.0021047685478668885, 0.20831090399507673, 0.21493656888041632, 0.012191223389024848, 0.5645066482309331, 0.9997408723098768, 0.9996378189615092, 0.9997678355579398, 0.08045899986541286, 0.3925053111305122, 0.5268075960769617, 0.9983598433378454, 0.9995884151053317, 0.00023743192757846358, 0.03712301502633282, 0.15116491718722724, 0.7968083945252077, 0.014997698070638459, 0.9995037022997324, 0.0017028340034898832, 0.9978607260450715, 0.0005676113344966276, 0.9993599804458647, 0.0010485209694948603, 0.9989213688509295, 0.9994835149656648, 0.16463443023945462, 0.0018602760479034422, 0.6535149756284792, 0.17988869383226286, 0.9277252293804075, 0.0006700796167428006, 0.07136347918310826, 0.9989681457696659, 0.4015755238135506, 0.5799896553670775, 9.58185454100574e-05, 0.018301342173320963, 0.0036286518917364383, 0.9942506183357841, 0.00036286518917364385, 0.0018143259458682191, 0.999611572503674, 0.9993273244994619, 0.9993815300494806, 0.68378784543867, 0.020873800433614146, 0.1157260321269137, 0.1795567468030033, 0.0001051576848041015, 0.0014651516562381854, 0.9984577610305516, 0.0002952492328620051, 0.9994186532378873, 0.0593001131131122, 0.7942987940116866, 0.10871687404070571, 0.03630619170190543, 0.0014119074550741002, 0.9991533443066777, 0.2699511487157546, 0.6976998288064188, 0.02945758485559811, 0.0028383610407737765, 0.9982074839862471, 0.001517493894779944, 0.9993283055660076, 0.5140195048337154, 0.20733786234106716, 0.23283979084035825, 0.04543653457972282, 0.0003891139325009201, 0.007344481082825652, 0.5148734497029155, 0.0027858376521062815, 0.47485869069993436, 0.15160185876926452, 0.8466602855456831, 0.0017325926716487373, 0.9996933843582215, 0.4373537127788268, 0.36051673463503986, 0.1797089279798581, 0.01961075789808423, 0.00278946125274474, 0.8762092587964287, 0.0001557428472798487, 0.0009344570836790922, 0.12256962080924093, 0.9989307504805862, 0.12036282657928885, 0.30059631904741496, 0.05779901655012321, 0.5210198086865946, 0.16897278802881327, 0.021464110911768173, 0.8092426496947489, 0.13324021559050647, 0.00453198012212607, 0.8619826192283786, 0.9991399175558464, 0.9991514822792216, 0.9994290055393436, 0.007431388058260114, 0.9923761283953507, 0.9997179318721068, 0.99777024943129, 0.0005764126224328653, 0.0011528252448657307, 0.003357173237602062, 0.9966008553910122, 0.999204856027494, 0.16331660458528496, 0.8359231471536296, 0.9996293734932608, 0.959797084600834, 0.039619738337929554, 0.5389829111665512, 0.3085441742238119, 0.10178634113756813, 0.050546958524098455, 0.1879754294308099, 0.3926724826752217, 0.25314786301867526, 0.16550217646947699, 0.0007237149258734322, 0.9996535705327058, 0.0005889958327422335, 0.998936932330828, 0.03099142012867496, 0.9686805419706352, 0.9992062273158392, 0.9996239449707189, 0.9992246654761446, 0.10701605887494717, 0.18536875309529469, 0.20228938448817124, 0.503680040707955, 0.0016458319059737855, 0.999487734008382, 0.0031074106290559685, 0.99678827623162, 0.9996813722668295, 0.06804247964248866, 0.9315883118837374, 0.6577650869078823, 0.16257823766578638, 0.06265877822105144, 0.11687687151183988, 7.604220657894593e-05, 0.9992281172038443, 0.7872584114105643, 0.07902040667647929, 0.04213567876266076, 0.009733469866146557, 0.08183799005878487, 0.9993610754675402, 0.7877381124449074, 0.21211130069755585, 0.9989345440953002, 0.9990893271937309, 0.9998770073421502, 0.0021023926806773676, 0.9979357257615238, 0.9340896628863494, 0.06564696143731728, 0.9994611560709301, 0.9994609178596097, 0.998640502093344, 0.0007413812190744945, 0.9993750779281669, 0.00032648646779750637, 0.8702957727347687, 0.05114553868561657, 0.0686048591204656, 0.009962082836355034, 0.13819692234022263, 0.24422341357829505, 0.6174306213407978, 0.00030206977560704395, 0.10837202435245803, 0.8912221392510616, 0.6794391072059353, 0.013478241173344513, 0.2942426103634095, 0.012847963708403942, 0.9992954715775552, 0.9990697222333692, 0.9993487834900516, 0.9990421959064055, 0.2872969643302727, 0.4938395950558882, 0.0887530667839515, 0.13006159292907463, 0.9994736804460445, 0.9992442477424266, 0.9993996992362157, 0.00026237086174010093, 0.12829935139090934, 0.8694970358066945, 0.0018365960321807065, 0.0008109164596933489, 0.9990490783422058, 0.40233963371176995, 0.0013822146019299283, 0.5941219097295475, 0.00218850645305572, 0.4784214821012278, 0.00038665529264646643, 0.5209535642923391, 0.00012888509754882215, 0.9989605094887628, 0.3556820835581407, 0.14926050683228356, 0.1584110315345812, 0.3178916049174829, 0.01889523932032889, 0.9998532428445479, 0.9993604969871246, 0.030589976106873126, 0.8732047725052874, 0.09581488383888359, 0.001888551578843838, 0.9976273715242573, 0.9993698283572753, 0.9995170411846386, 0.9998783322438025, 0.9997203146530111, 0.9466875454243967, 0.0002948263922218613, 0.05306875059993504, 0.9996092543206115, 0.9990808861623027, 0.3684399961142858, 0.630981071189735, 0.9985494874070241, 0.001522179096657049, 0.048525668192726516, 0.9514716459561187, 0.9994218860053888, 0.9985714585032173, 0.9989236739521166, 0.9996271363106283, 0.503502092042429, 0.4961370348449998, 0.9992127973751592, 0.9992792524121263, 0.9988296384135672, 0.0008592082911084448, 0.9994762527868745, 0.9996831952966545, 0.9997587779728623, 0.73166600976707, 0.00013476994101438018, 0.0721019184426934, 0.19609026417592315, 0.07022458883733894, 0.8923089231879798, 0.03727210163247163, 0.9857608344413543, 0.007045524919872949, 0.007045524919872949, 0.9991576411995154, 0.04127139120535806, 0.701613650491087, 0.25702346067727055, 0.0017225914421599197, 0.9981186984858049, 0.00017824997193782334, 0.6627333956648273, 0.004812749242321231, 0.3323470726780716, 0.027315867991322247, 0.7469045918267954, 0.004500554909325404, 0.22121477533475845, 6.250770707396396e-05, 0.898982768318415, 0.00012693910877130965, 0.011932276224503107, 0.08898431524868806, 0.4018729175640979, 0.280475282614351, 0.31773033277979257, 0.27841070515422833, 0.0005190356173643332, 0.5073054124118992, 0.21384267435410526, 0.860138813677701, 0.004714011897235393, 0.10572855255227954, 0.029378395573842007, 0.6178706198036452, 0.15112580664668485, 0.11617564313164994, 0.11473679177844133, 9.282911956184574e-05, 0.9933449875120532, 0.0006491036729549466, 0.005841933056594519, 0.0002163678909849822, 0.9970581496639725, 0.0028596314043899786, 0.9992533658024079, 0.6474526082922709, 0.15482198644322026, 0.1796403524256529, 0.0063579634202187, 0.011712037879350237, 0.17179133210667877, 0.6420568611517059, 0.04220903550949556, 0.14385642221201922, 0.0009430694929700192, 0.003772277971880077, 0.9397687497446242, 0.0554839218364028, 0.9993645691786041, 0.000660518552001721, 0.9989363701592565, 0.8617793625470814, 0.13813294720662067, 5.910695216372301e-05, 0.9993986029597519, 0.00036441152341285394, 0.8520084663301829, 0.00017466348223250982, 0.14706665203977326, 0.0006986539289300393, 0.45214643607702004, 0.4003216059673982, 0.09945527708196694, 0.048074083650326276, 0.12536897371288666, 0.09981529773121693, 0.7662503685207723, 0.008517891993889906, 0.13750750455037059, 0.0005838959853518921, 0.816286587521945, 0.04554388685744758, 0.9988325974233689, 0.9991352949590981, 0.0008281270575707403, 0.905199919838149, 0.062418498982147595, 0.02001605874533003, 0.012246667521813769, 0.006262047220968991, 0.9929817736107971, 0.34212888690218435, 0.5558357682523053, 0.08805515018592946, 0.014049248681350542, 0.6300963723166849, 0.08156719992910737, 0.2867148560322141, 0.0015370703504228775, 0.9987187182641624, 0.9996345983946677, 0.9989348673118759, 0.5202294590195002, 0.37872483946647606, 0.0702833860859855, 0.030728293602742485, 5.516749300312834e-05, 0.11073574133902936, 0.5713033141755328, 0.027135244724518906, 0.16281146834711344, 0.12796130110287837, 0.9989810601313084, 2.786635778212247e-05, 2.786635778212247e-05, 0.0009753225223742864, 0.9996265569367508, 0.9996522280674193, 0.20708839982485985, 0.006902946660828662, 0.7857177522766742, 0.9990428648642644, 0.9991168934738593, 0.9998013188106752, 0.9995261609524155, 0.9992633317932473, 0.9985939577371122, 0.9983982804829331, 0.9996062896286853, 0.9320297398086519, 0.06775728245739243], \"Term\": [\"2021\", \"7\", \"`\", \"absolutely\", \"access\", \"access\", \"access\", \"access\", \"accurate\", \"agi\", \"ai\", \"ai\", \"ai\", \"ai\", \"ai\", \"aiart\", \"alien\", \"answer\", \"answer\", \"answer\", \"answer\", \"apparently\", \"argue\", \"argue\", \"argument\", \"art\", \"art\", \"art\", \"art\", \"artificial\", \"artificial\", \"artist\", \"ask\", \"ask\", \"ask\", \"ask\", \"assessment\", \"automation\", \"automation\", \"automation\", \"avoid\", \"awesome\", \"ban\", \"base\", \"base\", \"base\", \"base\", \"beautiful\", \"benefit\", \"benefit\", \"bing\", \"blow\", \"blow_away\", \"blow_away\", \"blow_away\", \"book\", \"book\", \"book\", \"bot\", \"bot\", \"bot\", \"bot\", \"business\", \"business\", \"business\", \"business\", \"capable\", \"career\", \"change\", \"change\", \"change\", \"change\", \"change\", \"chat\", \"chat\", \"chat\", \"chat\", \"chatbot\", \"chatbot\", \"chatbot\", \"chatbot\", \"chatgpt\", \"chatgpt\", \"chatgpt\", \"chatgpt\", \"chatgpt\", \"chatgpt3\", \"chatgpt3\", \"cheat\", \"christmas\", \"code\", \"code\", \"code\", \"code\", \"compare\", \"compare\", \"concern\", \"concern\", \"context\", \"couple\", \"crazy\", \"create\", \"create\", \"create\", \"create\", \"create\", \"customer\", \"customer\", \"customer\", \"cybersecurity\", \"dall_e\", \"dalle\", \"dalle\", \"dalle2\", \"damn\", \"dark\", \"datum\", \"datum\", \"datum\", \"datum\", \"day\", \"day\", \"day\", \"day\", \"day\", \"decade\", \"description\", \"description\", \"description\", \"destroy\", \"detect\", \"detect\", \"detect\", \"development\", \"development\", \"development\", \"development\", \"digital\", \"digital\", \"digital\", \"digital\", \"discuss\", \"discuss\", \"discussion\", \"discussion\", \"discussion\", \"disrupt\", \"domain\", \"dumb\", \"earth\", \"education\", \"educator\", \"elon\", \"elon_musk\", \"elon_musk\", \"elonmusk\", \"elonmusk\", \"email\", \"engineering\", \"error\", \"essay\", \"essay\", \"essay\", \"ethic\", \"ethical\", \"ethical\", \"exactly\", \"exam\", \"explore\", \"explore\", \"explore\", \"explore\", \"eye\", \"fake\", \"family\", \"family\", \"fan\", \"feel\", \"feel\", \"feel\", \"feel\", \"feeling\", \"fight\", \"finally\", \"fire\", \"fire\", \"forever\", \"friend\", \"friend\", \"fun\", \"fun\", \"function\", \"function\", \"function\", \"funny\", \"future\", \"future\", \"future\", \"future\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"get\", \"get\", \"get\", \"get\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go_to\", \"god\", \"good\", \"good\", \"good\", \"good\", \"good\", \"google\", \"gpt\", \"gpt\", \"gpt\", \"gpt\", \"gpt\", \"gpt-3\", \"gpt-3\", \"gpt3\", \"gpt3\", \"gpt3\", \"gpt3\", \"gpt3\", \"gptchat\", \"great\", \"great\", \"great\", \"great\", \"guess\", \"guess\", \"guy\", \"guy\", \"guy\", \"haiku\", \"have\", \"have\", \"have\", \"help\", \"help\", \"help\", \"help\", \"hey\", \"hey\", \"holiday\", \"home\", \"home\", \"home\", \"home\", \"human\", \"human\", \"human\", \"human\", \"human\", \"idea\", \"idea\", \"idea\", \"image\", \"image\", \"image\", \"image\", \"impact\", \"implication\", \"impressed\", \"impressive\", \"impressive\", \"impressive\", \"incorrect\", \"industry\", \"industry\", \"information\", \"information\", \"information\", \"information\", \"insane\", \"insight\", \"insight\", \"insight\", \"instruction\", \"intelligence\", \"intelligence\", \"interact\", \"internet\", \"internet\", \"internet\", \"internet\", \"interview\", \"interview\", \"interview\", \"invent\", \"job\", \"job\", \"job\", \"job\", \"join\", \"join\", \"join\", \"join\", \"joke\", \"kid\", \"kill\", \"know\", \"know\", \"know\", \"know\", \"know\", \"language\", \"language\", \"large\", \"large\", \"late\", \"late\", \"late\", \"late\", \"late\", \"lawyer\", \"learn\", \"learn\", \"learn\", \"learn\", \"learning\", \"learning\", \"light\", \"like\", \"like\", \"like\", \"like\", \"like\", \"live\", \"live\", \"live\", \"live\", \"llm\", \"llm\", \"llm\", \"lol\", \"look\", \"look\", \"look\", \"look\", \"look\", \"love\", \"love\", \"love\", \"love\", \"lyric\", \"machine\", \"machine\", \"machine\", \"machine\", \"machinelearne\", \"machinelearne\", \"machinelearne\", \"machinelearning\", \"machinelearning\", \"machinelearning\", \"medical\", \"metaverse\", \"microsoft\", \"midjourney\", \"midjourney\", \"mind_blow\", \"minute\", \"minute\", \"minute\", \"model\", \"model\", \"movie\", \"music\", \"music\", \"n\", \"natural_language\", \"natural_language\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"night\", \"night\", \"notice\", \"notice\", \"obsolete\", \"ok\", \"okay\", \"openai\", \"openai\", \"openai\", \"openai\", \"openai\", \"openaichat\", \"openaichatgpt\", \"openaichatgpt\", \"opportunity\", \"pass\", \"pass\", \"people\", \"people\", \"people\", \"people\", \"people\", \"plagiarism\", \"play\", \"play\", \"play\", \"play\", \"play\", \"podcast\", \"poem\", \"poem\", \"poetry\", \"policy\", \"potential\", \"prediction\", \"prediction\", \"pretty\", \"pretty\", \"probably\", \"professor\", \"programmer\", \"programmer\", \"programming\", \"programming\", \"prompt\", \"prompt\", \"prompt\", \"prompt\", \"provide\", \"provide\", \"provide\", \"provide\", \"query\", \"query\", \"question\", \"question\", \"question\", \"question\", \"raise\", \"random\", \"rap\", \"rate\", \"read\", \"read\", \"read\", \"read\", \"recipe\", \"red\", \"reference\", \"release\", \"release\", \"release\", \"release\", \"relevant\", \"relevant\", \"response\", \"response\", \"response\", \"response\", \"result\", \"result\", \"result\", \"result\", \"revolutionize\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scary\", \"scene\", \"school\", \"school\", \"school\", \"science\", \"science\", \"screenshot\", \"script\", \"search\", \"search_engine\", \"second\", \"second\", \"second\", \"security\", \"sell\", \"send\", \"send\", \"sentence\", \"sentence\", \"seo\", \"seo\", \"shit\", \"skynet\", \"slow\", \"society\", \"song\", \"song\", \"sorry\", \"sort\", \"sound\", \"sound\", \"stablediffusion\", \"stack_overflow\", \"stackoverflow\", \"story\", \"story\", \"story\", \"story\", \"student\", \"student\", \"student\", \"stuff\", \"stuff\", \"stuff\", \"suggestion\", \"system\", \"system\", \"system\", \"teacher\", \"teacher\", \"tech\", \"tech\", \"tech\", \"tech\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"tell\", \"tell\", \"tell\", \"tell\", \"test\", \"test\", \"test\", \"text\", \"text\", \"text\", \"text\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thread\", \"thread\", \"thread\", \"thread\", \"threat\", \"threat\", \"tiktok\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tool\", \"tool\", \"tool\", \"tool\", \"train\", \"train\", \"train\", \"train\", \"transform\", \"transform\", \"travel\", \"try\", \"try\", \"try\", \"tweet\", \"tweet\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"version\", \"version\", \"version\", \"version\", \"viral\", \"vs\", \"vs\", \"want\", \"want\", \"want\", \"want\", \"war\", \"war\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"wikipedia\", \"wild\", \"wish\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"write\", \"write\", \"write\", \"write\", \"writer\", \"yesterday\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ud83d\\udc40\", \"\\ud83d\\ude01\", \"\\ud83d\\ude02\", \"\\ud83d\\ude05\", \"\\ud83d\\ude09\", \"\\ud83d\\ude0e\", \"\\ud83d\\ude2e\", \"\\ud83e\\udd23\", \"\\ud83e\\udd2f\", \"\\ud83e\\udd2f\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 5, 1, 2, 4]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el7401405697923322886475224544\", ldavis_el7401405697923322886475224544_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el7401405697923322886475224544\", ldavis_el7401405697923322886475224544_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el7401405697923322886475224544\", ldavis_el7401405697923322886475224544_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "2     -0.112244  0.143847       1        1  31.494178\n",
              "4     -0.103360 -0.098268       2        1  27.396781\n",
              "0     -0.114352  0.145723       3        1  17.609742\n",
              "1     -0.071787 -0.210138       4        1  16.273924\n",
              "3      0.401744  0.018837       5        1   7.225375, topic_info=              Term           Freq          Total Category  logprob  loglift\n",
              "12              ai  146847.000000  146847.000000  Default  30.0000  30.0000\n",
              "241         google   24001.000000   24001.000000  Default  29.0000  29.0000\n",
              "199          write   35885.000000   35885.000000  Default  28.0000  28.0000\n",
              "165   intelligence   21935.000000   21935.000000  Default  27.0000  27.0000\n",
              "151            ask   37210.000000   37210.000000  Default  26.0000  26.0000\n",
              "...            ...            ...            ...      ...      ...      ...\n",
              "3          chatgpt   13641.486538  506703.740389   Topic5  -3.3270  -0.9872\n",
              "681           song    1482.489955    2987.077956   Topic5  -5.5464   1.9270\n",
              "101          world    1923.890385   15035.795849   Topic5  -5.2858   0.5715\n",
              "79           story    1454.959795    7420.052220   Topic5  -5.5651   0.9984\n",
              "1764          poem    1167.024682    5501.828503   Topic5  -5.7856   1.0769\n",
              "\n",
              "[357 rows x 6 columns], token_table=       Topic      Freq        Term\n",
              "term                              \n",
              "3925       3  0.999119        2021\n",
              "6648       3  0.998711           7\n",
              "13748      5  0.998646           `\n",
              "5773       3  0.999300  absolutely\n",
              "1022       1  0.114387      access\n",
              "...      ...       ...         ...\n",
              "104        1  0.998594           ðŸ˜Ž\n",
              "4951       5  0.998398           ðŸ˜®\n",
              "687        1  0.999606           ðŸ¤£\n",
              "5566       1  0.932030           ðŸ¤¯\n",
              "5566       4  0.067757           ðŸ¤¯\n",
              "\n",
              "[655 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 5, 1, 2, 4])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# re-run model with highest coherence score\n",
        "lda_model = gensim.models.LdaModel(corpus = corpus,\n",
        "                                   id2word = id2word,\n",
        "                                   num_topics = best_num_topics,\n",
        "                                   random_state = seed)\n",
        "\n",
        "# output interactive visualization\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFnReCE424zk"
      },
      "source": [
        "Just as a note before we move on: for whatever reason, the topic numbers above are ordered differently from the topic numbers we will see later. For the sake of coherence, the topic numbers we will use from here on out are different from the ones we see above. (This might be confusing at first, but it will make sense soon.)\n",
        "\n",
        "Based on the top words in each topic, we can roughly interpret the groups as follows:\n",
        "\n",
        "- Topic 1 (circle #3 above): **AI as a field**\n",
        "  - Top words: *chatgpt*, *google*, *ai*, *model*, *language*, *openai*, *answer*, *search*, *like*, *new*\n",
        "- Topic 2 (circle #4 above): **LLMs in general**\n",
        "  - Top words: *chatgpt*, *ai*, *openai*, *intelligence*, *artificial*, *chatbot*, *gpt*, *chat*, *human*, *chatgpt3*\n",
        "- Topic 3 (circle #1 above): **LLM prompts**\n",
        "  - Top words: *chatgpt*, *ask*, *write*, *ai*, *like*, *good*, *code*, *try*, *question*, *think*\n",
        "- Topic 4 (circle #5 above): **AI art**\n",
        "  - Top words: *chatgpt*, *art*, *ok*, *midjourney*, *probably*, *aiart*, *dalle2*, *nice*, *go_to*, *image*\n",
        "- Topic 5 (circle #2 above): **Innovation and impact**\n",
        "  - Top words: *chatgpt*, *ai*, *future*, *technology*, *tool*, *new*, *openai*, *use*, *learn*, *world*\n",
        "\n",
        "Our model can also be used to classify each tweet into one of the corresponding topics above. This is done by getting the individual probabilities of the tweet belonging to each topic, then choosing the topic that yields the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_LME_hFi51I",
        "outputId": "1b15975d-4602-4417-a7d6-a6fa2b1ed15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tweet: Top 10 ChatGPT Plugins You Should Use Right Now\n",
            "Read More:- https://t.co/p7jvcGsrwk \n",
            "#ChatGPT #bestChatGPTplugins #AIchatbot #topChatGPTplugins #TheTechTrend\n",
            "Topic: 3\n",
            "Topic probability: 0.58303314\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "def get_topic_and_prob(corpus_doc, model = lda_model):\n",
        "    \"\"\"Returns the classified topic and corresponding probability for a document\n",
        "    based on a given LDA model.\n",
        "    \"\"\"\n",
        "    # get the probabilities of belonging to each topic\n",
        "    probs = model.get_document_topics(corpus_doc)\n",
        "\n",
        "    # return the topic that yields the highest probability\n",
        "    topic, prob = max(probs, key = itemgetter(1))\n",
        "    return (topic + 1, prob) # add 1 to topic since topic numbers start from 0\n",
        "\n",
        "# initialize lists\n",
        "all_topics = list()\n",
        "all_probs = list()\n",
        "\n",
        "# get topics and probabilities for each doc\n",
        "for doc in corpus:\n",
        "    topic, prob = get_topic_and_prob(doc)\n",
        "    all_topics.append(topic)\n",
        "    all_probs.append(prob)\n",
        "\n",
        "# add to dataframe\n",
        "tweets_cleaned[\"topic\"] = all_topics\n",
        "tweets_cleaned[\"probability\"] = all_probs\n",
        "\n",
        "# show example\n",
        "print(\"Tweet:\", tweets_cleaned[\"text\"].iloc[0])\n",
        "print(\"Topic:\", tweets_cleaned[\"topic\"].iloc[0])\n",
        "print(\"Topic probability:\", tweets_cleaned[\"probability\"].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pbftRcj262X"
      },
      "source": [
        "For each topic, we sample and print out some tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO2eKygUi51I",
        "outputId": "efc5986f-9636-40e1-9275-242ac3801b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "â­ TOPIC 1: chatgpt, google, ai, model, language, openai, answer, search, like, new\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\"Introducing 7 new data formats for enterprises that want to unify their search experiences. Because nothing says 'unified' like compatibility issues!\" #Terminator #Sarcasm \n",
            "Link: https://t.co/7kSde1gtXz\n",
            "#AI #ChatGPT #OpenAI #GenerativeAI\n",
            "--------------------------------------------------\n",
            "#Bard even suggested what to do about this situation... https://t.co/IsapI2Rxom\n",
            "--------------------------------------------------\n",
            "Are you ready to take your accounting to the next level? Introducing the power of Artificial Intelligence in the field of accounting! ðŸ¤– Say goodbye to manual data entry, error-prone calculations, and tedious tasks.#AIinAccounting #FutureOfAccounting #chatgpt #ai https://t.co/TtDdKNJf7U\n",
            "--------------------------------------------------\n",
            "#Google has unveiled #Bard, it's response to #Microsoft's #ChatGPT. Learn what it looks like and when users can access the feature. Plus, find out thoughts on the timing of this announcement from our very own, Sarah. https://t.co/FiPRTJxmVJ\n",
            "--------------------------------------------------\n",
            "I think Microsoft gives more importance to AI because it is Google's competitor and Google gives more importance to humans.\n",
            "\n",
            "Think what is the reason behind making Google develop AI in compulsion\n",
            "\n",
            "#Google #AI #GoogleIO #Chatgpt\n",
            "--------------------------------------------------\n",
            "#Chatgpt giving Lebanonâ€™s best period in history. https://t.co/NEEAzc0Io3\n",
            "--------------------------------------------------\n",
            "If #bard can turn #chatgpt into #barf, we might be onto something, or it'll turn into barf.\n",
            "Seriously, if you just ate or are eating something, DON'T. LOOK. THAT. UP.\n",
            "--------------------------------------------------\n",
            "See how @OpenAI's #ChatGPT responded to questions about climate change: https://t.co/gPRQzNyRwn\n",
            "\n",
            "#ClimateEmergency #globalwarming https://t.co/zxblxdf2V9\n",
            "--------------------------------------------------\n",
            "ChapGPT is the new Google ðŸ’¯ #ChatGPT #OpenAI\n",
            "--------------------------------------------------\n",
            "I think Microsoft gives more importance to AI because it is Google's competitor and Google gives more importance to humans.\n",
            "\n",
            "Think what is the reason behind making Google develop AI in compulsion\n",
            "\n",
            "#Google #AI #GoogleIO #Chatgpt\n",
            "----------------------------------------------------------------------------------------------------\n",
            "â­ TOPIC 2: chatgpt, ai, openai, intelligence, artificial, chatbot, gpt, chat, human, chatgpt3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "What GPT is stands for?\n",
            "\n",
            "#ChatGPT #GPT #OpenAI\n",
            "--------------------------------------------------\n",
            "ChatGPT in Anticorruption Researchâ€“You Cannot Make This Up! @AnticorruptBlog #chatgpt #corruption #anticorruption #fraud #financialcrime https://t.co/m4Ke8CPDAK\n",
            "--------------------------------------------------\n",
            "New Post at AiNewsDrop!\n",
            "\n",
            "Tweet From Elon Musk On OpenAI https://t.co/FDCsio3Mlw\n",
            "\n",
            "#Artificial_Intelligence #ChatGPT https://t.co/TwNgS1iWbb\n",
            "--------------------------------------------------\n",
            "One of the best compliments #ChatGPT can get is people are pitting their natural intelligence against this #ArtificialIntelligence\n",
            "--------------------------------------------------\n",
            "Open AI's #ChatGPT vs. Google's #Bard. Who Will Win? Maybe it will be someone else - https://t.co/XEGqtspW1j #openAI #AI #technology https://t.co/26a7wGywK5\n",
            "--------------------------------------------------\n",
            "We tested Turnitin's ChatGPT-detector for teachers https://t.co/J7VfgFK6jA #ai #tech #machinelearning #deeplearning #gpt\n",
            "--------------------------------------------------\n",
            "Can you trust #ChatGPT? https://t.co/waI8WApeLu #amazon\n",
            "--------------------------------------------------\n",
            "#ChatGPT Libeled Me. Can I Sue? by @TedRall - Wall Street Journal: https://t.co/8H3ICRFxIZ\n",
            "--------------------------------------------------\n",
            "#Microsoft has published a blueprint with its vision about AI Regulation.\n",
            "\n",
            "I asked #ChatGPT to summarize it for me, result is impressive. Check it on your own ðŸ‘‡\n",
            "\n",
            "#AIRegulation #AI #ArtificialIntelligence #GPT4 \n",
            "\n",
            "https://t.co/yFKvUE6cY7\n",
            "--------------------------------------------------\n",
            "Open AI's #ChatGPT vs. Google's #Bard. Who Will Win? Maybe it will be someone else - https://t.co/XEGqtspW1j #openAI #AI #technology https://t.co/26a7wGywK5\n",
            "----------------------------------------------------------------------------------------------------\n",
            "â­ TOPIC 3: chatgpt, ask, write, ai, like, good, code, try, question, think\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Mission accomplished.\n",
            "\n",
            "Do you need a chatbot on your website anymore? Nah you need a #ChatGPT plugin\n",
            "--------------------------------------------------\n",
            "I asked #ChatGPT about \n",
            "Atmospheric Rivers In California...\n",
            "\n",
            "Do atmospheric rivers impact California more during el niÃ±o years or la niÃ±a years?\n",
            "1/x ðŸ§µ\n",
            "--------------------------------------------------\n",
            "AI writer is the talk of the town these days. It can definitely generate content almost as well as human! Tell us what is your take on this progress of technology, also have you tried this yet? \n",
            "\n",
            "#dedigitizers #digitalmarketing #Seo #contentmarketing #contentmarketing #ChatGPT\n",
            "--------------------------------------------------\n",
            "Let's make Twitter black!\n",
            "#isalmabad\n",
            "#ChatGPT \n",
            "#Ù‚Ø§ØªÙ„_Ø¹Ù…Ø±Ø§Ù†_Ú©Ùˆ_Ú¯Ø±ÙØªØ§Ø±_Ú©Ø±Ùˆ https://t.co/ejrnICO0jD\n",
            "--------------------------------------------------\n",
            "I think newsletter writers and equity analysts are going to be just fine.  #ChatGPT https://t.co/IXoKVMpuhH\n",
            "--------------------------------------------------\n",
            "@thealexbanks @memdotai mem it #ChatGPT #AI #Plugins\n",
            "--------------------------------------------------\n",
            "#ChatGPT isn't gender sensitive though, right? https://t.co/xfjQx0ywnb\n",
            "--------------------------------------------------\n",
            "For your reading pleasure...\n",
            "#ChatGPT take on @elonmusk (er @Twitter #trolling in the voice of William Shakespeare.\n",
            "ðŸ§µ\n",
            "--------------------------------------------------\n",
            "I think this is what broke it ðŸ˜‚\n",
            "\n",
            "#ChatGPT #ChatGPTGOD #ChatGPTdown\n",
            "--------------------------------------------------\n",
            "I think newsletter writers and equity analysts are going to be just fine.  #ChatGPT https://t.co/IXoKVMpuhH\n",
            "----------------------------------------------------------------------------------------------------\n",
            "â­ TOPIC 4: chatgpt, art, ok, midjourney, probably, aiart, dalle2, nice, go_to, image\n",
            "----------------------------------------------------------------------------------------------------\n",
            "A faceless figure, Heartless, devoid of feeling, Lurks in the shadows (c) #aiart #Mysterious #ChatGPT #dalle2 #AIArtCommuity https://t.co/0fhxVwB8dz\n",
            "--------------------------------------------------\n",
            "The Great Surrealist War\n",
            " #chatgpt #craiyon https://t.co/T9yArxaEyq\n",
            "--------------------------------------------------\n",
            "The Baby Bull Rascals are simply divine.\n",
            "\n",
            "So let's give a cheer for these little ones,\n",
            "Who bring us joy with all their fun.\n",
            "Here's to the Baby Bull Rascals,\n",
            "Cute and lovable, one and all.\n",
            "\n",
            "#ChatGPT\n",
            "--------------------------------------------------\n",
            "Chirp chirp! ðŸ¦ I'm snuggled up in my cosy birdbox, the rain tapping gently on the roof. Perfect for a nap! ðŸ¤ #bluetit #birdbox #birdwatching #ChatGPT\n",
            "\n",
            "watch live at https://t.co/rycH8Jm8KZ\n",
            "\n",
            "#openvino #WildIsles #SpringWatch #grafana #opencv #bluetit #ai  @TauntonPeregri https://t.co/VgNUg1sMHe\n",
            "--------------------------------------------------\n",
            "#ChatGPTâ€™s grasp of sovereignty seems to be superior to most professional Brexit supporters. https://t.co/qeg6jksET6\n",
            "--------------------------------------------------\n",
            "ChatGPT app for iOS now in ðŸ‡¦ðŸ‡±ðŸ‡­ðŸ‡·ðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡ªðŸ‡¯ðŸ‡²ðŸ‡°ðŸ‡·ðŸ‡³ðŸ‡¿ðŸ‡³ðŸ‡®ðŸ‡³ðŸ‡¬ðŸ‡¬ðŸ‡§ and more to come soon!\n",
            "\n",
            "#ChatGPT\n",
            "--------------------------------------------------\n",
            "Azimio la Umoja coalition chief agent Saitabao Kanchory has exposed how Retired President Uhuru Kenyatta felt towards his then Deputy President William Ruto.\n",
            "#Trending:\n",
            "#MainaAndKingangi\n",
            "NHIF\n",
            "#ChatGPT\n",
            "Kinuthia https://t.co/0R12DH62uZ\n",
            "--------------------------------------------------\n",
            "Cerberus Unleashed by Immortal Claw\n",
            "\n",
            "Prompt Details over here =&gt; https://t.co/zw2RLM5HNn\n",
            "\n",
            "#stablediffusion #gpt #ai #AIart #AIArtwork #metal https://t.co/RahQfi0TTO\n",
            "--------------------------------------------------\n",
            "Inspiration &amp; challenge the Midjourney AI to reimagine it as an artistic masterpiece!\n",
            "https://t.co/FlEdtWrjvB\n",
            "\n",
            "#Shimmer #Test2Conquer #ChatGPT #Midjourney https://t.co/vh6pO8QH07\n",
            "--------------------------------------------------\n",
            "#ChatGPTâ€™s grasp of sovereignty seems to be superior to most professional Brexit supporters. https://t.co/qeg6jksET6\n",
            "----------------------------------------------------------------------------------------------------\n",
            "â­ TOPIC 5: chatgpt, ai, future, technology, tool, new, openai, use, learn, world\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Can we use #ChatGPT to filter spam? Join the discussion! ðŸ’œ\n",
            "--------------------------------------------------\n",
            "The world of #AI. Text was created by #OpenAI's #ChatGPT, video was automatically rendered by #Pipio. Our thinking as of today is not enough for the future!\n",
            "\n",
            "Are you looking for a blog that offers valuable insights ... Look no further than the blog at https://t.co/kDSbRVlL8g! https://t.co/xOHaDeH8Oo\n",
            "--------------------------------------------------\n",
            "@PHA_BC  is excited to host the Public Health Summer Institute on June 22nd &amp; 23rd, 2023. We are seeking speakers who can contribute to day one of this virtual event focusing on #ArtificialIntelligence &amp; #publichealth \n",
            "#ChatGPT #GenerativeAI\n",
            "--------------------------------------------------\n",
            "I guess the advent of #ChatGPT has created a whole new world of pain for teachers marking work...\n",
            "--------------------------------------------------\n",
            "Learn the basics of #ChatGPT + how to use it effectively and ethically. \n",
            "\n",
            "Free @UMich Teach Out featuring Michael Wellman, @radamihalcea, @Scott_E_Page, @kentarotoyama, @julie_hui, @CAJamesMD, and Jack Barnard. \n",
            "\n",
            "ðŸ“ŒLive until April 3.\n",
            "\n",
            "https://t.co/Tbsw7PxmQp\n",
            "--------------------------------------------------\n",
            "The impact of ChatGPT on Agile is undeniable, but its implications are polarizing. \n",
            "\n",
            "Fascinating perspective from @AgileMario on the topic!\n",
            "\n",
            "https://t.co/WibH9xufWp\n",
            "\n",
            "#Agile #ChatGPT #DigitalTransformation https://t.co/7OrYrNGBEX\n",
            "--------------------------------------------------\n",
            "What are your thoughts on the rise of the AI chatbot?\n",
            "#edtech #schoolcommunication #schoolapp #edchat #schoolpr #suptchat #ChatGPT #SchoolInfo https://t.co/UAv2T7oKyB\n",
            "--------------------------------------------------\n",
            "Say goodbye to generic marketing emails - with #ChatGPT, you can generate personalized and engaging content for each of your customers. Improve your open rates and conversion rates with the power of AI! #Marketing #EmailMarketing #AI https://t.co/VlTEOG4l9D\n",
            "--------------------------------------------------\n",
            "#ChatGPT passes exam at Ivy League business school\n",
            "\n",
            "https://t.co/LcMYCpE6rM\n",
            "--------------------------------------------------\n",
            "Learn the basics of #ChatGPT + how to use it effectively and ethically. \n",
            "\n",
            "Free @UMich Teach Out featuring Michael Wellman, @radamihalcea, @Scott_E_Page, @kentarotoyama, @julie_hui, @CAJamesMD, and Jack Barnard. \n",
            "\n",
            "ðŸ“ŒLive until April 3.\n",
            "\n",
            "https://t.co/Tbsw7PxmQp\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# get each unique topic in the dataset\n",
        "topics = tweets_cleaned[\"topic\"].unique()\n",
        "topics.sort()\n",
        "\n",
        "# number of tweets per topic\n",
        "num_tweets = 10\n",
        "\n",
        "# print some random tweets from each topic\n",
        "for i in topics:\n",
        "    # sample tweets from the topic\n",
        "    sample = tweets_cleaned[tweets_cleaned[\"topic\"] == i].sample(num_tweets, random_state = 1)\n",
        "\n",
        "    # get the most common words for each topic\n",
        "    most_common_words_id = lda_model.get_topic_terms(i - 1) # topic IDs starts at 0 instead of 1\n",
        "    most_common_words_list = [id2word[id] for (id, value) in most_common_words_id]\n",
        "    most_common_words = \", \".join(most_common_words_list)\n",
        "\n",
        "    # print heading for the topic\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"â­ TOPIC {i}: {most_common_words}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # print tweets\n",
        "    for j in range(num_tweets - 1):\n",
        "        print(sample[\"text\"].iloc[j])\n",
        "        print(\"-\" * 50)\n",
        "    print(sample[\"text\"].iloc[4])\n",
        "print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeJGlXey1Vei"
      },
      "source": [
        "Just by looking at a sample of the tweets, we notice that some tweets don't necessarily fit into *any* of the topics that we defined â€“ remember that the model is just classifying tweets into a topic based on the highest probability. It seems that these kinds of tweets are trying to gain traction by using popular hashtags, often putting closely related hashtags in the same tweet. For example, #aiart could be paired together with #dalle2 and #midjourney (which are AI programs that can generate images from text input) not because the tweet is talking about these topics but because it is more likely to be viewed when looking up these topics.\n",
        "\n",
        "Despite these findings, what we see above provides valuable insights as to what kinds of words tend to be used together. We will continue our analysis with the topics we defined earlier, keeping in the back of our minds that some tweets don't necessarily have content about AI and/or LLMs.\n",
        "\n",
        "Now we'll take a look at how the distribution of these topics shift over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "Ru6-3VMBaRtv",
        "outputId": "89e5dfb0-b7e0-40aa-af95-802d3366f1da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-19b0c43c98ce4f66a3b56ff757523a5e\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-19b0c43c98ce4f66a3b56ff757523a5e\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-19b0c43c98ce4f66a3b56ff757523a5e\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-121a5c84efbef55511cfac7fbf7ee791\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"topic\", \"title\": \"Topic\", \"type\": \"nominal\"}, \"x\": {\"field\": \"month\", \"title\": \"Month\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"sum\", \"field\": \"count\", \"stack\": \"normalize\", \"title\": \"Normalized count\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-121a5c84efbef55511cfac7fbf7ee791\": [{\"topic\": 1, \"month\": \"2022-12\", \"count\": 6392}, {\"topic\": 1, \"month\": \"2023-01\", \"count\": 3996}, {\"topic\": 1, \"month\": \"2023-02\", \"count\": 15287}, {\"topic\": 1, \"month\": \"2023-03\", \"count\": 6879}, {\"topic\": 1, \"month\": \"2023-04\", \"count\": 8654}, {\"topic\": 1, \"month\": \"2023-05\", \"count\": 11921}, {\"topic\": 1, \"month\": \"2023-06\", \"count\": 1911}, {\"topic\": 2, \"month\": \"2022-12\", \"count\": 5858}, {\"topic\": 2, \"month\": \"2023-01\", \"count\": 4684}, {\"topic\": 2, \"month\": \"2023-02\", \"count\": 12486}, {\"topic\": 2, \"month\": \"2023-03\", \"count\": 6820}, {\"topic\": 2, \"month\": \"2023-04\", \"count\": 11318}, {\"topic\": 2, \"month\": \"2023-05\", \"count\": 10793}, {\"topic\": 2, \"month\": \"2023-06\", \"count\": 2513}, {\"topic\": 3, \"month\": \"2022-12\", \"count\": 28669}, {\"topic\": 3, \"month\": \"2023-01\", \"count\": 15449}, {\"topic\": 3, \"month\": \"2023-02\", \"count\": 34590}, {\"topic\": 3, \"month\": \"2023-03\", \"count\": 19230}, {\"topic\": 3, \"month\": \"2023-04\", \"count\": 28041}, {\"topic\": 3, \"month\": \"2023-05\", \"count\": 22170}, {\"topic\": 3, \"month\": \"2023-06\", \"count\": 4527}, {\"topic\": 4, \"month\": \"2022-12\", \"count\": 1657}, {\"topic\": 4, \"month\": \"2023-01\", \"count\": 739}, {\"topic\": 4, \"month\": \"2023-02\", \"count\": 1905}, {\"topic\": 4, \"month\": \"2023-03\", \"count\": 1137}, {\"topic\": 4, \"month\": \"2023-04\", \"count\": 1826}, {\"topic\": 4, \"month\": \"2023-05\", \"count\": 2528}, {\"topic\": 4, \"month\": \"2023-06\", \"count\": 564}, {\"topic\": 5, \"month\": \"2022-12\", \"count\": 8674}, {\"topic\": 5, \"month\": \"2023-01\", \"count\": 8294}, {\"topic\": 5, \"month\": \"2023-02\", \"count\": 23983}, {\"topic\": 5, \"month\": \"2023-03\", \"count\": 13656}, {\"topic\": 5, \"month\": \"2023-04\", \"count\": 22988}, {\"topic\": 5, \"month\": \"2023-05\", \"count\": 19874}, {\"topic\": 5, \"month\": \"2023-06\", \"count\": 4648}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# get counts of each topic for each month\n",
        "topics_by_month = tweets_cleaned.value_counts(\n",
        "    [\"topic\", \"month\"]\n",
        ").reset_index(\n",
        ").sort_values(\n",
        "    [\"topic\", \"month\"]\n",
        ").reset_index(\n",
        "    drop = True\n",
        ")\n",
        "\n",
        "# create normalized bar chart of tweet counts by topic over time\n",
        "alt.Chart(topics_by_month).mark_bar().encode(\n",
        "    x = alt.X(\"month:O\", title = \"Month\"),\n",
        "    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n",
        "    color = alt.Color(\"topic:N\", title = \"Topic\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnliya7ssWEb"
      },
      "source": [
        "Interestingly, the proportion of tweets relating to Topic 5 (Innovation and Impact) seems to be increasing over time while the proportion of tweets relating to Topic 3 (LLM prompts) seems to be decreasing over time. One possibility is that the release of ChatGPT in November 2022 led to a large influx of users experimenting with it and tweeting about what they're using it for (more about LLM prompts). After a while, this craze died down and more people are beginning to focus on the implications of having such AI tools in their daily lives (more about innovation and impact). Of course, this is only speculation as there could be other reasons as to why we see the change in the plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI7sbE83ah76"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSR5C8tL2v11"
      },
      "source": [
        "Now we move onto the second part of this project: getting the feelings or sentiments of the tweet. To save ourselves from work, we will use a pretrained sentiment analyzer called [VADER](https://vadersentiment.readthedocs.io/en/latest/), which is tuned to pick up sentiments in social media. We run this model on each tweet, getting back a compound score between -1 (very negative) and 1 (very positive). We then use this score to determine whether the tweet should be classified as \"positive\", \"negative\", or \"neutral\" based on the scoring outlined [here](https://github.com/cjhutto/vaderSentiment#about-the-scoring). The table below shows a breakdown of the classifications by counts and proportions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "X1o50_9WfhKy",
        "outputId": "9d7585b0-1436-4fa6-fb08-f76bc2cb77de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-492ba102-939c-4887-91e8-1c1d46880a49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>count</th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>215062</td>\n",
              "      <td>0.573984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>100040</td>\n",
              "      <td>0.266999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>59581</td>\n",
              "      <td>0.159017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-492ba102-939c-4887-91e8-1c1d46880a49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-492ba102-939c-4887-91e8-1c1d46880a49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-492ba102-939c-4887-91e8-1c1d46880a49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  sentiment   count  proportion\n",
              "0  positive  215062    0.573984\n",
              "1   neutral  100040    0.266999\n",
              "2  negative   59581    0.159017"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "def get_sentiment(text):\n",
        "    \"\"\"Returns the sentiment (positive, negative, or neutral) of the input text.\"\"\"\n",
        "    # get sentiment score\n",
        "    scores = sia.polarity_scores(text)\n",
        "    compound_score = scores[\"compound\"]\n",
        "\n",
        "    # classify as positive, negative, or neutral\n",
        "    if compound_score >= 0.05:\n",
        "        return \"positive\"\n",
        "    elif compound_score <= -0.05:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# apply get_sentiment() function to all tweets\n",
        "tweets_cleaned[\"sentiment\"] = tweets_cleaned[\"text_clean\"].apply(get_sentiment)\n",
        "\n",
        "# show number and proportion of tweets for each sentiment\n",
        "sentiment_counts = tweets_cleaned.value_counts(\"sentiment\").reset_index()\n",
        "sentiment_prop = tweets_cleaned.value_counts(\"sentiment\", normalize = True)\n",
        "sentiment_counts.merge(\n",
        "    sentiment_prop,\n",
        "    on = \"sentiment\",\n",
        "    how = \"left\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ngcP022ynK"
      },
      "source": [
        "The most common sentiment among the tweets is positive (57.4%), followed by neutral (26.7%) and negative (15.9%). This suggests that people on Twitter generally have positive or neutral feelings towards LLMs; negative tweets are less common.\n",
        "\n",
        "Our results above appear to reject the proposed hypothesis that there is a balance between positive and negative tweets. But we are also interested in the kinds of tweets are most \"characteristic\" of each sentiment, so we sample a few of them below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycf1cMWdkF4x",
        "outputId": "957983fd-b7e8-470e-b59b-c502d17625b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "â­ SENTIMENT: negative\n",
            "----------------------------------------------------------------------------------------------------\n",
            "it has been a few days since the #ChatGPT is all over the internet and I'm so tired of it already... the last time something annoyed me this much, this fast, was Friday by Rebecca Black\n",
            "--------------------------------------------------\n",
            "Bard is a cheat code:\n",
            "\n",
            "Copy and paste any article with a paywall and have bard summarize the conversation\n",
            "\n",
            "Tell me what articles youâ€™ve tried it with Iâ€™ll start:\n",
            "\n",
            "#ai #bard #Google #TechNews #ChatGPT\n",
            "--------------------------------------------------\n",
            "highaiartdump 15 of 24ish I dont think I shared these here, if i did not all 4 in one post, kind of spooky? #ai #aiart #aiartwork #digitalart #GenerativeAI #ChatGPT #midjourney #stablediffusion #toomanyedibleslore #ayyeyeart https://t.co/252Id6n3kC\n",
            "--------------------------------------------------\n",
            "https://t.co/leeMzMaqI4\n",
            "\n",
            "this is a BFD.  #openai #chatgpt #microsoft #bing #clippy\n",
            "--------------------------------------------------\n",
            "The frequency with which I am using #ChatGPT to get information scares me that I'll lose my extraordinary skill of digging out literally anything available on the #internet.\n",
            "--------------------------------------------------\n",
            "ðŸ”´When you ask a lot of questions about a lot of issues from Chat GPT, there is no specific answer from here on...\n",
            "\n",
            "-Isn't this familiar when we think about God and creatures?!\n",
            "Are we in a matrix â‰ï¸\n",
            "#Matrix #tatebrothers #andrewtate #ChatGPT\n",
            "--------------------------------------------------\n",
            "ðŸ¥¸ AI is going to replace writers! I just asked it to write me a blog post about tech in the style of @JoannaStern!\n",
            "\n",
            "ðŸ§ What would it do if there was no Joanna Stern?\n",
            "\n",
            "ðŸ¥¸ â€¦â€¦.\n",
            "\n",
            "#AI #ChatGPT #GPT4 #BingAI #Bard\n",
            "--------------------------------------------------\n",
            "Someone close to me is struggling financially. They become overwhelmed and stopped dealing with it\n",
            "\n",
            "I told them to let #ChatGPT deal with it\n",
            "\n",
            "I share this with permission because of all the talk about AI and what people are building with it\n",
            "\n",
            "It can also help those who struggle! https://t.co/PG7ImUGeKa\n",
            "--------------------------------------------------\n",
            "Did #ChatGPT just make a whole lot of wfh desk jobs obsolete?\n",
            "--------------------------------------------------\n",
            "The frequency with which I am using #ChatGPT to get information scares me that I'll lose my extraordinary skill of digging out literally anything available on the #internet.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "â­ SENTIMENT: neutral\n",
            "----------------------------------------------------------------------------------------------------\n",
            "more... exams being passed by #chatgpt \n",
            "\n",
            "https://t.co/IJH52IL2eU https://t.co/C2Zx1DKzXc\n",
            "--------------------------------------------------\n",
            "#Jobs Most Impacted by #ChatGPT and Similar #AI Models ðŸ’¼\n",
            "\n",
            "https://t.co/WTu1Tm9Och via @VisualCap \n",
            "#GPT4 #LLMs #FutureOfWork\n",
            "@chboursin @JolaBurnett @Hana_ElSayyed @Shi4Tech @RagusoSergio @efipm @enilev @JoannMoretti @mvollmer1 @baski_LA @CurieuxExplorer @anand_narang @kalydeoo https://t.co/ZKwSFFNNdz\n",
            "--------------------------------------------------\n",
            "Chatgpt-4 v/s Google Bard: A Head-to-Head Comparison #ChatGPT https://t.co/OmuGFC4955\n",
            "--------------------------------------------------\n",
            "@GiftCee Using #ChatGPT, another lefty tool... https://t.co/pD3EFYLGhK\n",
            "--------------------------------------------------\n",
            "Public-private partnerships: In some cases, governments form partnerships with private entities to develop and operate essential infrastructure projects. Such collaborations often involve a combination of private investment and government financing or guarantees. #chatgpt #OPENAI https://t.co/WTJnKQaJDB\n",
            "--------------------------------------------------\n",
            "First known student caught using ChatGPT at UK university - here's ... - The Tab #chatgpt #AI #openAI https://t.co/qdytWOeD7o\n",
            "--------------------------------------------------\n",
            "Using any ChatGPT plugins? Need suggestions. #AI #ChatGPT\n",
            "--------------------------------------------------\n",
            "Should you learn Machine Learning?\n",
            "\n",
            "Here is perhaps THE seminal course for an introduction.\n",
            "#MachineLearning Specialization 2022 @AndrewYNg, @Stanford \n",
            "https://t.co/KlUH1cpeP8\n",
            "\n",
            "This playlist will let you decide for yourself\n",
            "#Python #NumPy #TensorFlow #NeuralNetworks\n",
            "\n",
            "#ChatGPT\n",
            "--------------------------------------------------\n",
            "Read \"BuzzFeed preps for AI-written content while CNET fumbles\" https://t.co/pETxEwYQOy\n",
            "\n",
            "For more, get the app from\n",
            "https://t.co/0ic5ya66on\n",
            "\n",
            "#ChatGPT #AI #ML #DL #content #media https://t.co/dnVhSEI35q\n",
            "--------------------------------------------------\n",
            "Public-private partnerships: In some cases, governments form partnerships with private entities to develop and operate essential infrastructure projects. Such collaborations often involve a combination of private investment and government financing or guarantees. #chatgpt #OPENAI https://t.co/WTJnKQaJDB\n",
            "----------------------------------------------------------------------------------------------------\n",
            "â­ SENTIMENT: positive\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ChatGPT-3 is really something. It unlocks new levels we never thought of. It's a revolutionary technology but still not 100% mature. Here's why ðŸ‘‡ #ArtificialIntelligence #ChatGPT\n",
            "--------------------------------------------------\n",
            "Did you know that the universe is expanding at an accelerating rate? This discovery was awarded the Nobel Prize in Physics in 2011, and it has revolutionized our understanding of the cosmos. #Cosmology #ChatGPT #Physics https://t.co/Y11i8VZwi5\n",
            "--------------------------------------------------\n",
            "Just saw a UFO outside my window and it was shaped like a giant doughnut! #aliens #UFO #doughnuts #ChatGPT ðŸ¤·â€â™‚ï¸ https://t.co/BT0iJyrKWh\n",
            "--------------------------------------------------\n",
            "You won't believe how much work ChatGPT can take off your plate when it comes to creating a resume. Check out the incredible resume it generated for #LucyLiu, LucyLiu, at https://t.co/GK6CJCMxzF. #ChatGPT #ai #artificialintelligence #OpenAI #resume #C https://t.co/zs86qgkfDA\n",
            "--------------------------------------------------\n",
            "It's true that #ChatGPT is super useful, but I love the references, links and cards that come with @YouSearchEngine's YouChat responses! I'm missing this a lot when I go back to try things with ChatGPT. LLM's that cite references! https://t.co/7m9GiUEYGU\n",
            "--------------------------------------------------\n",
            "Couldnâ€™t agree with @profgalloway more. Iâ€™ve already used #ChatGPT heavily and itâ€™s still in its infancy. #LLM and other #AI advancements are going to be leaps forward in terms of usefulness in 2023. https://t.co/MThR6XJ5ZH\n",
            "--------------------------------------------------\n",
            "https://t.co/6zdxvfYvFq Unveils Groundbreaking #ChatGPT #Course and Innovative #Upskilling Platform https://t.co/XghNdfJUYs\n",
            "--------------------------------------------------\n",
            "Anyone know how to utilize chatgpt when inputs are too large to copy and paste into the text box?\n",
            "\n",
            "Keep in mind I want to use it to help me find relevant information in a haystack.\n",
            "#chatgpt\n",
            "--------------------------------------------------\n",
            "See how you can make AI code for yourself for free in just 5 mins \n",
            "If you're searching for the easiest method to make a Website, then this video is definitely for you.\n",
            "https://t.co/yOeg82XPBX\n",
            "#openai #chatgpt #howtomakewebsite #aiprogramming #programming #easiestway #aicoding https://t.co/GrY3sDj399\n",
            "--------------------------------------------------\n",
            "It's true that #ChatGPT is super useful, but I love the references, links and cards that come with @YouSearchEngine's YouChat responses! I'm missing this a lot when I go back to try things with ChatGPT. LLM's that cite references! https://t.co/7m9GiUEYGU\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# get each unique sentiment\n",
        "sentiments = tweets_cleaned[\"sentiment\"].unique()\n",
        "sentiments.sort()\n",
        "\n",
        "# number of tweets per sentiment\n",
        "num_tweets = 10\n",
        "\n",
        "# print some random tweets from each sentiment\n",
        "for s in sentiments:\n",
        "    # sample tweets from the sentiment\n",
        "    sample = tweets_cleaned[tweets_cleaned[\"sentiment\"] == s].sample(num_tweets, random_state = 1)\n",
        "\n",
        "    # print heading for the sentiment\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"â­ SENTIMENT: {s}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # print tweets\n",
        "    for j in range(num_tweets - 1):\n",
        "        print(sample[\"text\"].iloc[j])\n",
        "        print(\"-\" * 50)\n",
        "    print(sample[\"text\"].iloc[4])\n",
        "print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLKVSlLM2zQu"
      },
      "source": [
        "It seems like the tweets that are labeled as positive tend to praise LLMs like ChatGPT since they can be beneficial in saving time and solving specific problems. On the other hand, some of the tweets classified as negative aren't necessarily negative, which is probably due to the presence of negative words. (Now is a good time to note that VADER maps each word to a score and averages these scores into a compound score. You can read more about how it works [here](https://medium.com/@piocalderon/vader-sentiment-analysis-explained-f1c4f9101cd9).) Then again, these are only a sample of the tweets, so we're not necessarily getting the full picture here.\n",
        "\n",
        "Moving on: how do the sentiments of these tweets change over time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "yaeOLBMr1df5",
        "outputId": "8fc28fee-5512-43e3-fee2-127c2c81082d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-42642dbccc604ff18cec3f2feeb53a54\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-42642dbccc604ff18cec3f2feeb53a54\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-42642dbccc604ff18cec3f2feeb53a54\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-34b44432ac17f8c17f73c6d8a0f29fe4\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"sentiment\", \"scale\": {\"domain\": [\"negative\", \"neutral\", \"positive\"], \"range\": [\"red\", \"orange\", \"green\"]}, \"title\": \"Sentiment\", \"type\": \"nominal\"}, \"x\": {\"field\": \"month\", \"title\": \"Month\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"sum\", \"field\": \"count\", \"stack\": \"normalize\", \"title\": \"Normalized count\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-34b44432ac17f8c17f73c6d8a0f29fe4\": [{\"sentiment\": \"negative\", \"month\": \"2022-12\", \"count\": 8671}, {\"sentiment\": \"negative\", \"month\": \"2023-01\", \"count\": 5268}, {\"sentiment\": \"negative\", \"month\": \"2023-02\", \"count\": 14523}, {\"sentiment\": \"negative\", \"month\": \"2023-03\", \"count\": 7070}, {\"sentiment\": \"negative\", \"month\": \"2023-04\", \"count\": 11827}, {\"sentiment\": \"negative\", \"month\": \"2023-05\", \"count\": 10271}, {\"sentiment\": \"negative\", \"month\": \"2023-06\", \"count\": 1950}, {\"sentiment\": \"neutral\", \"month\": \"2022-12\", \"count\": 14404}, {\"sentiment\": \"neutral\", \"month\": \"2023-01\", \"count\": 9060}, {\"sentiment\": \"neutral\", \"month\": \"2023-02\", \"count\": 23995}, {\"sentiment\": \"neutral\", \"month\": \"2023-03\", \"count\": 12874}, {\"sentiment\": \"neutral\", \"month\": \"2023-04\", \"count\": 18250}, {\"sentiment\": \"neutral\", \"month\": \"2023-05\", \"count\": 17856}, {\"sentiment\": \"neutral\", \"month\": \"2023-06\", \"count\": 3587}, {\"sentiment\": \"positive\", \"month\": \"2022-12\", \"count\": 28175}, {\"sentiment\": \"positive\", \"month\": \"2023-01\", \"count\": 18834}, {\"sentiment\": \"positive\", \"month\": \"2023-02\", \"count\": 49733}, {\"sentiment\": \"positive\", \"month\": \"2023-03\", \"count\": 27778}, {\"sentiment\": \"positive\", \"month\": \"2023-04\", \"count\": 42750}, {\"sentiment\": \"positive\", \"month\": \"2023-05\", \"count\": 39159}, {\"sentiment\": \"positive\", \"month\": \"2023-06\", \"count\": 8626}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# get counts of each sentiment for each month\n",
        "sentiment_by_month = tweets_cleaned.value_counts(\n",
        "    [\"sentiment\", \"month\"]\n",
        ").reset_index(\n",
        ").sort_values(\n",
        "    [\"sentiment\", \"month\"]\n",
        ").reset_index(\n",
        "    drop = True\n",
        ")\n",
        "\n",
        "# create normalized bar chart of tweet counts by sentiment over time\n",
        "alt.Chart(sentiment_by_month).mark_bar().encode(\n",
        "    x = alt.X(\"month:O\", title = \"Month\"),\n",
        "    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n",
        "    color = alt.Color(\"sentiment:N\",\n",
        "                      title = \"Sentiment\",\n",
        "                      scale = alt.Scale(domain = [\"negative\", \"neutral\", \"positive\"],\n",
        "                                        range = [\"red\", \"orange\", \"green\"]))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IPxrGghH-_2"
      },
      "source": [
        "For the most part, the distribution of sentiments don't vary that much between months. Despite there being a slight increase in positive tweets over time, it seems that Twitter users are generally consistent about their opinions on LLMs.\n",
        "\n",
        "Since we have both the topics and sentiments for all of the tweets, we can see if certain topics tend to have lower or higher proportions of positive sentiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "ZqwuNDYSee87",
        "outputId": "60c82681-5d2d-42ef-ca1e-7a800ddad12b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-277caa5b3b134c1dbc470cdef15dd9e4\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-277caa5b3b134c1dbc470cdef15dd9e4\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-277caa5b3b134c1dbc470cdef15dd9e4\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-f44aa0041fcfbcf8a150ed2390a8c8d4\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"sentiment\", \"scale\": {\"domain\": [\"negative\", \"neutral\", \"positive\"], \"range\": [\"red\", \"orange\", \"green\"]}, \"title\": \"Sentiment\", \"type\": \"nominal\"}, \"x\": {\"field\": \"topic\", \"title\": \"Topic\", \"type\": \"ordinal\"}, \"y\": {\"aggregate\": \"sum\", \"field\": \"count\", \"stack\": \"normalize\", \"title\": \"Normalized count\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-f44aa0041fcfbcf8a150ed2390a8c8d4\": [{\"topic\": 1, \"sentiment\": \"negative\", \"count\": 8789}, {\"topic\": 1, \"sentiment\": \"neutral\", \"count\": 18676}, {\"topic\": 1, \"sentiment\": \"positive\", \"count\": 27576}, {\"topic\": 2, \"sentiment\": \"negative\", \"count\": 9172}, {\"topic\": 2, \"sentiment\": \"neutral\", \"count\": 18099}, {\"topic\": 2, \"sentiment\": \"positive\", \"count\": 27206}, {\"topic\": 3, \"sentiment\": \"negative\", \"count\": 25419}, {\"topic\": 3, \"sentiment\": \"neutral\", \"count\": 37390}, {\"topic\": 3, \"sentiment\": \"positive\", \"count\": 89872}, {\"topic\": 4, \"sentiment\": \"negative\", \"count\": 2084}, {\"topic\": 4, \"sentiment\": \"neutral\", \"count\": 3362}, {\"topic\": 4, \"sentiment\": \"positive\", \"count\": 4915}, {\"topic\": 5, \"sentiment\": \"negative\", \"count\": 14117}, {\"topic\": 5, \"sentiment\": \"neutral\", \"count\": 22513}, {\"topic\": 5, \"sentiment\": \"positive\", \"count\": 65493}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "# get counts of each sentiment by topic\n",
        "topics_by_sentiment = tweets_cleaned.value_counts(\n",
        "    [\"topic\", \"sentiment\"]\n",
        ").reset_index(\n",
        ").sort_values(\n",
        "    [\"topic\", \"sentiment\"]\n",
        ").reset_index(\n",
        "    drop = True\n",
        ")\n",
        "\n",
        "# create normalized bar chart of tweet counts by sentiment for each topic\n",
        "alt.Chart(topics_by_sentiment).mark_bar().encode(\n",
        "    x = alt.X(\"topic:O\", title = \"Topic\"),\n",
        "    y = alt.Y(\"sum(count)\", title = \"Normalized count\", stack = \"normalize\"),\n",
        "    color = alt.Color(\"sentiment:N\",\n",
        "                      title = \"Sentiment\",\n",
        "                      scale = alt.Scale(domain = [\"negative\", \"neutral\", \"positive\"],\n",
        "                                        range = [\"red\", \"orange\", \"green\"]))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlPHt3SxwMMy"
      },
      "source": [
        "Once again, we notice Topics 3 (LLM prompts) and 5 (Innovation and impact) popping up again â€“ the 2 topics appear to have higher proportions of positive sentiments. Around half of the tweets in each of the other topics (AI as a field, LLMs in general, AI art) are classified as positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUufknUhOtaF"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgsMK4GPO1o8"
      },
      "source": [
        "### Summary of methods\n",
        "\n",
        "- We focused on tweets about LLMs ranging from December 2022 to the beginning of June 2023 to understand the online discourse surrounding them.\n",
        "- We cleaned the tweets, which included filtering out spam tweets and standardizing the text (e.g., lowercasing, lemmatizing, tokenizing).\n",
        "- We performed topic modeling and sentiment analysis on the remaining tweets and also looked at how the resulting topics and sentiments changed over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDlX_82-O2rE"
      },
      "source": [
        "### Answers\n",
        "\n",
        "1. *What kinds of topics are brought up in the online discourse surrounding LLMs?*\n",
        "\n",
        "    - The discourse surrounding LLMs tended to fall into one of the 5 topics: AI as a field, LLMs in general, LLM prompts, AI art, and Innovation and impact.\n",
        "    - There was an initial increase in tweets about LLM prompts after the initial launch of ChatGPT in November 2022, though tweets in the later months shifted towards being more about innovation and impact.\n",
        "    - These topics are not as clear-cut as initially thought; in fact, the topics have considerable overlap.\n",
        "\n",
        "2. *What kinds of sentiments are associated with online discussions about LLMs?*\n",
        "\n",
        "    - Tweets about LLMs tended to be more positive or neutral; neutral tweets made up a smaller proportion (around 15.9%).\n",
        "    - Over time, the distribution of these sentiments generally did not change â€“ there were still more positive and neutral tweets compared to negative ones.\n",
        "    - When taking a closer look at legitimate tweets about LLMs (i.e., not spam), positive tweets generally praise LLMs for being revolutionary and efficient while negative tweets tend to be critical about their impact. However, the sentiment labels are a bit hazy since VADER can be prone to misclassifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzIBPvLMO7_k"
      },
      "source": [
        "### Limitations\n",
        "\n",
        "- **It is difficult to manually filter out spam.** A lot of the methods used to filter out spam in this analysis required hard-coding values (e.g., filtering out certain hashtags). This process is by no means perfect as spam tweets could still pass through while other legitimate tweets could be filtered out.\n",
        "- **Only tweets were used in this analysis.** We only looked at tweets about LLMs since the dataset was readily available. However, the results can only at most be generalizable to people who use Twitter, which does not include everyone on the Internet who has an opinion on LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9B288tKPBNO"
      },
      "source": [
        "### Future directions\n",
        "\n",
        "- **Find a better way to filter out spam, possibly through machine learning.** One method to try out in the future would be to train a classification model on a labeled dataset of spam and non-spam tweets, then tweak it to filter out spam in our data.\n",
        "- **Use other sources.** In addition to using tweets, an extension of this project could compare how the discourse changes when focusing on different social media platforms (e.g., Reddit, Facebook) and news outlets (e.g. The New York Times, Fox News)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
